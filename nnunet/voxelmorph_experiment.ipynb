{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2D ACDC training (+validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 431, 2)\n"
     ]
    }
   ],
   "source": [
    "import ants\n",
    "import numpy as np\n",
    "\n",
    "depth = 0\n",
    "\n",
    "moving_image = np.load(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task036_Lib\\custom_experiment_planner_stage0\\patient007_frame01.npz\")['data'][0]\n",
    "fixed_image = np.load(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task036_Lib\\custom_experiment_planner_stage0\\patient007_frame01.npz\")['data'][0]\n",
    "\n",
    "moving_image = moving_image[depth]\n",
    "fixed_image = fixed_image[depth]\n",
    "\n",
    "moving_image = ants.from_numpy(moving_image)\n",
    "fixed_image = ants.from_numpy(fixed_image)\n",
    "\n",
    "registration = ants.registration(\n",
    "    fixed=fixed_image,\n",
    "    moving=moving_image,\n",
    "    type_of_transform='SyN'\n",
    ")\n",
    "\n",
    "# Get the deformation field transformation\n",
    "deformation_field = registration['fwdtransforms'][0]  # Assuming only one transformation is returned\n",
    "\n",
    "# Load the deformation field image\n",
    "deformation_field_image = ants.image_read(deformation_field).numpy()\n",
    "\n",
    "print(deformation_field_image.shape)\n",
    "\n",
    "transformed_image = ants.apply_transforms(\n",
    "    fixed=fixed_image,\n",
    "    moving=moving_image,\n",
    "    transformlist=registration['fwdtransforms']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:54<00:00, 10.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from monai.transforms import ResizeWithPadOrCrop\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from monai.transforms import NormalizeIntensity\n",
    "import torch\n",
    "from nnunet.lib.training_utils import read_config\n",
    "from nnunet.lib.utils import ConvBlocks2D\n",
    "from nnunet.lib.training_utils import build_2d_model\n",
    "from nnunet.training.network_training.processor import Processor\n",
    "\n",
    "def crop_get_padding(arr):\n",
    "    # 2, H, W, D\n",
    "    C, H, W, D = arr.shape\n",
    "    roi_center = [i // 2 for i in [H, W]]\n",
    "\n",
    "    y1 = max(roi_center[0] - 112, 0)\n",
    "    y2 = min(roi_center[0] + 112, H)\n",
    "\n",
    "    x1 = max(roi_center[1] - 112, 0)\n",
    "    x2 = min(roi_center[1] + 112, W)\n",
    "\n",
    "    new_arr = arr[:, y1:y2, x1:x2, :]\n",
    "    padding = ((0, 0), (y1, H - y2), (x1, W - x2), (0, 0))\n",
    "    \n",
    "    padded = np.pad(new_arr, padding)\n",
    "    assert arr.shape == padded.shape\n",
    "\n",
    "    return new_arr, padding\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\binary\"\n",
    "cropper_config = read_config(os.path.join(Path.cwd(), 'adversarial_acdc.yaml'), False, False)\n",
    "\n",
    "cropping_conv_layer = ConvBlocks2D\n",
    "cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=224, window_size=7, middle=False, num_classes=2)\n",
    "cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "cropping_network.eval()\n",
    "cropping_network.do_ds = False\n",
    "\n",
    "processor = Processor(crop_size=128, image_size=224, cropping_network=cropping_network)\n",
    "\n",
    "output_folder = 'voxelmorph_ACDC_2D'\n",
    "output_folder_gt = 'voxelmorph_ACDC_gt_2D'\n",
    "\n",
    "delete_if_exist(output_folder)\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "delete_if_exist(output_folder_gt)\n",
    "Path(output_folder_gt).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_list = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task031_ACDC\\custom_experiment_planner_stage0\", '*.npz')))\n",
    "path_list_pkl = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task031_ACDC\\custom_experiment_planner_stage0\", '*.pkl')))\n",
    "#path_list = [x for x in path_list if '_u' not in os.path.basename(x)[:-4]]\n",
    "\n",
    "assert len(path_list) == len(path_list_pkl)\n",
    "cropper = ResizeWithPadOrCrop(spatial_size=(224, 224, -1))\n",
    "\n",
    "patient_list = sorted(list(set([os.path.basename(x).split('_')[0] for x in path_list])))\n",
    "\n",
    "all_patient_paths = []\n",
    "all_patient_paths_pkl = []\n",
    "for patient in patient_list:\n",
    "    patient_files = []\n",
    "    patient_files_pkl = []\n",
    "    for (path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        if patient in path:\n",
    "            patient_files.append(path)\n",
    "        if patient in pkl_path:\n",
    "            patient_files_pkl.append(pkl_path)\n",
    "    all_patient_paths.append(sorted(patient_files))\n",
    "    all_patient_paths_pkl.append(sorted(patient_files_pkl))\n",
    "\n",
    "for (path_list, path_list_pkl) in tqdm(zip(all_patient_paths, all_patient_paths_pkl), total=len(all_patient_paths)):\n",
    "\n",
    "    patient_nb = os.path.basename(path_list[0]).split('_')[0]\n",
    "    cfg_path = os.path.join('ACDC_training', patient_nb, 'info.cfg')\n",
    "    with open(cfg_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        ed_number = int(lines[0].split(' ')[-1]) - 1\n",
    "        es_number = int(lines[1].split(' ')[-1]) - 1\n",
    "\n",
    "    sequence = []\n",
    "    filename_list = []\n",
    "    for (npz_path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        filename = os.path.basename(npz_path)[:-4] + '.nii.gz'\n",
    "        data = np.load(npz_path)\n",
    "        arr = data['data']\n",
    "        arr = arr.transpose((0, 2, 3, 1))\n",
    "\n",
    "        initial_shape = list(arr.shape[1:])\n",
    "\n",
    "        cropped = cropper(arr)\n",
    "        if cropped.shape[0] == 1:\n",
    "            cropped = np.tile(cropped, [2, 1, 1, 1])\n",
    "        sequence.append(cropped)\n",
    "        filename_list.append(filename)\n",
    "    sequence = np.stack(sequence, axis=0) # T, C, H, W, D\n",
    "    sequence = sequence.transpose(4, 0, 1, 2, 3) # D, T, C, H, W\n",
    "\n",
    "    block = []\n",
    "    padding_need_list = []\n",
    "    for d in range(len(sequence)):\n",
    "\n",
    "        unlabeled = torch.from_numpy(sequence[d]).to('cuda:0')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_centroid, _ = processor.preprocess_no_registration(data=torch.clone(unlabeled[:, :1])) # T, C(1), H, W\n",
    "\n",
    "            cropped_unlabeled, padding_need = processor.crop_and_pad(data=unlabeled, mean_centroid=mean_centroid)\n",
    "            cropped_unlabeled = cropped_unlabeled.cpu()\n",
    "            padding_need = padding_need.cpu()\n",
    "            padding_need_list.append(padding_need)\n",
    "\n",
    "        block.append(cropped_unlabeled)\n",
    "    block = np.stack(block, axis=-1) # T, C, H, W, D\n",
    "    padding_need = np.stack(padding_need_list, axis=-1) # 4, D\n",
    "    \n",
    "    #divisor = cropped[0].max() - cropped[0].min()\n",
    "    #subtrahend = cropped[0].min()\n",
    "    #cropped_img = NormalizeIntensity(subtrahend=subtrahend, divisor=divisor)(cropped[0])\n",
    "\n",
    "    for t in range(len(block)):\n",
    "        current_volume = block[t]\n",
    "        current_filename = filename_list[t]\n",
    "\n",
    "        grayscale = current_volume[0]\n",
    "        grayscale_flattened = grayscale.reshape(-1, current_volume[0].shape[-1])\n",
    "\n",
    "        if np.any((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0))) == 0:\n",
    "            print(current_filename)\n",
    "            print((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0)))\n",
    "\n",
    "        grayscale_flattened = (grayscale_flattened - np.min(grayscale_flattened, axis=0)) / (np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0) + 1e-8)\n",
    "        grayscale = grayscale_flattened.reshape(current_volume[0].shape)\n",
    "\n",
    "        nib.save(nib.Nifti1Image(grayscale, affine=np.eye(4)), os.path.join(output_folder, current_filename))\n",
    "        if '_u' not in current_filename:\n",
    "            cropped_gt = current_volume[1]\n",
    "            cropped_gt[cropped_gt < 0] = 0\n",
    "            nib.save(nib.Nifti1Image(cropped_gt, affine=np.eye(4)), os.path.join(output_folder_gt, current_filename))\n",
    "        \n",
    "        with open(path_list_pkl[t], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data['padding_need'] = padding_need\n",
    "            data['voxelmorph_size_before'] = initial_shape\n",
    "            data['ed_number'] = ed_number\n",
    "            data['es_number'] = es_number\n",
    "\n",
    "            with open(os.path.join(output_folder, os.path.basename(path_list_pkl[t])), 'wb') as f:\n",
    "                pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2D ACDC testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:31<00:00,  9.03s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from monai.transforms import ResizeWithPadOrCrop\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from monai.transforms import NormalizeIntensity\n",
    "import torch\n",
    "from nnunet.lib.training_utils import read_config\n",
    "from nnunet.lib.utils import ConvBlocks2DGroup\n",
    "from nnunet.lib.training_utils import build_2d_model\n",
    "from nnunet.training.network_training.processor import Processor\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\binary\"\n",
    "cropper_config = read_config(os.path.join(Path.cwd(), 'adversarial_acdc.yaml'), False, False)\n",
    "\n",
    "cropping_conv_layer = ConvBlocks2DGroup\n",
    "cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=224, window_size=7, middle=False, num_classes=2)\n",
    "cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "cropping_network.eval()\n",
    "cropping_network.do_ds = False\n",
    "\n",
    "processor = Processor(crop_size=128, image_size=224, cropping_network=cropping_network)\n",
    "\n",
    "output_folder = 'voxelmorph_ACDC_2D_testing'\n",
    "output_folder_gt = 'voxelmorph_ACDC_gt_2D_testing'\n",
    "\n",
    "delete_if_exist(output_folder)\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "delete_if_exist(output_folder_gt)\n",
    "Path(output_folder_gt).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_list = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task035_ACDC\\custom_experiment_planner_stage0\", '*.npz')))\n",
    "path_list_pkl = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task035_ACDC\\custom_experiment_planner_stage0\", '*.pkl')))\n",
    "#path_list = [x for x in path_list if '_u' not in os.path.basename(x)[:-4]]\n",
    "\n",
    "assert len(path_list) == len(path_list_pkl)\n",
    "cropper = ResizeWithPadOrCrop(spatial_size=(224, 224, -1))\n",
    "\n",
    "patient_list = sorted(list(set([os.path.basename(x).split('_')[0] for x in path_list])))\n",
    "\n",
    "all_patient_paths = []\n",
    "all_patient_paths_pkl = []\n",
    "for patient in patient_list:\n",
    "    patient_files = []\n",
    "    patient_files_pkl = []\n",
    "    for (path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        if patient in path:\n",
    "            patient_files.append(path)\n",
    "        if patient in pkl_path:\n",
    "            patient_files_pkl.append(pkl_path)\n",
    "    all_patient_paths.append(sorted(patient_files))\n",
    "    all_patient_paths_pkl.append(sorted(patient_files_pkl))\n",
    "\n",
    "for (path_list, path_list_pkl) in tqdm(zip(all_patient_paths, all_patient_paths_pkl), total=len(all_patient_paths)):\n",
    "\n",
    "    patient_nb = os.path.basename(path_list[0]).split('_')[0]\n",
    "    cfg_path = os.path.join('ACDC_testing', patient_nb, 'info.cfg')\n",
    "    with open(cfg_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        ed_number = int(lines[0].split(' ')[-1]) - 1\n",
    "        es_number = int(lines[1].split(' ')[-1]) - 1\n",
    "\n",
    "    sequence = []\n",
    "    filename_list = []\n",
    "    for (npz_path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        filename = os.path.basename(npz_path)[:-4] + '.nii.gz'\n",
    "        data = np.load(npz_path)\n",
    "        arr = data['data']\n",
    "        arr = arr.transpose((0, 2, 3, 1))\n",
    "\n",
    "        initial_shape = list(arr.shape[1:])\n",
    "\n",
    "        cropped = cropper(arr)\n",
    "        if cropped.shape[0] == 1:\n",
    "            cropped = np.tile(cropped, [2, 1, 1, 1])\n",
    "        sequence.append(cropped)\n",
    "        filename_list.append(filename)\n",
    "    sequence = np.stack(sequence, axis=0) # T, C, H, W, D\n",
    "    sequence = sequence.transpose(4, 0, 1, 2, 3) # D, T, C, H, W\n",
    "\n",
    "    block = []\n",
    "    padding_need_list = []\n",
    "    for d in range(len(sequence)):\n",
    "\n",
    "        unlabeled = torch.from_numpy(sequence[d]).to('cuda:0')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_centroid, _ = processor.preprocess_no_registration(data=torch.clone(unlabeled[:, :1])) # T, C(1), H, W\n",
    "\n",
    "            cropped_unlabeled, padding_need = processor.crop_and_pad(data=unlabeled, mean_centroid=mean_centroid)\n",
    "            cropped_unlabeled = cropped_unlabeled.cpu()\n",
    "            padding_need = padding_need.cpu()\n",
    "            padding_need_list.append(padding_need)\n",
    "\n",
    "        block.append(cropped_unlabeled)\n",
    "    block = np.stack(block, axis=-1) # T, C, H, W, D\n",
    "    padding_need = np.stack(padding_need_list, axis=-1) # 4, D\n",
    "    \n",
    "    #divisor = cropped[0].max() - cropped[0].min()\n",
    "    #subtrahend = cropped[0].min()\n",
    "    #cropped_img = NormalizeIntensity(subtrahend=subtrahend, divisor=divisor)(cropped[0])\n",
    "\n",
    "    for t in range(len(block)):\n",
    "        current_volume = block[t]\n",
    "        current_filename = filename_list[t]\n",
    "\n",
    "        grayscale = current_volume[0]\n",
    "        grayscale_flattened = grayscale.reshape(-1, current_volume[0].shape[-1])\n",
    "\n",
    "        if np.any((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0))) == 0:\n",
    "            print(current_filename)\n",
    "            print((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0)))\n",
    "\n",
    "        grayscale_flattened = (grayscale_flattened - np.min(grayscale_flattened, axis=0)) / (np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0) + 1e-8)\n",
    "        grayscale = grayscale_flattened.reshape(current_volume[0].shape)\n",
    "\n",
    "        nib.save(nib.Nifti1Image(grayscale, affine=np.eye(4)), os.path.join(output_folder, current_filename))\n",
    "        if '_u' not in current_filename:\n",
    "            cropped_gt = current_volume[1]\n",
    "            cropped_gt[cropped_gt < 0] = 0\n",
    "            nib.save(nib.Nifti1Image(cropped_gt, affine=np.eye(4)), os.path.join(output_folder_gt, current_filename))\n",
    "        \n",
    "        with open(path_list_pkl[t], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data['padding_need'] = padding_need\n",
    "            data['voxelmorph_size_before'] = initial_shape\n",
    "            data['ed_number'] = ed_number\n",
    "            data['es_number'] = es_number\n",
    "\n",
    "            with open(os.path.join(output_folder, os.path.basename(path_list_pkl[t])), 'wb') as f:\n",
    "                pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2D Lib training (+validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [46:21<00:00, 12.65s/it] \n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from monai.transforms import ResizeWithPadOrCrop\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from monai.transforms import NormalizeIntensity\n",
    "import torch\n",
    "from nnunet.lib.training_utils import read_config\n",
    "from nnunet.lib.utils import ConvBlocks2DGroup\n",
    "from nnunet.lib.training_utils import build_2d_model\n",
    "from nnunet.training.network_training.processor import Processor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\binary_lib\"\n",
    "cropper_config = read_config(os.path.join(Path.cwd(), 'adversarial_acdc.yaml'), False, False)\n",
    "\n",
    "cropping_conv_layer = ConvBlocks2DGroup\n",
    "cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=384, window_size=8, middle=False, num_classes=2, processor=None)\n",
    "cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "cropping_network.eval()\n",
    "cropping_network.do_ds = False\n",
    "\n",
    "processor = Processor(crop_size=192, image_size=384, cropping_network=cropping_network)\n",
    "\n",
    "output_folder = 'Lib_no_resampling'\n",
    "output_folder_gt = 'Lib_no_resampling_gt'\n",
    "\n",
    "delete_if_exist(output_folder)\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "delete_if_exist(output_folder_gt)\n",
    "Path(output_folder_gt).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_list = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task045_Lib\\custom_experiment_planner_stage0\", '*.npz')))\n",
    "path_list_pkl = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task045_Lib\\custom_experiment_planner_stage0\", '*.pkl')))\n",
    "#path_list = [x for x in path_list if '_u' not in os.path.basename(x)[:-4]]\n",
    "\n",
    "assert len(path_list) == len(path_list_pkl)\n",
    "cropper = ResizeWithPadOrCrop(spatial_size=(384, 384, -1))\n",
    "\n",
    "patient_list = sorted(list(set([os.path.basename(x).split('_')[0] for x in path_list])))\n",
    "\n",
    "all_patient_paths = []\n",
    "all_patient_paths_pkl = []\n",
    "for patient in patient_list:\n",
    "    patient_files = []\n",
    "    patient_files_pkl = []\n",
    "    for (path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        if patient in path:\n",
    "            patient_files.append(path)\n",
    "        if patient in pkl_path:\n",
    "            patient_files_pkl.append(pkl_path)\n",
    "    all_patient_paths.append(sorted(patient_files))\n",
    "    all_patient_paths_pkl.append(sorted(patient_files_pkl))\n",
    "\n",
    "for (path_list, path_list_pkl) in tqdm(zip(all_patient_paths, all_patient_paths_pkl), total=len(all_patient_paths)):\n",
    "    sequence = []\n",
    "    filename_list = []\n",
    "    for (npz_path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        filename = os.path.basename(npz_path)[:-4] + '.nii.gz'\n",
    "        data = np.load(npz_path)\n",
    "        arr = data['data']\n",
    "        arr = arr.transpose((0, 2, 3, 1))\n",
    "\n",
    "        initial_shape = list(arr.shape[1:])\n",
    "\n",
    "        cropped = cropper(arr)\n",
    "        if cropped.shape[0] == 1:\n",
    "            cropped = np.tile(cropped, [2, 1, 1, 1])\n",
    "        sequence.append(cropped)\n",
    "        filename_list.append(filename)\n",
    "    sequence = np.stack(sequence, axis=0) # T, C, H, W, D\n",
    "    sequence = sequence.transpose(4, 0, 1, 2, 3) # D, T, C, H, W\n",
    "\n",
    "    block = []\n",
    "    padding_need_list = []\n",
    "    for d in range(len(sequence)):\n",
    "\n",
    "        unlabeled = torch.from_numpy(sequence[d]).to('cuda:0')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_centroid, _ = processor.preprocess_no_registration(data=torch.clone(unlabeled[:, :1])) # T, C(1), H, W\n",
    "\n",
    "            cropped_unlabeled, padding_need = processor.crop_and_pad(data=unlabeled, mean_centroid=mean_centroid)\n",
    "            cropped_unlabeled = cropped_unlabeled.cpu()\n",
    "            padding_need = padding_need.cpu()\n",
    "            padding_need_list.append(padding_need)\n",
    "\n",
    "        block.append(cropped_unlabeled)\n",
    "    block = np.stack(block, axis=-1) # T, C, H, W, D\n",
    "    padding_need = np.stack(padding_need_list, axis=-1) # 4, D\n",
    "    \n",
    "    #divisor = cropped[0].max() - cropped[0].min()\n",
    "    #subtrahend = cropped[0].min()\n",
    "    #cropped_img = NormalizeIntensity(subtrahend=subtrahend, divisor=divisor)(cropped[0])\n",
    "\n",
    "    for t in range(len(block)):\n",
    "        current_volume = block[t]\n",
    "        current_filename = filename_list[t]\n",
    "\n",
    "        grayscale = current_volume[0]\n",
    "        grayscale_flattened = grayscale.reshape(-1, current_volume[0].shape[-1])\n",
    "\n",
    "        if np.any((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0))) == 0:\n",
    "            print(current_filename)\n",
    "            print((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0)))\n",
    "\n",
    "        grayscale_flattened = (grayscale_flattened - np.min(grayscale_flattened, axis=0)) / (np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0) + 1e-8)\n",
    "        grayscale = grayscale_flattened.reshape(current_volume[0].shape)\n",
    "\n",
    "        #if t == 36 and '11' in path_list[0]:\n",
    "        #    fig, ax = plt.subplots(1, 1)\n",
    "        #    ax.imshow(grayscale[:, :, 0], cmap='gray')\n",
    "        #    plt.show()\n",
    "        #    plt.waitforbuttonpress()\n",
    "        #    plt.close(fig)\n",
    "\n",
    "        nib.save(nib.Nifti1Image(grayscale, affine=np.eye(4)), os.path.join(output_folder, current_filename))\n",
    "        if '_u' not in current_filename:\n",
    "            cropped_gt = current_volume[1]\n",
    "            cropped_gt[cropped_gt < 0] = 0\n",
    "            nib.save(nib.Nifti1Image(cropped_gt, affine=np.eye(4)), os.path.join(output_folder_gt, current_filename))\n",
    "        \n",
    "        with open(path_list_pkl[t], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data['padding_need'] = padding_need\n",
    "            data['voxelmorph_size_before'] = initial_shape\n",
    "\n",
    "            with open(os.path.join(output_folder, os.path.basename(path_list_pkl[t])), 'wb') as f:\n",
    "                pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2D Lib testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [10:24<00:00, 12.25s/it]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from monai.transforms import ResizeWithPadOrCrop\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from monai.transforms import NormalizeIntensity\n",
    "import torch\n",
    "from nnunet.lib.training_utils import read_config\n",
    "from nnunet.lib.utils import ConvBlocks2DGroup\n",
    "from nnunet.lib.training_utils import build_2d_model\n",
    "from nnunet.training.network_training.processor import Processor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\binary_lib\"\n",
    "cropper_config = read_config(os.path.join(Path.cwd(), 'adversarial_acdc.yaml'), False, False)\n",
    "\n",
    "cropping_conv_layer = ConvBlocks2DGroup\n",
    "cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=384, window_size=8, middle=False, num_classes=2, processor=None)\n",
    "cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "cropping_network.eval()\n",
    "cropping_network.do_ds = False\n",
    "\n",
    "processor = Processor(crop_size=192, image_size=384, cropping_network=cropping_network)\n",
    "\n",
    "output_folder = 'Lib_no_resampling_testing'\n",
    "output_folder_gt = 'Lib_no_resampling_gt_testing'\n",
    "\n",
    "delete_if_exist(output_folder)\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "delete_if_exist(output_folder_gt)\n",
    "Path(output_folder_gt).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_list = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task046_Lib\\custom_experiment_planner_stage0\", '*.npz')))\n",
    "path_list_pkl = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task046_Lib\\custom_experiment_planner_stage0\", '*.pkl')))\n",
    "#path_list = [x for x in path_list if '_u' not in os.path.basename(x)[:-4]]\n",
    "\n",
    "assert len(path_list) == len(path_list_pkl)\n",
    "cropper = ResizeWithPadOrCrop(spatial_size=(384, 384, -1))\n",
    "\n",
    "patient_list = sorted(list(set([os.path.basename(x).split('_')[0] for x in path_list])))\n",
    "\n",
    "all_patient_paths = []\n",
    "all_patient_paths_pkl = []\n",
    "for patient in patient_list:\n",
    "    patient_files = []\n",
    "    patient_files_pkl = []\n",
    "    for (path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        if patient in path:\n",
    "            patient_files.append(path)\n",
    "        if patient in pkl_path:\n",
    "            patient_files_pkl.append(pkl_path)\n",
    "    all_patient_paths.append(sorted(patient_files))\n",
    "    all_patient_paths_pkl.append(sorted(patient_files_pkl))\n",
    "\n",
    "for (path_list, path_list_pkl) in tqdm(zip(all_patient_paths, all_patient_paths_pkl), total=len(all_patient_paths)):\n",
    "    sequence = []\n",
    "    filename_list = []\n",
    "    for (npz_path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        filename = os.path.basename(npz_path)[:-4] + '.nii.gz'\n",
    "        data = np.load(npz_path)\n",
    "        arr = data['data']\n",
    "        arr = arr.transpose((0, 2, 3, 1))\n",
    "\n",
    "        initial_shape = list(arr.shape[1:])\n",
    "\n",
    "        cropped = cropper(arr)\n",
    "        if cropped.shape[0] == 1:\n",
    "            cropped = np.tile(cropped, [2, 1, 1, 1])\n",
    "        sequence.append(cropped)\n",
    "        filename_list.append(filename)\n",
    "    sequence = np.stack(sequence, axis=0) # T, C, H, W, D\n",
    "    sequence = sequence.transpose(4, 0, 1, 2, 3) # D, T, C, H, W\n",
    "\n",
    "    block = []\n",
    "    padding_need_list = []\n",
    "    for d in range(len(sequence)):\n",
    "\n",
    "        unlabeled = torch.from_numpy(sequence[d]).to('cuda:0')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_centroid, _ = processor.preprocess_no_registration(data=torch.clone(unlabeled[:, :1])) # T, C(1), H, W\n",
    "\n",
    "            cropped_unlabeled, padding_need = processor.crop_and_pad(data=unlabeled, mean_centroid=mean_centroid)\n",
    "            cropped_unlabeled = cropped_unlabeled.cpu()\n",
    "            padding_need = padding_need.cpu()\n",
    "            padding_need_list.append(padding_need)\n",
    "\n",
    "        block.append(cropped_unlabeled)\n",
    "    block = np.stack(block, axis=-1) # T, C, H, W, D\n",
    "    padding_need = np.stack(padding_need_list, axis=-1) # 4, D\n",
    "    \n",
    "    #divisor = cropped[0].max() - cropped[0].min()\n",
    "    #subtrahend = cropped[0].min()\n",
    "    #cropped_img = NormalizeIntensity(subtrahend=subtrahend, divisor=divisor)(cropped[0])\n",
    "\n",
    "    for t in range(len(block)):\n",
    "        current_volume = block[t]\n",
    "        current_filename = filename_list[t]\n",
    "\n",
    "        grayscale = current_volume[0]\n",
    "        grayscale_flattened = grayscale.reshape(-1, current_volume[0].shape[-1])\n",
    "\n",
    "        if np.any((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0))) == 0:\n",
    "            print(current_filename)\n",
    "            print((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0)))\n",
    "\n",
    "        grayscale_flattened = (grayscale_flattened - np.min(grayscale_flattened, axis=0)) / (np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0) + 1e-8)\n",
    "        grayscale = grayscale_flattened.reshape(current_volume[0].shape)\n",
    "\n",
    "        #if t == 36 and '11' in path_list[0]:\n",
    "        #    fig, ax = plt.subplots(1, 1)\n",
    "        #    ax.imshow(grayscale[:, :, 0], cmap='gray')\n",
    "        #    plt.show()\n",
    "        #    plt.waitforbuttonpress()\n",
    "        #    plt.close(fig)\n",
    "\n",
    "        nib.save(nib.Nifti1Image(grayscale, affine=np.eye(4)), os.path.join(output_folder, current_filename))\n",
    "        if '_u' not in current_filename:\n",
    "            cropped_gt = current_volume[1]\n",
    "            cropped_gt[cropped_gt < 0] = 0\n",
    "            nib.save(nib.Nifti1Image(cropped_gt, affine=np.eye(4)), os.path.join(output_folder_gt, current_filename))\n",
    "        \n",
    "        with open(path_list_pkl[t], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data['padding_need'] = padding_need\n",
    "            data['voxelmorph_size_before'] = initial_shape\n",
    "\n",
    "            with open(os.path.join(output_folder, os.path.basename(path_list_pkl[t])), 'wb') as f:\n",
    "                pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
