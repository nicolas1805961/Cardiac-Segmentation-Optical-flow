{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [1:15:07<00:00, 20.49s/it]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from monai.transforms import ResizeWithPadOrCrop\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from monai.transforms import NormalizeIntensity\n",
    "import torch\n",
    "from nnunet.lib.training_utils import read_config\n",
    "from nnunet.lib.utils import ConvBlocks2DGroup\n",
    "from nnunet.lib.training_utils import build_2d_model\n",
    "from nnunet.training.network_training.processor2 import Processor2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "#cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\binary_lib\"\n",
    "#cropper_config = read_config(os.path.join(Path.cwd(), 'adversarial_acdc.yaml'), False, False)\n",
    "#\n",
    "#cropping_conv_layer = ConvBlocks2DGroup\n",
    "#cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=384, window_size=8, middle=False, num_classes=2, processor=None)\n",
    "#cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "#cropping_network.eval()\n",
    "#cropping_network.do_ds = False\n",
    "\n",
    "cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\Quorum_cardioTrack_all_phases\"\n",
    "cropper_config = read_config(os.path.join(cropper_weights_folder_path, 'config.yaml'), False, False)\n",
    "\n",
    "cropping_conv_layer = ConvBlocks2DGroup\n",
    "cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=384, window_size=8, middle=False, num_classes=4, processor=None)\n",
    "cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "cropping_network.eval()\n",
    "cropping_network.do_ds = False\n",
    "\n",
    "processor = Processor2(crop_size=192, image_size=384, cropping_network=cropping_network)\n",
    "\n",
    "pretrained_config = read_config(os.path.join(Path.cwd(), '2d_cardiotrack_crop_normalized_2', 'config.yaml'), False, False)\n",
    "pretrained_conv_layer = ConvBlocks2DGroup\n",
    "pretrained_network = build_2d_model(pretrained_config, conv_layer=pretrained_conv_layer, norm=getattr(torch.nn, pretrained_config['norm']), log_function=print, image_size=192, window_size=8, middle=False, num_classes=4, processor=None)\n",
    "pretrained_network.load_state_dict(torch.load(os.path.join('2d_cardiotrack_crop_normalized_2', 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "pretrained_network.eval()\n",
    "pretrained_network.do_ds = False\n",
    "\n",
    "output_folder = 'Lib_resampling_training_mask_points'\n",
    "\n",
    "rv_folder = os.path.join(output_folder, 'RV')\n",
    "delete_if_exist(rv_folder)\n",
    "Path(rv_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lv_folder = os.path.join(output_folder, 'LV')\n",
    "delete_if_exist(lv_folder)\n",
    "Path(lv_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_list = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task032_Lib\\custom_experiment_planner_stage0\", '*.npz')))\n",
    "path_list_pkl = sorted(glob(os.path.join(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task032_Lib\\custom_experiment_planner_stage0\", '*.pkl')))\n",
    "#path_list = [x for x in path_list if '_u' not in os.path.basename(x)[:-4]]\n",
    "\n",
    "assert len(path_list) == len(path_list_pkl)\n",
    "cropper = ResizeWithPadOrCrop(spatial_size=(384, 384, -1))\n",
    "\n",
    "patient_list = sorted(list(set([os.path.basename(x).split('_')[0] for x in path_list])))\n",
    "\n",
    "all_patient_paths = []\n",
    "all_patient_paths_pkl = []\n",
    "for patient in patient_list:\n",
    "    patient_files = []\n",
    "    patient_files_pkl = []\n",
    "    for (path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        if patient in path:\n",
    "            patient_files.append(path)\n",
    "        if patient in pkl_path:\n",
    "            patient_files_pkl.append(pkl_path)\n",
    "    all_patient_paths.append(sorted(patient_files))\n",
    "    all_patient_paths_pkl.append(sorted(patient_files_pkl))\n",
    "\n",
    "for (path_list, path_list_pkl) in tqdm(zip(all_patient_paths, all_patient_paths_pkl), total=len(all_patient_paths)):\n",
    "    sequence = []\n",
    "    filename_list = []\n",
    "    pad_width_list = []\n",
    "    for (npz_path, pkl_path) in zip(path_list, path_list_pkl):\n",
    "        basename = os.path.basename(npz_path)\n",
    "        #print(basename)\n",
    "\n",
    "        filename = os.path.basename(npz_path)[:-4] + '.nii.gz'\n",
    "        data = np.load(npz_path)\n",
    "        arr = data['data']\n",
    "        arr = arr.transpose((0, 2, 3, 1))\n",
    "\n",
    "        initial_shape = list(arr.shape[1:])\n",
    "        #print(arr.shape)\n",
    "\n",
    "        pad_width = []\n",
    "        for i, sp_i in enumerate([384, 384]):\n",
    "            width = sp_i - initial_shape[i]\n",
    "            pad_width.append((int(width // 2), int(width - (width // 2))))\n",
    "\n",
    "        pad_vertical = 384 - initial_shape[0]\n",
    "        pad_horizontal = 384 - initial_shape[1]\n",
    "\n",
    "        cropped = cropper(arr)\n",
    "        if cropped.shape[0] == 1:\n",
    "            cropped = np.tile(cropped, [2, 1, 1, 1])\n",
    "        sequence.append(cropped)\n",
    "        filename_list.append(filename)\n",
    "    sequence = np.stack(sequence, axis=0) # T, C, H, W, D\n",
    "    sequence = sequence.transpose(4, 0, 1, 2, 3) # D, T, C, H, W\n",
    "\n",
    "    sequence[:, :, 1][sequence[:, :, 1] < 0] = 0\n",
    "\n",
    "    patient_name = basename.split('_')[0]\n",
    "    point_path_list_lv = sorted(glob(os.path.join(r'custom_lib_t_4', patient_name, 'contour', 'LV', '*.npy')))\n",
    "    point_path_list_rv = sorted(glob(os.path.join(r'custom_lib_t_4', patient_name, 'contour', 'RV', '*.npy')))\n",
    "    assert len(point_path_list_lv) == len(point_path_list_rv) == len(sequence)\n",
    "\n",
    "    with open(path_list_pkl[0], 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        #print(data.keys())\n",
    "        shape_original_after_cropping = data.get('size_after_cropping')\n",
    "        size_after_resampling = data.get('size_after_resampling')\n",
    "        #print(data.get('size_after_resampling'))\n",
    "        #print(shape_original_after_cropping)\n",
    "        rescale_y = size_after_resampling[1] / shape_original_after_cropping[1]\n",
    "        rescale_x = size_after_resampling[2] / shape_original_after_cropping[2]\n",
    "        rescale_factor = np.array([rescale_y, rescale_x])\n",
    "        #print(rescale_y)\n",
    "        #print(rescale_x)\n",
    "        #print(rescale_factor.shape)\n",
    "        #print(data.get('original_size_of_raw_data'))\n",
    "        bbox = data.get('crop_bbox')\n",
    "        #print(bbox)\n",
    "        #print(pad_width)\n",
    "\n",
    "    block = []\n",
    "    padding_need_list = []\n",
    "    for d in range(len(sequence)):\n",
    "\n",
    "        lv_points = np.load(point_path_list_lv[d]) - 1\n",
    "        rv_points = np.load(point_path_list_rv[d]) - 1\n",
    "\n",
    "        # cropped data fixing\n",
    "        rv_points[0] = rv_points[0] - bbox[1][0]\n",
    "        rv_points[1] = rv_points[1] - bbox[2][0]\n",
    "\n",
    "        lv_points[0] = lv_points[0] - bbox[1][0]\n",
    "        lv_points[1] = lv_points[1] - bbox[2][0]\n",
    "        lv_points[2] = lv_points[2] - bbox[1][0]\n",
    "        lv_points[3] = lv_points[3] - bbox[2][0]\n",
    "\n",
    "        # resampling fixing\n",
    "        rv_points[0] = rv_points[0] * rescale_y\n",
    "        rv_points[1] = rv_points[1] * rescale_x\n",
    "\n",
    "        lv_points[0] = lv_points[0] * rescale_y\n",
    "        lv_points[1] = lv_points[1] * rescale_x\n",
    "        lv_points[2] = lv_points[2] * rescale_y\n",
    "        lv_points[3] = lv_points[3] * rescale_x\n",
    "\n",
    "        unlabeled = torch.from_numpy(sequence[d])\n",
    "\n",
    "\n",
    "        #fig, ax = plt.subplots(1, 1)\n",
    "        #ax.imshow(arr[0, :, :, d], cmap='gray')\n",
    "        #ax.scatter(rv_points[1, :, 0], rv_points[0, :, 0])\n",
    "        #ax.scatter(lv_points[1, :, 0], lv_points[0, :, 0])\n",
    "        #ax.scatter(lv_points[3, :, 0], lv_points[2, :, 0])\n",
    "        #while not plt.waitforbuttonpress(): pass\n",
    "        #plt.close(fig)\n",
    "\n",
    "        rv_points[0] = rv_points[0] + pad_width[0][0]\n",
    "        rv_points[1] = rv_points[1] + pad_width[1][0]\n",
    "\n",
    "        lv_points[0] = lv_points[0] + pad_width[0][0]\n",
    "        lv_points[1] = lv_points[1] + pad_width[1][0]\n",
    "        lv_points[2] = lv_points[2] + pad_width[0][0]\n",
    "        lv_points[3] = lv_points[3] + pad_width[1][0]\n",
    "        \n",
    "        #fig, ax = plt.subplots(1, 1)\n",
    "        #ax.imshow(unlabeled[0, 0], cmap='gray')\n",
    "        #ax.scatter(rv_points[1, :, 0], rv_points[0, :, 0])\n",
    "        #ax.scatter(lv_points[1, :, 0], lv_points[0, :, 0])\n",
    "        #ax.scatter(lv_points[3, :, 0], lv_points[2, :, 0])\n",
    "        #while not plt.waitforbuttonpress(): pass\n",
    "        #plt.close(fig)\n",
    "\n",
    "        #if unlabeled[0, 0].std() > 1000:\n",
    "        #    print(unlabeled[0, 0].std())\n",
    "        #    continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_centroid, _ = processor.preprocess_no_registration(data=torch.clone(unlabeled[:, :1]).to('cuda:0')) # T, C(1), H, W\n",
    "\n",
    "            #strain_mask = processor.get_strain_mask_3(torch.clone(unlabeled[:, 1]), path_list[0])\n",
    "            #strain_mask = torch.from_numpy(strain_mask)\n",
    "            #strain_mask = strain_mask.permute(3, 0, 1, 2)\n",
    "            #unlabeled = torch.cat([unlabeled, strain_mask], dim=1)\n",
    "            cropped_unlabeled, padding_need = processor.crop_and_pad(data=unlabeled, mean_centroid=mean_centroid)\n",
    "            padding_need = padding_need.numpy()\n",
    "\n",
    "            rv_points[0] = rv_points[0] - padding_need[2]\n",
    "            rv_points[1] = rv_points[1] - padding_need[0]\n",
    "\n",
    "            lv_points[0] = lv_points[0] - padding_need[2]\n",
    "            lv_points[1] = lv_points[1] - padding_need[0]\n",
    "            lv_points[2] = lv_points[2] - padding_need[2]\n",
    "            lv_points[3] = lv_points[3] - padding_need[0]\n",
    "\n",
    "            #fig, ax = plt.subplots(1, 1)\n",
    "            #ax.imshow(cropped_unlabeled[0, 0], cmap='gray')\n",
    "            #ax.scatter(rv_points[1, :, 0], rv_points[0, :, 0])\n",
    "            #ax.scatter(lv_points[1, :, 0], lv_points[0, :, 0])\n",
    "            #ax.scatter(lv_points[3, :, 0], lv_points[2, :, 0])\n",
    "            #while not plt.waitforbuttonpress(): pass\n",
    "            #plt.close(fig)\n",
    "\n",
    "            np.save(os.path.join(lv_folder, patient_name + '_slice' + str(d+1).zfill(2) + '.npy'), lv_points)\n",
    "            np.save(os.path.join(rv_folder, patient_name + '_slice' + str(d+1).zfill(2) + '.npy'), rv_points)\n",
    "\n",
    "        #np.save(os.path.join(output_folder, current_filename[:-7] + '.npy'), out)\n",
    "        #nib.save(nib.Nifti1Image(out_block, affine=np.eye(4)), os.path.join(output_folder_mask, current_filename))\n",
    "        \n",
    "        #with open(path_list_pkl[t], 'rb') as f:\n",
    "        #    data = pickle.load(f)\n",
    "        #    data['padding_need'] = padding_need\n",
    "        #    data['voxelmorph_size_before'] = initial_shape\n",
    "#\n",
    "        #    with open(os.path.join(output_folder, os.path.basename(path_list_pkl[t])), 'wb') as f:\n",
    "        #        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = glob(os.path.join(r'custom_lib_t_4', '**', '*.gz'), recursive=True)\n",
    "for path in path_list:\n",
    "    print(path)\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/271 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/271 [00:49<3:44:10, 49.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/271 [01:11<2:30:03, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/271 [01:32<2:04:07, 27.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/271 [02:25<2:47:52, 37.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/271 [02:53<2:30:53, 34.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/271 [03:20<2:20:22, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7/271 [03:47<2:13:12, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/271 [04:41<2:44:49, 37.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9/271 [05:25<2:53:05, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 10/271 [06:18<3:10:02, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 11/271 [06:46<2:49:18, 39.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/271 [07:14<2:33:53, 35.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 13/271 [08:07<2:56:32, 41.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 14/271 [09:10<3:23:17, 47.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 15/271 [09:37<2:56:18, 41.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 16/271 [10:22<3:00:02, 42.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_lib_t_4\\patient017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 16/271 [10:36<2:48:59, 39.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28840/302751892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mmean_centroid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_no_registration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# T, C(1), H, W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mstrain_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strain_mask_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\training\\network_training\\processor2.py\u001b[0m in \u001b[0;36mpreprocess_no_registration\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;34m'''data: T, 1, H, W'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mtemp_volume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# T, H, W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mone_hot_volume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_volume\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# T, 4, H, W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\training\\network_training\\processor2.py\u001b[0m in \u001b[0;36mdiscretize\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mcurrent_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# B(1), 1, H, W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m                 \u001b[0msoftmaxed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from monai.transforms import ResizeWithPadOrCrop\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from monai.transforms import NormalizeIntensity\n",
    "import torch\n",
    "from nnunet.lib.training_utils import read_config\n",
    "from nnunet.lib.utils import ConvBlocks2DGroup\n",
    "from nnunet.lib.training_utils import build_2d_model\n",
    "from nnunet.training.network_training.processor2 import Processor2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "#cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\binary_lib\"\n",
    "#cropper_config = read_config(os.path.join(Path.cwd(), 'adversarial_acdc.yaml'), False, False)\n",
    "#\n",
    "#cropping_conv_layer = ConvBlocks2DGroup\n",
    "#cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=384, window_size=8, middle=False, num_classes=2, processor=None)\n",
    "#cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "#cropping_network.eval()\n",
    "#cropping_network.do_ds = False\n",
    "\n",
    "cropper_weights_folder_path = r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\Quorum_cardioTrack_all_phases\"\n",
    "cropper_config = read_config(os.path.join(cropper_weights_folder_path, 'config.yaml'), False, False)\n",
    "\n",
    "cropping_conv_layer = ConvBlocks2DGroup\n",
    "cropping_network = build_2d_model(cropper_config, conv_layer=cropping_conv_layer, norm=getattr(torch.nn, cropper_config['norm']), log_function=None, image_size=384, window_size=8, middle=False, num_classes=4, processor=None)\n",
    "cropping_network.load_state_dict(torch.load(os.path.join(cropper_weights_folder_path, 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "cropping_network.eval()\n",
    "cropping_network.do_ds = False\n",
    "\n",
    "processor = Processor2(crop_size=192, image_size=384, cropping_network=cropping_network)\n",
    "\n",
    "pretrained_config = read_config(os.path.join(Path.cwd(), '2d_cardiotrack_crop_normalized_2', 'config.yaml'), False, False)\n",
    "pretrained_conv_layer = ConvBlocks2DGroup\n",
    "pretrained_network = build_2d_model(pretrained_config, conv_layer=pretrained_conv_layer, norm=getattr(torch.nn, pretrained_config['norm']), log_function=print, image_size=192, window_size=8, middle=False, num_classes=4, processor=None)\n",
    "pretrained_network.load_state_dict(torch.load(os.path.join('2d_cardiotrack_crop_normalized_2', 'model_final_checkpoint.model'))['state_dict'], strict=True)\n",
    "pretrained_network.eval()\n",
    "pretrained_network.do_ds = False\n",
    "\n",
    "cropper = ResizeWithPadOrCrop(spatial_size=(384, 384, -1))\n",
    "\n",
    "output_folder = 'Lib_resampling_mask_points'\n",
    "delete_if_exist(output_folder)\n",
    "\n",
    "rv_folder = os.path.join(output_folder, 'RV')\n",
    "Path(rv_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lv_folder = os.path.join(output_folder, 'LV')\n",
    "Path(lv_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "patient_list = glob(os.path.join(r'custom_lib_t_4', '*'))\n",
    "patient_list = [x for x in patient_list if 'cfg' not in x]\n",
    "\n",
    "for patient in tqdm(patient_list):\n",
    "    print(patient)\n",
    "    nifti_list = sorted(glob(os.path.join(patient, '*.gz')))\n",
    "    img_list = [x for x in nifti_list if 'gt' not in x]\n",
    "    gt_list = [x for x in nifti_list if 'gt' in x]\n",
    "    pkl_list = sorted(glob(os.path.join(patient, '*.pkl')))\n",
    "\n",
    "    sequence = []\n",
    "    filename_list = []\n",
    "    pad_width_list = []\n",
    "    for img_path, gt_path in zip(img_list, gt_list):\n",
    "\n",
    "        data_img = nib.load(img_path)\n",
    "        data_gt = nib.load(gt_path)\n",
    "\n",
    "        img = data_img.get_fdata()\n",
    "        gt = data_gt.get_fdata()\n",
    "\n",
    "        arr = np.stack([img, gt], axis=0)\n",
    "\n",
    "        initial_shape = list(arr.shape[1:])\n",
    "\n",
    "        pad_width = []\n",
    "        for i, sp_i in enumerate([384, 384]):\n",
    "            width = sp_i - initial_shape[i]\n",
    "            pad_width.append((int(width // 2), int(width - (width // 2))))\n",
    "\n",
    "        pad_vertical = 384 - initial_shape[0]\n",
    "        pad_horizontal = 384 - initial_shape[1]\n",
    "\n",
    "        cropped = cropper(arr)\n",
    "        if cropped.shape[0] == 1:\n",
    "            cropped = np.tile(cropped, [2, 1, 1, 1])\n",
    "        sequence.append(cropped)\n",
    "        filename_list.append(os.path.basename(img_path))\n",
    "    sequence = np.stack(sequence, axis=0) # T, C, H, W, D\n",
    "    sequence = sequence.transpose(4, 0, 1, 2, 3) # D, T, C, H, W\n",
    "\n",
    "    sequence[:, :, 1][sequence[:, :, 1] < 0] = 0\n",
    "\n",
    "    patient_name = os.path.basename(img_path).split('_')[0]\n",
    "    point_path_list_lv = sorted(glob(os.path.join(r'custom_lib_t_4', patient_name, 'contour', 'LV', '*.npy')))\n",
    "    point_path_list_rv = sorted(glob(os.path.join(r'custom_lib_t_4', patient_name, 'contour', 'RV', '*.npy')))\n",
    "    assert len(point_path_list_lv) == len(point_path_list_rv) == len(sequence)\n",
    "    #print(pad_width)\n",
    "\n",
    "    block = []\n",
    "    padding_need_list = []\n",
    "    for d in range(len(sequence)):\n",
    "\n",
    "        lv_points = np.load(point_path_list_lv[d]) - 1\n",
    "        rv_points = np.load(point_path_list_rv[d]) - 1\n",
    "\n",
    "        unlabeled = torch.from_numpy(sequence[d]).float()\n",
    "\n",
    "        #fig, ax = plt.subplots(1, 1)\n",
    "        #ax.imshow(arr[0, :, :, d], cmap='gray')\n",
    "        #ax.scatter(rv_points[0, :, 0], rv_points[1, :, 0])\n",
    "        #ax.scatter(lv_points[0, :, 0], lv_points[1, :, 0])\n",
    "        #ax.scatter(lv_points[2, :, 0], lv_points[3, :, 0])\n",
    "        #while not plt.waitforbuttonpress(): pass\n",
    "        #plt.close(fig)\n",
    "\n",
    "        rv_points[0] = rv_points[0] + pad_width[1][0]\n",
    "        rv_points[1] = rv_points[1] + pad_width[0][0]\n",
    "\n",
    "        lv_points[0] = lv_points[0] + pad_width[1][0]\n",
    "        lv_points[1] = lv_points[1] + pad_width[0][0]\n",
    "        lv_points[2] = lv_points[2] + pad_width[1][0]\n",
    "        lv_points[3] = lv_points[3] + pad_width[0][0]\n",
    "\n",
    "        #fig, ax = plt.subplots(1, 1)\n",
    "        #ax.imshow(unlabeled[0, 0], cmap='gray')\n",
    "        #ax.scatter(rv_points[0, :, 0], rv_points[1, :, 0])\n",
    "        #ax.scatter(lv_points[0, :, 0], lv_points[1, :, 0])\n",
    "        #ax.scatter(lv_points[2, :, 0], lv_points[3, :, 0])\n",
    "        #while not plt.waitforbuttonpress(): pass\n",
    "        #plt.close(fig)\n",
    "\n",
    "        #if unlabeled[0, 0].std() > 1000:\n",
    "        #    print(unlabeled[0, 0].std())\n",
    "        #    continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_centroid, _ = processor.preprocess_no_registration(data=torch.clone(unlabeled[:, :1]).to('cuda:0')) # T, C(1), H, W\n",
    "\n",
    "            strain_mask = processor.get_strain_mask_3(torch.clone(unlabeled[:, 1]), img_list[0])\n",
    "            strain_mask = torch.from_numpy(strain_mask)\n",
    "            strain_mask = strain_mask.permute(3, 0, 1, 2)\n",
    "            unlabeled = torch.cat([unlabeled, strain_mask], dim=1)\n",
    "            cropped_unlabeled, padding_need = processor.crop_and_pad(data=unlabeled, mean_centroid=mean_centroid)\n",
    "            cropped_unlabeled = cropped_unlabeled.cpu()\n",
    "            padding_need = padding_need.cpu().numpy()\n",
    "            padding_need_list.append(padding_need)\n",
    "\n",
    "            rv_points[0] = rv_points[0] - padding_need[0]\n",
    "            rv_points[1] = rv_points[1] - padding_need[2]\n",
    "\n",
    "            lv_points[0] = lv_points[0] - padding_need[0]\n",
    "            lv_points[1] = lv_points[1] - padding_need[2]\n",
    "            lv_points[2] = lv_points[2] - padding_need[0]\n",
    "            lv_points[3] = lv_points[3] - padding_need[2]\n",
    "\n",
    "            #fig, ax = plt.subplots(1, 1)\n",
    "            #ax.imshow(cropped_unlabeled[0, 0], cmap='gray')\n",
    "            #ax.scatter(rv_points[0, :, 0], rv_points[1, :, 0])\n",
    "            #ax.scatter(lv_points[0, :, 0], lv_points[1, :, 0])\n",
    "            #ax.scatter(lv_points[2, :, 0], lv_points[3, :, 0])\n",
    "            #while not plt.waitforbuttonpress(): pass\n",
    "            #plt.close(fig)\n",
    "\n",
    "            np.save(os.path.join(lv_folder, patient_name + '_slice' + str(d+1).zfill(2) + '.npy'), lv_points)\n",
    "            np.save(os.path.join(rv_folder, patient_name + '_slice' + str(d+1).zfill(2) + '.npy'), rv_points)\n",
    "\n",
    "        block.append(cropped_unlabeled)\n",
    "    block = np.stack(block, axis=-1) # T, C, H, W, D\n",
    "    padding_need = np.stack(padding_need_list, axis=-1) # 4, D\n",
    "    \n",
    "    #divisor = cropped[0].max() - cropped[0].min()\n",
    "    #subtrahend = cropped[0].min()\n",
    "    #cropped_img = NormalizeIntensity(subtrahend=subtrahend, divisor=divisor)(cropped[0])\n",
    "\n",
    "    for t in range(len(block)):\n",
    "        current_volume = block[t]\n",
    "        current_filename = filename_list[t]\n",
    "\n",
    "        grayscale = current_volume[0]\n",
    "        grayscale_flattened = grayscale.reshape(-1, current_volume[0].shape[-1])\n",
    "\n",
    "        if np.any((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0))) == 0:\n",
    "            print(current_filename)\n",
    "            print((np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0)))\n",
    "\n",
    "        grayscale_flattened = (grayscale_flattened - np.min(grayscale_flattened, axis=0)) / (np.max(grayscale_flattened, axis=0) - np.min(grayscale_flattened, axis=0) + 1e-8)\n",
    "        grayscale = grayscale_flattened.reshape(current_volume[0].shape)\n",
    "\n",
    "        #if t == 36 and '11' in path_list[0]:\n",
    "        #    fig, ax = plt.subplots(1, 1)\n",
    "        #    ax.imshow(grayscale[:, :, 0], cmap='gray')\n",
    "        #    plt.show()\n",
    "        #    plt.waitforbuttonpress()\n",
    "        #    plt.close(fig)\n",
    "\n",
    "        cropped_gt = current_volume[1]\n",
    "        cropped_gt[cropped_gt < 0] = 0\n",
    "\n",
    "        out = np.concatenate([grayscale[None], cropped_gt[None], current_volume[2:]], axis=0)\n",
    "        #fig, ax = plt.subplots(1,3)\n",
    "        #ax[0].imshow(current_volume[2, :, :, 1], cmap='gray')\n",
    "        #ax[1].imshow(current_volume[3, :, :, 1], cmap='gray')\n",
    "        #ax[2].imshow(current_volume[4, :, :, 1], cmap='gray')\n",
    "        #plt.show()\n",
    "        #plt.waitforbuttonpress()\n",
    "        #plt.close(fig)\n",
    "\n",
    "        np.save(os.path.join(output_folder, current_filename[:-7] + '.npy'), out)\n",
    "        #nib.save(nib.Nifti1Image(out_block, affine=np.eye(4)), os.path.join(output_folder_mask, current_filename))\n",
    "        \n",
    "        with open(pkl_list[t], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data['padding_need'] = padding_need\n",
    "            data['voxelmorph_size_before'] = initial_shape\n",
    "\n",
    "            with open(os.path.join(output_folder, current_filename[:-7] + '.pkl'), 'wb') as f:\n",
    "                pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
