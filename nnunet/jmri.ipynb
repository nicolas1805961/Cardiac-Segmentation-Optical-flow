{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results by criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_list = ['250005-003','250005-004','250005-007','276004-001','348003-002','703001-018','703003-010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [04:54<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 2)\n",
      "[285.61025641 267.34358974]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "shape_list = []\n",
    "path_list = glob('custom_quorum_RV_removed\\**\\*gt.nii.gz', recursive=True)\n",
    "for path in tqdm(path_list):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    myshape = arr.shape[:2]\n",
    "    shape_list.append(myshape)\n",
    "shape_list = np.stack(shape_list)\n",
    "print(shape_list.shape)\n",
    "print(shape_list.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import regionprops\n",
    "import io\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "names = [('patient020_frame11', 2), ('patient047_frame09', 4), ('patient085_frame09', 14), ('patient086_frame08', 5), ('patient085_frame01', 14), ('patient086_frame01', 2)]\n",
    "fig, ax = plt.subplots(len(names), 4, figsize=(7, 8))\n",
    "#fig, ax = plt.subplots(4, len(names), figsize=(8, 5))\n",
    "for idx, payload in enumerate(names):\n",
    "    name, index = payload\n",
    "    baseline_path_pred = os.path.join(r'ACDC_output\\Baseline\\cv_niftis_raw', name + '.nii.gz')\n",
    "    no_transformer_path_pred = os.path.join(r'ACDC_output\\no_transformer\\cv_niftis_raw', name + '.nii.gz')\n",
    "\n",
    "    path_gt = os.path.join(r'ACDC_output\\Baseline\\gt_niftis', name + '.nii.gz')\n",
    "    path_img = os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\imagesTr', name + '_0000.nii.gz')\n",
    "\n",
    "    data_baseline = nib.load(baseline_path_pred)\n",
    "    data_no_transformer = nib.load(no_transformer_path_pred)\n",
    "    data_gt = nib.load(path_gt)\n",
    "    data_img = nib.load(path_img)\n",
    "\n",
    "    baseline_pred = data_baseline.get_fdata()\n",
    "    no_transformer_pred = data_no_transformer.get_fdata()\n",
    "    gt = data_gt.get_fdata()\n",
    "    img = data_img.get_fdata()\n",
    "\n",
    "    label = np.logical_or(gt[:, :, index] > 0, baseline_pred[:, :, index] > 0)\n",
    "    label = np.logical_or(label, no_transformer_pred[:, :, index] > 0).astype(np.uint8)\n",
    "    regions = regionprops(label)\n",
    "    assert len(regions) == 1\n",
    "    min_row, min_col, max_row, max_col = regions[0].bbox\n",
    "\n",
    "    #for j in range(img.shape[-1]):\n",
    "    #    print(j)\n",
    "    #    fig, ax = plt.subplots(1, 4)\n",
    "    #    ax[0].imshow(img[:, :, j], cmap='gray')\n",
    "    #    ax[1].imshow(gt[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    ax[2].imshow(baseline_pred[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    ax[3].imshow(no_transformer_pred[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    plt.show()\n",
    "    #    plt.waitforbuttonpress()\n",
    "    #    plt.close(fig)\n",
    "    #print('************************************')\n",
    "\n",
    "    img = img[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    gt = gt[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    baseline_pred = baseline_pred[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    no_transformer_pred = no_transformer_pred[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    \n",
    "    ax[idx, 0].imshow(img, cmap='gray')\n",
    "    ax[idx, 0].axis('off')\n",
    "    ax[idx, 1].imshow(gt, cmap='gray', vmin=0, vmax=3)\n",
    "    ax[idx, 1].axis('off')\n",
    "    ax[idx, 2].imshow(baseline_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    ax[idx, 2].axis('off')\n",
    "    ax[idx, 3].imshow(no_transformer_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    ax[idx, 3].axis('off')\n",
    "\n",
    "    #ax[0, idx].imshow(img, cmap='gray')\n",
    "    #ax[0, idx].axis('off')\n",
    "    #ax[1, idx].imshow(gt, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[1, idx].axis('off')\n",
    "    #ax[2, idx].imshow(baseline_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[2, idx].axis('off')\n",
    "    #ax[3, idx].imshow(no_transformer_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[3, idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "#plt.savefig(\"visual_ablation_study_2.png\", dpi=600)\n",
    "\n",
    "png1 = io.BytesIO()\n",
    "plt.savefig(png1, format=\"png\", dpi=600)\n",
    "\n",
    "png2 = Image.open(png1)\n",
    "\n",
    "png2.save(\"Image_5.tiff\", dpi=(600, 600))\n",
    "png1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import regionprops\n",
    "import io\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "img_list = []\n",
    "gt_list = []\n",
    "baseline_pred_list = []\n",
    "no_transformer_pred_list = []\n",
    "\n",
    "names = [('patient020_frame11', 2), ('patient047_frame09', 4), ('patient085_frame09', 14), ('patient086_frame08', 5), ('patient085_frame01', 14), ('patient086_frame01', 2)]\n",
    "fig, ax = plt.subplots(len(names), 4, figsize=(7, 8))\n",
    "#fig, ax = plt.subplots(4, len(names), figsize=(8, 5))\n",
    "for idx, payload in enumerate(names):\n",
    "    name, index = payload\n",
    "    baseline_path_pred = os.path.join(r'ACDC_output\\Baseline\\cv_niftis_raw', name + '.nii.gz')\n",
    "    no_transformer_path_pred = os.path.join(r'ACDC_output\\no_transformer\\cv_niftis_raw', name + '.nii.gz')\n",
    "\n",
    "    path_gt = os.path.join(r'ACDC_output\\Baseline\\gt_niftis', name + '.nii.gz')\n",
    "    path_img = os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\imagesTr', name + '_0000.nii.gz')\n",
    "\n",
    "    data_baseline = nib.load(baseline_path_pred)\n",
    "    data_no_transformer = nib.load(no_transformer_path_pred)\n",
    "    data_gt = nib.load(path_gt)\n",
    "    data_img = nib.load(path_img)\n",
    "\n",
    "    baseline_pred = data_baseline.get_fdata()\n",
    "    no_transformer_pred = data_no_transformer.get_fdata()\n",
    "    gt = data_gt.get_fdata()\n",
    "    img = data_img.get_fdata()\n",
    "\n",
    "    label = np.logical_or(gt[:, :, index] > 0, baseline_pred[:, :, index] > 0)\n",
    "    label = np.logical_or(label, no_transformer_pred[:, :, index] > 0).astype(np.uint8)\n",
    "    regions = regionprops(label)\n",
    "    assert len(regions) == 1\n",
    "    min_row, min_col, max_row, max_col = regions[0].bbox\n",
    "\n",
    "    #for j in range(img.shape[-1]):\n",
    "    #    print(j)\n",
    "    #    fig, ax = plt.subplots(1, 4)\n",
    "    #    ax[0].imshow(img[:, :, j], cmap='gray')\n",
    "    #    ax[1].imshow(gt[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    ax[2].imshow(baseline_pred[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    ax[3].imshow(no_transformer_pred[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    plt.show()\n",
    "    #    plt.waitforbuttonpress()\n",
    "    #    plt.close(fig)\n",
    "    #print('************************************')\n",
    "\n",
    "    img = img[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    gt = gt[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    baseline_pred = baseline_pred[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    no_transformer_pred = no_transformer_pred[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "\n",
    "    img_list.append(img)\n",
    "    gt_list.append(gt)\n",
    "    baseline_pred_list.append(baseline_pred)\n",
    "    no_transformer_pred_list.append(no_transformer_pred)\n",
    "\n",
    "row_lengths = [x.shape[0] for x in img_list]\n",
    "col_lengths = [x.shape[1] for x in img_list]\n",
    "max_row = max(row_lengths)\n",
    "max_col = max(col_lengths)\n",
    "\n",
    "for i in range(len(img_list)):\n",
    "    current_img = img_list[i]\n",
    "    current_gt = gt_list[i]\n",
    "    current_baseline_pred = baseline_pred_list[i]\n",
    "    current_no_transformer_pred = no_transformer_pred_list[i]\n",
    "\n",
    "    pad_col = max_col - current_img.shape[1]\n",
    "    pad_row = max_row - current_img.shape[0]\n",
    "\n",
    "    if pad_col % 2 == 0:\n",
    "        pad_left = pad_col // 2\n",
    "        pad_right = pad_col // 2\n",
    "    else:\n",
    "        pad_left = pad_col // 2 + 1\n",
    "        pad_right = pad_col // 2\n",
    "\n",
    "    if pad_row % 2 == 0:\n",
    "        pad_top = pad_row // 2\n",
    "        pad_bottom = pad_row // 2\n",
    "    else:\n",
    "        pad_top = pad_row // 2 + 1\n",
    "        pad_bottom = pad_row // 2\n",
    "\n",
    "    pad_width = ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "    current_img = np.pad(current_img, pad_width=pad_width)\n",
    "    current_gt = np.pad(current_gt, pad_width=pad_width)\n",
    "    current_baseline_pred = np.pad(current_baseline_pred, pad_width=pad_width)\n",
    "    current_no_transformer_pred = np.pad(current_no_transformer_pred, pad_width=pad_width)\n",
    "\n",
    "    #current_img = current_img\n",
    "    #current_gt = current_gt\n",
    "    #current_baseline_pred = current_baseline_pred\n",
    "    #current_no_transformer_pred = current_no_transformer_pred\n",
    "    \n",
    "    ax[i, 0].imshow(current_img, cmap='gray')\n",
    "    ax[i, 0].axis('off')\n",
    "    ax[i, 1].imshow(current_gt, cmap='gray', vmin=0, vmax=3)\n",
    "    ax[i, 1].axis('off')\n",
    "    ax[i, 2].imshow(current_baseline_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    ax[i, 2].axis('off')\n",
    "    ax[i, 3].imshow(current_no_transformer_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    ax[i, 3].axis('off')\n",
    "\n",
    "    #ax[0, idx].imshow(img, cmap='gray')\n",
    "    #ax[0, idx].axis('off')\n",
    "    #ax[1, idx].imshow(gt, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[1, idx].axis('off')\n",
    "    #ax[2, idx].imshow(baseline_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[2, idx].axis('off')\n",
    "    #ax[3, idx].imshow(no_transformer_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[3, idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "#plt.savefig(\"visual_ablation_study_2.png\", dpi=600)\n",
    "\n",
    "png1 = io.BytesIO()\n",
    "plt.savefig(png1, format=\"png\", dpi=600)\n",
    "\n",
    "png2 = Image.open(png1)\n",
    "\n",
    "png2.save(\"Image_5.tiff\", dpi=(600, 600))\n",
    "png1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import regionprops\n",
    "import io\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img_list = []\n",
    "gt_list = []\n",
    "baseline_pred_list = []\n",
    "no_transformer_pred_list = []\n",
    "\n",
    "names = [('patient020_frame11', 2), ('patient047_frame09', 4), ('patient085_frame09', 14), ('patient086_frame08', 5), ('patient085_frame01', 14), ('patient086_frame01', 2)]\n",
    "fig, ax = plt.subplots(len(names), 3, figsize=(6, 8))\n",
    "#fig, ax = plt.subplots(4, len(names), figsize=(8, 5))\n",
    "for idx, payload in enumerate(names):\n",
    "    name, index = payload\n",
    "    baseline_path_pred = os.path.join(r'ACDC_output\\Baseline\\cv_niftis_raw', name + '.nii.gz')\n",
    "    no_transformer_path_pred = os.path.join(r'ACDC_output\\no_transformer\\cv_niftis_raw', name + '.nii.gz')\n",
    "\n",
    "    path_gt = os.path.join(r'ACDC_output\\Baseline\\gt_niftis', name + '.nii.gz')\n",
    "    path_img = os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\imagesTr', name + '_0000.nii.gz')\n",
    "\n",
    "    data_baseline = nib.load(baseline_path_pred)\n",
    "    data_no_transformer = nib.load(no_transformer_path_pred)\n",
    "    data_gt = nib.load(path_gt)\n",
    "    data_img = nib.load(path_img)\n",
    "\n",
    "    baseline_pred = data_baseline.get_fdata()\n",
    "    no_transformer_pred = data_no_transformer.get_fdata()\n",
    "    gt = data_gt.get_fdata()\n",
    "    img = data_img.get_fdata()\n",
    "\n",
    "    label = np.logical_or(gt[:, :, index] > 0, baseline_pred[:, :, index] > 0)\n",
    "    label = np.logical_or(label, no_transformer_pred[:, :, index] > 0).astype(np.uint8)\n",
    "    regions = regionprops(label)\n",
    "    assert len(regions) == 1\n",
    "    min_row, min_col, max_row, max_col = regions[0].bbox\n",
    "\n",
    "    #for j in range(img.shape[-1]):\n",
    "    #    print(j)\n",
    "    #    fig, ax = plt.subplots(1, 4)\n",
    "    #    ax[0].imshow(img[:, :, j], cmap='gray')\n",
    "    #    ax[1].imshow(gt[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    ax[2].imshow(baseline_pred[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    ax[3].imshow(no_transformer_pred[:, :, j], cmap='gray', vmin=0, vmax=3)\n",
    "    #    plt.show()\n",
    "    #    plt.waitforbuttonpress()\n",
    "    #    plt.close(fig)\n",
    "    #print('************************************')\n",
    "\n",
    "    img = img[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    gt = gt[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    baseline_pred = baseline_pred[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "    no_transformer_pred = no_transformer_pred[min_row - 10:max_row + 10, min_col - 10:max_col + 10, index]\n",
    "\n",
    "    img_list.append(img)\n",
    "    gt_list.append(gt)\n",
    "    baseline_pred_list.append(baseline_pred)\n",
    "    no_transformer_pred_list.append(no_transformer_pred)\n",
    "\n",
    "row_lengths = [x.shape[0] for x in img_list]\n",
    "col_lengths = [x.shape[1] for x in img_list]\n",
    "max_row = max(row_lengths)\n",
    "max_col = max(col_lengths)\n",
    "\n",
    "for i in range(len(img_list)):\n",
    "    current_img = img_list[i]\n",
    "    current_gt = gt_list[i]\n",
    "    current_baseline_pred = baseline_pred_list[i]\n",
    "    current_no_transformer_pred = no_transformer_pred_list[i]\n",
    "\n",
    "    pad_col = max_col - current_img.shape[1]\n",
    "    pad_row = max_row - current_img.shape[0]\n",
    "\n",
    "    if pad_col % 2 == 0:\n",
    "        pad_left = pad_col // 2\n",
    "        pad_right = pad_col // 2\n",
    "    else:\n",
    "        pad_left = pad_col // 2 + 1\n",
    "        pad_right = pad_col // 2\n",
    "\n",
    "    if pad_row % 2 == 0:\n",
    "        pad_top = pad_row // 2\n",
    "        pad_bottom = pad_row // 2\n",
    "    else:\n",
    "        pad_top = pad_row // 2 + 1\n",
    "        pad_bottom = pad_row // 2\n",
    "\n",
    "    pad_width = ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "    current_img = np.pad(current_img, pad_width=pad_width)\n",
    "    current_gt = np.pad(current_gt, pad_width=pad_width)\n",
    "    current_baseline_pred = np.pad(current_baseline_pred, pad_width=pad_width)\n",
    "    current_no_transformer_pred = np.pad(current_no_transformer_pred, pad_width=pad_width)\n",
    "\n",
    "    col_img_list = []\n",
    "    for arr in [current_gt, current_baseline_pred, current_no_transformer_pred]:\n",
    "        current_img_arr = np.copy(current_img)\n",
    "        current_img_arr = cv.normalize(current_img_arr, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX).astype(np.uint8)\n",
    "        current_img_arr = cv.cvtColor(current_img_arr, cv.COLOR_GRAY2RGB)\n",
    "        for j in range(1, 4):\n",
    "            seg = (arr == j).astype(np.uint8)\n",
    "            contours, hierarchy = cv.findContours(seg, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "            color = [0, 0, 0]\n",
    "            color[j - 1] = 255\n",
    "            cv.drawContours(current_img_arr, contours, -1, color, 1)\n",
    "        col_img_list.append(current_img_arr)\n",
    "\n",
    "    #current_img = current_img\n",
    "    #current_gt = current_gt\n",
    "    #current_baseline_pred = current_baseline_pred\n",
    "    #current_no_transformer_pred = current_no_transformer_pred\n",
    "    \n",
    "    ax[i, 0].imshow(col_img_list[0], cmap='gray')\n",
    "    ax[i, 0].axis('off')\n",
    "    ax[i, 1].imshow(col_img_list[1], cmap='gray', vmin=0, vmax=3)\n",
    "    ax[i, 1].axis('off')\n",
    "    ax[i, 2].imshow(col_img_list[2], cmap='gray', vmin=0, vmax=3)\n",
    "    ax[i, 2].axis('off')\n",
    "\n",
    "    #ax[0, idx].imshow(img, cmap='gray')\n",
    "    #ax[0, idx].axis('off')\n",
    "    #ax[1, idx].imshow(gt, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[1, idx].axis('off')\n",
    "    #ax[2, idx].imshow(baseline_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[2, idx].axis('off')\n",
    "    #ax[3, idx].imshow(no_transformer_pred, cmap='gray', vmin=0, vmax=3)\n",
    "    #ax[3, idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "plt.savefig(\"Transformer_vs_conv.png\", dpi=600)\n",
    "\n",
    "#png1 = io.BytesIO()\n",
    "#plt.savefig(png1, format=\"png\", dpi=600)\n",
    "#\n",
    "#png2 = Image.open(png1)\n",
    "#\n",
    "#png2.save(\"Image_6.tiff\", dpi=(600, 600))\n",
    "#png1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df_disease = pd.read_excel(r'disease_saud\\radiomics_all.xlsx')\n",
    "payload_disease = df_disease.loc[df_disease['Descriptor'] == 'Infarction_Ratio']\n",
    "\n",
    "results_list = []\n",
    "#with open(r'Quorum_output_RV_removed\\Baseline\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "with open(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\rubbish\\fold_0\\temp_allClasses\\summary.json\", 'r') as fd_in:\n",
    "    metric_file = json.load(fd_in)['results']['all']\n",
    "    for data_dict in metric_file:\n",
    "        name = data_dict['center']\n",
    "        phase = data_dict['reference'].split('\\\\')[-1][11:13]\n",
    "        infraction_percent = payload_disease\n",
    "        center = name.split('-')[0]\n",
    "        manufacturer = data_dict['manufacturer']\n",
    "\n",
    "        #csv_path = os.path.join(r'data_saud_2\\3D_RV_removed_2', phase.upper(), name, name + '_metadata.csv')\n",
    "        csv_path = os.path.join(r'data_saud_2\\3D_RV_removed', phase.upper(), name, name + '_metadata.csv')\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        if name in payload_disease:\n",
    "            infraction_percent = payload_disease[name][0]\n",
    "        else:\n",
    "            infraction_percent = ''\n",
    "\n",
    "        field_strength = str(data_dict['strength'])\n",
    "\n",
    "        rv_dice = data_dict['1']['Dice']\n",
    "        myo_dice = data_dict['2']['Dice']\n",
    "        lv_dice = data_dict['3']['Dice']\n",
    "        dice = (rv_dice + myo_dice + lv_dice) / 3\n",
    "\n",
    "        rv_hd = data_dict['1']['Hausdorff Distance']\n",
    "        myo_hd = data_dict['2']['Hausdorff Distance']\n",
    "        lv_hd = data_dict['3']['Hausdorff Distance']\n",
    "        hd = (rv_hd + myo_hd + lv_hd) / 3\n",
    "\n",
    "        rv_assd = data_dict['1']['Avg. Symmetric Surface Distance']\n",
    "        myo_assd = data_dict['2']['Avg. Symmetric Surface Distance']\n",
    "        lv_assd = data_dict['3']['Avg. Symmetric Surface Distance']\n",
    "        assd = (rv_assd + myo_assd + lv_assd) / 3\n",
    "\n",
    "        results_list.append({'Name': name, \n",
    "                            'Center': center, \n",
    "                            'Manufacturer': manufacturer, \n",
    "                            'Phase': phase, \n",
    "                            'infraction_percent': infraction_percent,\n",
    "                            'Field Strength': field_strength,\n",
    "                            'RV dice': rv_dice,\n",
    "                            'MYO dice': myo_dice,\n",
    "                            'LV dice': lv_dice,\n",
    "                            'Mean dice': dice, \n",
    "                            'RV HD': rv_hd,\n",
    "                            'MYO HD': myo_hd,\n",
    "                            'LV HD': lv_hd,\n",
    "                            'Mean HD': hd,\n",
    "                            'RV_assd': rv_assd,\n",
    "                            'MYO_assd': myo_assd,\n",
    "                            'LV_assd': lv_assd,\n",
    "                            'Mean assd': assd\n",
    "                            })\n",
    "\n",
    "with open(os.path.join('rubbish', 'fold_0', 'Criteria_jmp_cross_dataset.csv'), 'w') as fd_csv:\n",
    "    writer = csv.DictWriter(fd_csv, fieldnames=list(results_list[0].keys()))\n",
    "    writer.writeheader() \n",
    "    writer.writerows(results_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "results_list = []\n",
    "#with open(r'Quorum_output_RV_removed\\Baseline\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "with open(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\Medis_to_ACDC\\cv_niftis_raw\\summary.json\", 'r') as fd_in:\n",
    "    metric_file = json.load(fd_in)['results']['all']\n",
    "    for data_dict in metric_file:\n",
    "        phase = os.path.basename(data_dict['reference']).split('.')[0][-2:]\n",
    "        name = os.path.basename(data_dict['reference']).split('_')[0]\n",
    "\n",
    "        rv_dice = data_dict['1']['Dice']\n",
    "        myo_dice = data_dict['2']['Dice']\n",
    "        lv_dice = data_dict['3']['Dice']\n",
    "        dice = (rv_dice + myo_dice + lv_dice) / 3\n",
    "\n",
    "        rv_hd = data_dict['1']['Hausdorff Distance']\n",
    "        myo_hd = data_dict['2']['Hausdorff Distance']\n",
    "        lv_hd = data_dict['3']['Hausdorff Distance']\n",
    "        hd = (rv_hd + myo_hd + lv_hd) / 3\n",
    "\n",
    "        rv_assd = data_dict['1']['Avg. Symmetric Surface Distance']\n",
    "        myo_assd = data_dict['2']['Avg. Symmetric Surface Distance']\n",
    "        lv_assd = data_dict['3']['Avg. Symmetric Surface Distance']\n",
    "        assd = (rv_assd + myo_assd + lv_assd) / 3\n",
    "\n",
    "        results_list.append({'Name': name,\n",
    "                            'Phase': phase, \n",
    "                            'RV dice': rv_dice,\n",
    "                            'MYO dice': myo_dice,\n",
    "                            'LV dice': lv_dice,\n",
    "                            'Mean dice': dice, \n",
    "                            'RV HD': rv_hd,\n",
    "                            'MYO HD': myo_hd,\n",
    "                            'LV HD': lv_hd,\n",
    "                            'Mean HD': hd,\n",
    "                            'RV_assd': rv_assd,\n",
    "                            'MYO_assd': myo_assd,\n",
    "                            'LV_assd': lv_assd,\n",
    "                            'Mean assd': assd\n",
    "                            })\n",
    "\n",
    "with open(os.path.join(r'C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\Medis_to_ACDC', 'metrics.csv'), 'w') as fd_csv:\n",
    "    writer = csv.DictWriter(fd_csv, fieldnames=list(results_list[0].keys()))\n",
    "    writer.writeheader() \n",
    "    writer.writerows(results_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siemens = 248\n",
      "philips = 14\n",
      "ge = 48\n",
      "1.5 = 280\n",
      "3.0 = 30\n",
      "ed = 155\n",
      "es = 155\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "with open(r'splits_final.pkl', 'rb') as fd:\n",
    "    data = pickle.load(fd)\n",
    "    train_names = data[0]['train']\n",
    "    train_names = [x[:-3] for x in train_names]\n",
    "\n",
    "def update_dict(d, key, value):\n",
    "    if key not in d:\n",
    "        d[key] = [value]\n",
    "    else:\n",
    "        d[key].append(value)\n",
    "    return d\n",
    "\n",
    "centers = {}\n",
    "manufacturers = {}\n",
    "strengths = {}\n",
    "depths = {}\n",
    "phases = {}\n",
    "path_list = glob(r'custom_quorum\\**\\*.csv')\n",
    "for path in path_list:\n",
    "    df = pd.read_csv(path)\n",
    "    filename = path.split('\\\\')[-1]\n",
    "    patient_name = path.split('\\\\')[-2]\n",
    "    phase = filename[:2]\n",
    "    manufacturer = df['Manufacturer'].iloc[0]\n",
    "    strength = df['Field Strength'].iloc[0]\n",
    "\n",
    "    if patient_name in train_names:\n",
    "        manufacturers = update_dict(manufacturers, str(manufacturer), filename)\n",
    "        strengths = update_dict(strengths, str(strength), filename)\n",
    "        phases = update_dict(phases, str(phase), filename)\n",
    "\n",
    "for current_dict in [manufacturers, strengths, phases]:\n",
    "    for k in current_dict.keys():\n",
    "        print(f'{k} = {len(current_dict[k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250001 = 10\n",
      "250005 = 2\n",
      "276004 = 2\n",
      "276005 = 2\n",
      "276008 = 2\n",
      "348001 = 6\n",
      "348002 = 6\n",
      "348003 = 2\n",
      "348004 = 8\n",
      "348007 = 6\n",
      "616003 = 4\n",
      "616005 = 2\n",
      "616010 = 2\n",
      "703001 = 10\n",
      "703003 = 2\n",
      "703004 = 8\n",
      "724002 = 2\n",
      "724006 = 4\n",
      "siemens = 62\n",
      "philips = 4\n",
      "ge = 14\n",
      "1.5 = 66\n",
      "3.0 = 14\n",
      "ed = 40\n",
      "es = 40\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "def update_dict(d, key, value):\n",
    "    if key not in d:\n",
    "        d[key] = [value]\n",
    "    else:\n",
    "        d[key].append(value)\n",
    "    return d\n",
    "\n",
    "output_file_name = 'quorum_output_field.yaml'\n",
    "if os.path.exists(output_file_name):\n",
    "    os.remove(output_file_name)\n",
    "\n",
    "centers = {}\n",
    "manufacturers = {}\n",
    "strengths = {}\n",
    "depths = {}\n",
    "phases = {}\n",
    "\n",
    "path_list = glob(r'Quorum_output_2\\only_sfb\\fold_0\\temp_allClasses\\*.gz')\n",
    "for path in path_list:\n",
    "    filename = path.split('\\\\')[-1]\n",
    "    phase = filename[11:13]\n",
    "    name = filename.split('_')[0]\n",
    "    df = pd.read_csv(os.path.join('custom_quorum_2', name, phase + '_info.csv'))\n",
    "    center = df['Name'].iloc[0].split('-')[0]\n",
    "    manufacturer = df['Manufacturer'].iloc[0]\n",
    "    strength = df['Field Strength'].iloc[0]\n",
    "\n",
    "    centers = update_dict(centers, str(center), filename)\n",
    "    manufacturers = update_dict(manufacturers, str(manufacturer), filename)\n",
    "    strengths = update_dict(strengths, str(strength), filename)\n",
    "    phases = update_dict(phases, str(phase), filename)\n",
    "\n",
    "with open(r'Quorum_output_2\\only_sfb\\fold_0\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "    metric_file = json.load(fd_in)['results']['all']\n",
    "    results_dict = {}\n",
    "    for current_dict, criteria_name in zip([centers, manufacturers, strengths, phases], ['Center', 'Manufacturer', 'Field Strength', 'Phase']):\n",
    "        for key in current_dict.keys():\n",
    "            print(f'{key} = {len(current_dict[key])}')\n",
    "            current_values = current_dict[key]\n",
    "            list_of_dict = [x for x in metric_file if x['reference'].split('/')[-1] in current_values]\n",
    "            mean_dice_list = []\n",
    "            mean_hausdorff_list = []\n",
    "            for data_dict in list_of_dict:\n",
    "                rv_dice = data_dict['1']['Dice']\n",
    "                myo_dice = data_dict['2']['Dice']\n",
    "                lv_dice = data_dict['3']['Dice']\n",
    "                results_list.append({criteria_name: key, 'RV': rv_dice, 'MYO': myo_dice, 'LV': lv_dice, 'Mean': (rv_dice + myo_dice + lv_dice) / 3})\n",
    "                mean_dice_list.append([data_dict['1']['Dice'], data_dict['2']['Dice'], data_dict['3']['Dice']])\n",
    "                mean_hausdorff_list.append([data_dict['1']['Hausdorff Distance'], data_dict['2']['Hausdorff Distance'], data_dict['3']['Hausdorff Distance']])\n",
    "            class_dice = np.stack(mean_dice_list, axis=0).mean(axis=0)\n",
    "            class_hausdorff = np.stack(mean_hausdorff_list, axis=0).mean(axis=0)\n",
    "            results_dict[key] = {'Hausdorff distance': class_hausdorff.tolist(), \n",
    "                                'Mean Hausdorff distance': float(class_hausdorff.mean()), \n",
    "                                'Dice score': class_dice.tolist(),\n",
    "                                'Mean dice score': float(class_dice.mean())}\n",
    "\n",
    "with open(output_file_name, 'w') as fd:\n",
    "    yaml.safe_dump(results_dict, fd, default_flow_style=False)\n",
    "    #for results_dict in results_dicts:\n",
    "    #    for key in results_dict.keys():\n",
    "    #        fd.write(key + ': ' + str(results_dict[key]) + '\\n')\n",
    "    #    fd.write('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ED/ES volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.234375\n",
      "187504.625\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "data = nib.load(r'quorum_output\\validation_raw\\patient003_ed.nii.gz')\n",
    "zoom = data.header.get_zooms()\n",
    "pixel_volume = np.prod(zoom)\n",
    "print(pixel_volume)\n",
    "arr = data.get_fdata()\n",
    "nb_pixels = np.count_nonzero(arr == 1)\n",
    "print(nb_pixels * pixel_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "#\n",
    "#import nibabel as nib\n",
    "#import numpy as np\n",
    "#from scipy.ndimage import distance_transform_edt\n",
    "#from scipy.interpolate import interpn\n",
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#def bwperim(bw, n=4):\n",
    "#    \"\"\"\n",
    "#    perim = bwperim(bw, n=4)\n",
    "#    Find the perimeter of objects in binary images.\n",
    "#    A pixel is part of an object perimeter if its value is one and there\n",
    "#    is at least one zero-valued pixel in its neighborhood.\n",
    "#    By default the neighborhood of a pixel is 4 nearest pixels, but\n",
    "#    if `n` is set to 8 the 8 nearest pixels will be considered.\n",
    "#    Parameters\n",
    "#    ----------\n",
    "#      bw : A black-and-white image\n",
    "#      n : Connectivity. Must be 4 or 8 (default: 8)\n",
    "#    Returns\n",
    "#    -------\n",
    "#      perim : A boolean image\n",
    "#    \"\"\"\n",
    "#\n",
    "#    if n not in (4,8):\n",
    "#        raise ValueError('mahotas.bwperim: n must be 4 or 8')\n",
    "#    rows,cols = bw.shape\n",
    "#\n",
    "#    # Translate image by one pixel in all directions\n",
    "#    north = np.zeros((rows,cols))\n",
    "#    south = np.zeros((rows,cols))\n",
    "#    west = np.zeros((rows,cols))\n",
    "#    east = np.zeros((rows,cols))\n",
    "#\n",
    "#    north[:-1,:] = bw[1:,:]\n",
    "#    south[1:,:]  = bw[:-1,:]\n",
    "#    west[:,:-1]  = bw[:,1:]\n",
    "#    east[:,1:]   = bw[:,:-1]\n",
    "#    idx = (north == bw) & \\\n",
    "#          (south == bw) & \\\n",
    "#          (west  == bw) & \\\n",
    "#          (east  == bw)\n",
    "#    if n == 8:\n",
    "#        north_east = np.zeros((rows, cols))\n",
    "#        north_west = np.zeros((rows, cols))\n",
    "#        south_east = np.zeros((rows, cols))\n",
    "#        south_west = np.zeros((rows, cols))\n",
    "#        north_east[:-1, 1:]   = bw[1:, :-1]\n",
    "#        north_west[:-1, :-1] = bw[1:, 1:]\n",
    "#        south_east[1:, 1:]     = bw[:-1, :-1]\n",
    "#        south_west[1:, :-1]   = bw[:-1, 1:]\n",
    "#        idx &= (north_east == bw) & \\\n",
    "#               (south_east == bw) & \\\n",
    "#               (south_west == bw) & \\\n",
    "#               (north_west == bw)\n",
    "#    return ~idx * bw\n",
    "#\n",
    "#def signed_bwdist(im):\n",
    "#    '''\n",
    "#    Find perim and return masked image (signed/reversed)\n",
    "#    '''    \n",
    "#    perimeter = bwperim(im)\n",
    "#\n",
    "#    distance_map = bwdist(perimeter)\n",
    "#\n",
    "#    im = -distance_map*np.logical_not(im) + distance_map*im\n",
    "#    return im\n",
    "#\n",
    "#def bwdist(im):\n",
    "#    '''\n",
    "#    Find distance map of image\n",
    "#    '''\n",
    "#    dist_im = distance_transform_edt(1-im)\n",
    "#    return dist_im\n",
    "#\n",
    "#def interp_shape(arr, new_depth):\n",
    "#    '''\n",
    "#    Interpolate between two contours\n",
    "#\n",
    "#    Input: top \n",
    "#            [X,Y] - Image of top contour (mask)\n",
    "#           bottom\n",
    "#            [X,Y] - Image of bottom contour (mask)\n",
    "#           precision\n",
    "#             float  - % between the images to interpolate \n",
    "#                Ex: num=0.5 - Interpolate the middle image between top and bottom image\n",
    "#    Output: out\n",
    "#            [X,Y] - Interpolated image at num (%) between top and bottom\n",
    "#\n",
    "#    '''\n",
    "#    X, Y, Z = arr.shape\n",
    "#\n",
    "#    distance_arr = []\n",
    "#    for i in range(Z):\n",
    "#        distance_arr.append(signed_bwdist(arr[:, :, i]))\n",
    "#    distance_arr = np.stack(distance_arr, axis=-1)\n",
    "#\n",
    "#    x = np.arange(0, X)\n",
    "#    y = np.arange(0, Y)\n",
    "#    z = np.arange(0, Z)\n",
    "#    points = (x, y, z)\n",
    "#\n",
    "#    stop = Z-1\n",
    "#\n",
    "#    # create ndgrids\n",
    "#    grid = np.mgrid[:X, :Y, 0:stop:(new_depth * 1j)]\n",
    "#    xi = np.rollaxis(grid, 0, 4)\n",
    "#    xi = xi.reshape((X * Y * new_depth, 3))\n",
    "#\n",
    "#    out = interpn(points, distance_arr, xi)\n",
    "#    out = out.reshape((X, Y, new_depth))\n",
    "#\n",
    "#    # Threshold distmap to values above 0\n",
    "#    out = out > 0\n",
    "#\n",
    "#    #fig, ax = plt.subplots(2, out.shape[-1])\n",
    "#    #for t in range(out.shape[-1]):\n",
    "#    #    if t < arr.shape[-1]:\n",
    "#    #        ax[0, t].imshow(arr[:, :, t], cmap='gray')\n",
    "#    #    ax[1, t].imshow(out[:, :, t], cmap='gray')\n",
    "#    #plt.show()\n",
    "#\n",
    "#    #print(out.shape)\n",
    "#\n",
    "#    return out\n",
    "#\n",
    "#\n",
    "#data = nib.load(r'data_saud_2\\inference_2\\patient030_ed.nii.gz')\n",
    "#arr = data.get_fdata()\n",
    "#arr = arr == 1\n",
    "#\n",
    "#X, Y, Z = arr.shape\n",
    "##print(arr.shape)\n",
    "## Run interpolation\n",
    "#out = interp_shape(arr, Z+1)\n",
    "##print(out.shape)\n",
    "##fig, ax = plt.subplots(1, Z+1)\n",
    "##for i in range(Z+1):\n",
    "##    ax[i].imshow(out[:, :, i], cmap='gray')\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from scipy.interpolate import interpn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bwperim(bw, n=4):\n",
    "    \"\"\"\n",
    "    perim = bwperim(bw, n=4)\n",
    "    Find the perimeter of objects in binary images.\n",
    "    A pixel is part of an object perimeter if its value is one and there\n",
    "    is at least one zero-valued pixel in its neighborhood.\n",
    "    By default the neighborhood of a pixel is 4 nearest pixels, but\n",
    "    if `n` is set to 8 the 8 nearest pixels will be considered.\n",
    "    Parameters\n",
    "    ----------\n",
    "      bw : A black-and-white image\n",
    "      n : Connectivity. Must be 4 or 8 (default: 8)\n",
    "    Returns\n",
    "    -------\n",
    "      perim : A boolean image\n",
    "    \"\"\"\n",
    "\n",
    "    if n not in (4,8):\n",
    "        raise ValueError('mahotas.bwperim: n must be 4 or 8')\n",
    "    rows,cols = bw.shape\n",
    "\n",
    "    # Translate image by one pixel in all directions\n",
    "    north = np.zeros((rows,cols))\n",
    "    south = np.zeros((rows,cols))\n",
    "    west = np.zeros((rows,cols))\n",
    "    east = np.zeros((rows,cols))\n",
    "\n",
    "    north[:-1,:] = bw[1:,:]\n",
    "    south[1:,:]  = bw[:-1,:]\n",
    "    west[:,:-1]  = bw[:,1:]\n",
    "    east[:,1:]   = bw[:,:-1]\n",
    "    idx = (north == bw) & \\\n",
    "          (south == bw) & \\\n",
    "          (west  == bw) & \\\n",
    "          (east  == bw)\n",
    "    if n == 8:\n",
    "        north_east = np.zeros((rows, cols))\n",
    "        north_west = np.zeros((rows, cols))\n",
    "        south_east = np.zeros((rows, cols))\n",
    "        south_west = np.zeros((rows, cols))\n",
    "        north_east[:-1, 1:]   = bw[1:, :-1]\n",
    "        north_west[:-1, :-1] = bw[1:, 1:]\n",
    "        south_east[1:, 1:]     = bw[:-1, :-1]\n",
    "        south_west[1:, :-1]   = bw[:-1, 1:]\n",
    "        idx &= (north_east == bw) & \\\n",
    "               (south_east == bw) & \\\n",
    "               (south_west == bw) & \\\n",
    "               (north_west == bw)\n",
    "    return ~idx * bw\n",
    "\n",
    "def signed_bwdist(im):\n",
    "    '''\n",
    "    Find perim and return masked image (signed/reversed)\n",
    "    '''    \n",
    "    perimeter = bwperim(im)\n",
    "\n",
    "    distance_map = bwdist(perimeter)\n",
    "\n",
    "    im = -distance_map*np.logical_not(im) + distance_map*im\n",
    "    return im\n",
    "\n",
    "def bwdist(im):\n",
    "    '''\n",
    "    Find distance map of image\n",
    "    '''\n",
    "    dist_im = distance_transform_edt(1-im)\n",
    "    return dist_im\n",
    "\n",
    "def interp_shape(top, bottom, precision):\n",
    "    '''\n",
    "    Interpolate between two contours\n",
    "\n",
    "    Input: top \n",
    "            [X,Y] - Image of top contour (mask)\n",
    "           bottom\n",
    "            [X,Y] - Image of bottom contour (mask)\n",
    "           precision\n",
    "             float  - % between the images to interpolate \n",
    "                Ex: num=0.5 - Interpolate the middle image between top and bottom image\n",
    "    Output: out\n",
    "            [X,Y] - Interpolated image at num (%) between top and bottom\n",
    "\n",
    "    '''\n",
    "    top = signed_bwdist(top)\n",
    "    bottom = signed_bwdist(bottom)\n",
    "\n",
    "    # row,cols definition\n",
    "    r, c = top.shape\n",
    "\n",
    "    # Reverse % indexing\n",
    "    precision = 1+precision\n",
    "\n",
    "    # rejoin top, bottom into a single array of shape (2, r, c)\n",
    "    top_and_bottom = np.stack((top, bottom))\n",
    "\n",
    "    # create ndgrids \n",
    "    points = (np.r_[0, 2], np.arange(r), np.arange(c))\n",
    "    xi = np.rollaxis(np.mgrid[:r, :c], 0, 3).reshape((r*c, 2))\n",
    "    xi = np.c_[np.full((r*c),precision), xi]\n",
    "\n",
    "    # Interpolate for new plane\n",
    "    out = interpn(points, top_and_bottom, xi)\n",
    "    out = out.reshape((r, c))\n",
    "\n",
    "    # Threshold distmap to values above 0\n",
    "    out = out > 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "#data = nib.load(r'data_saud_2\\inference_2\\patient030_ed.nii.gz')\n",
    "#arr = data.get_fdata()\n",
    "#arr = arr == 1\n",
    "#\n",
    "#X, Y, Z = arr.shape\n",
    "##print(arr.shape)\n",
    "## Run interpolation\n",
    "#out = interp_shape(arr, Z+1)\n",
    "#print(out.shape)\n",
    "#fig, ax = plt.subplots(1, Z+1)\n",
    "#for i in range(Z+1):\n",
    "#    ax[i].imshow(out[:, :, i], cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only patient in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def update_dict(d, key, value):\n",
    "    if key not in d:\n",
    "        d[key] = [value]\n",
    "    else:\n",
    "        d[key].append(value)\n",
    "    return d\n",
    "\n",
    "\n",
    "cut_prediction_path = r'Quorum_output_2\\only_sfb\\fold_0\\temp_allClasses'\n",
    "all_prediction_path = r'data_saud_2\\inference'\n",
    "\n",
    "data_dict_cut = {}\n",
    "data_dict_all = {}\n",
    "\n",
    "path_list = glob(os.path.join(cut_prediction_path, '*.gz'))\n",
    "cut_prediction_names = []\n",
    "for path in path_list:\n",
    "    filename = path.split('\\\\')[-1]\n",
    "    phase = filename[11:13]\n",
    "    name = filename.split('_')[0]\n",
    "    df = pd.read_csv(os.path.join('custom_quorum_2', name, phase + '_info.csv'))\n",
    "    actual_name = df['Name'].to_numpy()[0]\n",
    "    spacing = (df['Space Between Slices'] - df['Slice Thickness']).to_numpy()[0]\n",
    "    cut_prediction_names.append(actual_name)\n",
    "    if phase == 'ed':\n",
    "        update_dict(data_dict_cut, 'ed', (path, spacing, actual_name))\n",
    "    elif phase == 'es':\n",
    "        update_dict(data_dict_cut, 'es', (path, spacing, actual_name))\n",
    "\n",
    "csv_list = glob(os.path.join(all_prediction_path, '*.csv'))\n",
    "for csv_path in csv_list:\n",
    "    filename = csv_path.split('\\\\')[-1]\n",
    "    phase = filename[11:13]\n",
    "    df = pd.read_csv(csv_path)\n",
    "    actual_name = df['Name'].to_numpy()[0]\n",
    "    if actual_name in cut_prediction_names:\n",
    "        spacing = (df['Space Between Slices'] - df['Slice Thickness']).to_numpy()[0]\n",
    "        path = csv_path[:-4] + '.nii.gz'\n",
    "        if phase == 'ed':\n",
    "            update_dict(data_dict_all, 'ed', (path, spacing, actual_name))\n",
    "        elif phase == 'es':\n",
    "            update_dict(data_dict_all, 'es', (path, spacing, actual_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient003_ed.nii.gz', 'patient003_es.nii.gz', 'patient005_ed.nii.gz', 'patient005_es.nii.gz', 'patient011_ed.nii.gz', 'patient011_es.nii.gz', 'patient017_ed.nii.gz', 'patient017_es.nii.gz', 'patient018_ed.nii.gz', 'patient018_es.nii.gz', 'patient020_ed.nii.gz', 'patient020_es.nii.gz', 'patient024_ed.nii.gz', 'patient024_es.nii.gz', 'patient026_ed.nii.gz', 'patient026_es.nii.gz', 'patient030_ed.nii.gz', 'patient030_es.nii.gz', 'patient033_ed.nii.gz', 'patient033_es.nii.gz', 'patient038_ed.nii.gz', 'patient038_es.nii.gz', 'patient040_ed.nii.gz', 'patient040_es.nii.gz', 'patient046_ed.nii.gz', 'patient046_es.nii.gz', 'patient049_ed.nii.gz', 'patient049_es.nii.gz', 'patient054_ed.nii.gz', 'patient054_es.nii.gz', 'patient059_ed.nii.gz', 'patient059_es.nii.gz', 'patient064_ed.nii.gz', 'patient064_es.nii.gz', 'patient072_ed.nii.gz', 'patient072_es.nii.gz', 'patient073_ed.nii.gz', 'patient073_es.nii.gz', 'patient077_ed.nii.gz', 'patient077_es.nii.gz', 'patient086_ed.nii.gz', 'patient086_es.nii.gz', 'patient087_ed.nii.gz', 'patient087_es.nii.gz', 'patient088_ed.nii.gz', 'patient088_es.nii.gz', 'patient091_ed.nii.gz', 'patient091_es.nii.gz', 'patient094_ed.nii.gz', 'patient094_es.nii.gz', 'patient109_ed.nii.gz', 'patient109_es.nii.gz', 'patient122_ed.nii.gz', 'patient122_es.nii.gz', 'patient123_ed.nii.gz', 'patient123_es.nii.gz', 'patient129_ed.nii.gz', 'patient129_es.nii.gz', 'patient131_ed.nii.gz', 'patient131_es.nii.gz', 'patient139_ed.nii.gz', 'patient139_es.nii.gz', 'patient149_ed.nii.gz', 'patient149_es.nii.gz', 'patient165_ed.nii.gz', 'patient165_es.nii.gz', 'patient168_ed.nii.gz', 'patient168_es.nii.gz', 'patient169_ed.nii.gz', 'patient169_es.nii.gz', 'patient170_ed.nii.gz', 'patient170_es.nii.gz', 'patient175_ed.nii.gz', 'patient175_es.nii.gz', 'patient182_ed.nii.gz', 'patient182_es.nii.gz', 'patient187_ed.nii.gz', 'patient187_es.nii.gz', 'patient194_ed.nii.gz', 'patient194_es.nii.gz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:01<00:00, 277.80it/s]\n",
      "100%|██████████| 390/390 [01:33<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847711\n",
      "844363\n",
      "700370\n",
      "700857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "\n",
    "nb_rv_removed = 0\n",
    "right_names = []\n",
    "path_list = glob(r'Quorum_output_RV_removed\\Baseline\\temp_allClasses\\*.gz')\n",
    "for path in path_list:\n",
    "    right_names.append(path.split('\\\\')[-1])\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    nb_rv_removed += np.count_nonzero(arr == 3)\n",
    "\n",
    "print(right_names)\n",
    "\n",
    "nb_base = 0\n",
    "path_list = glob(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses\\*.gz')\n",
    "for path in path_list:\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    nb_base += np.count_nonzero(arr == 3)\n",
    "\n",
    "nb_gt_rv_removed = 0\n",
    "path_list = glob(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task030_Quorum_RV_removed\\labelsTr\\*.gz')\n",
    "for path in tqdm(path_list):\n",
    "    if path.split('\\\\')[-1] not in right_names:\n",
    "        continue\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    nb_gt_rv_removed += np.count_nonzero(arr == 3)\n",
    "\n",
    "nb_gt = 0\n",
    "path_list = glob(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task029_Quorum\\labelsTr\\*.gz')\n",
    "for path in tqdm(path_list):\n",
    "    if path.split('\\\\')[-1] not in right_names:\n",
    "        continue\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    nb_gt += np.count_nonzero(arr == 3)\n",
    "\n",
    "print(nb_rv_removed)\n",
    "print(nb_gt_rv_removed)\n",
    "print(nb_base)\n",
    "print(nb_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 13)\n",
      "(256, 256, 13)\n",
      "(256, 256, 13)\n",
      "(256, 256, 15)\n",
      "(256, 256, 15)\n",
      "(256, 256, 15)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11712/3746089268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_rv_removed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mwaitforbuttonpress\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m   2306\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2308\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mwaitforbuttonpress\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   3123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3125\u001b[1;33m         _blocking_input.blocking_input_loop(\n\u001b[0m\u001b[0;32m   3126\u001b[0m             self, [\"button_press_event\", \"key_press_event\"], timeout, handler)\n\u001b[0;32m   3127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\_blocking_input.py\u001b[0m in \u001b[0;36mblocking_input_loop\u001b[1;34m(figure, event_names, timeout, handler)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmpl_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevent_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Start event loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Run even on exception like ctrl-c.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Disconnect the callbacks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backends\\backend_qt.py\u001b[0m in \u001b[0;36mstart_event_loop\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_maybe_allow_interrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mqt_compat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backends\\qt_compat.py\u001b[0m in \u001b[0;36m_maybe_allow_interrupt\u001b[1;34m(qapp)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_sigint_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhandler_args\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[0mold_sigint_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mhandler_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses\\patient109_es.nii.gz')\n",
    "pred1 = data.get_fdata()\n",
    "\n",
    "data_rv_removed = nib.load(r'Quorum_output_RV_removed\\Baseline\\temp_allClasses\\patient109_es.nii.gz')\n",
    "pred_rv_removed = data_rv_removed.get_fdata()\n",
    "\n",
    "data_img1 = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task029_Quorum\\imagesTr\\patient109_es_0000.nii.gz')\n",
    "img1 = data_img1.get_fdata()\n",
    "\n",
    "data_img2 = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task030_Quorum_RV_removed\\imagesTr\\patient109_es_0000.nii.gz')\n",
    "img2 = data_img2.get_fdata()\n",
    "\n",
    "data_label1 = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task029_Quorum\\labelsTr\\patient109_es.nii.gz')\n",
    "label1 = data_label1.get_fdata()\n",
    "\n",
    "data_label2 = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task030_Quorum_RV_removed\\labelsTr\\patient109_es.nii.gz')\n",
    "label2 = data_label2.get_fdata()\n",
    "\n",
    "print(img1.shape)\n",
    "print(pred1.shape)\n",
    "print(label1.shape)\n",
    "print(img2.shape)\n",
    "print(pred_rv_removed.shape)\n",
    "print(label2.shape)\n",
    "\n",
    "for i in range(max(img1.shape[-1], img1.shape[-2])):\n",
    "    fig, ax = plt.subplots(1, 6)\n",
    "    ax[0].imshow(img1[:, :, i], cmap='gray')\n",
    "    ax[1].imshow(pred1[:, :, i], cmap='gray')\n",
    "    ax[2].imshow(label1[:, :, i], cmap='gray')\n",
    "    ax[3].imshow(img2[:, :, i], cmap='gray')\n",
    "    ax[4].imshow(pred_rv_removed[:, :, i], cmap='gray')\n",
    "    ax[5].imshow(label2[:, :, i], cmap='gray')\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:08<00:00, 15.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2 as cv\n",
    "from skimage.morphology import binary_erosion\n",
    "\n",
    "def get_volume(arr, zoom, spacing):\n",
    "    pixel_size = np.prod(zoom)\n",
    "    gap_size = np.prod((zoom[0], zoom[1], spacing))\n",
    "    \n",
    "    volume_ml_interp = 0\n",
    "    for s in range(len(arr)):\n",
    "        top = arr[s]\n",
    "        assert np.all(np.isin(top, [0, 1]))\n",
    "        volume_ml_interp += (pixel_size * top.sum()) / 1000\n",
    "        if s < len(arr) - 1:\n",
    "            bottom = arr[s + 1]\n",
    "            assert np.all(np.isin(bottom, [0, 1]))\n",
    "            interpolated = interp_shape(top, bottom, precision=0.5)\n",
    "            volume_ml_interp += (gap_size * interpolated.sum()) / 1000\n",
    "    \n",
    "    return volume_ml_interp\n",
    "\n",
    "df = pd.read_excel('Quorum_Qmass_12_07_2021.xlsx')\n",
    "#df_keep = pd.read_csv(r'data_saud_2\\keep.csv', converters={\"Slices\": lambda x: list(map(int, x.strip(\"[]\").split(\", \")))})\n",
    "\n",
    "name_list = []\n",
    "results_list = []\n",
    "results_list_gt = []\n",
    "#with open(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "with open(r'cross_dataset\\fold_0\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "    metric_file = json.load(fd_in)['results']['all']\n",
    "    for idx in tqdm(range(0, len(metric_file), 2)):\n",
    "        spacing_ed = float(metric_file[idx]['spacing between slices']) - float(metric_file[idx]['slice thickness'])\n",
    "        spacing_es = float(metric_file[idx+1]['spacing between slices']) - float(metric_file[idx+1]['slice thickness'])\n",
    "        name1 = metric_file[idx]['center']\n",
    "        name2 = metric_file[idx+1]['center']\n",
    "\n",
    "        assert name1 == name2\n",
    "        name_list.append(name1)\n",
    "\n",
    "        path1 = metric_file[idx]['test']\n",
    "        path2 = metric_file[idx+1]['test']\n",
    "        filename1 = path1.split('\\\\')[-1]\n",
    "        filename2 = path2.split('\\\\')[-1]\n",
    "        #if name1 in flipped_list:\n",
    "        #    data_ed = nib.load(os.path.join(r'Quorum_output_2\\only_sfb\\temp_allClasses', filename2))\n",
    "        #    data_es = nib.load(os.path.join(r'Quorum_output_2\\only_sfb\\temp_allClasses', filename1))\n",
    "        #else:\n",
    "        #data_ed = nib.load(os.path.join(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses', filename1))\n",
    "        #data_es = nib.load(os.path.join(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses', filename2))\n",
    "        data_ed = nib.load(os.path.join(r'cross_dataset\\fold_0\\temp_allClasses', filename1))\n",
    "        data_es = nib.load(os.path.join(r'cross_dataset\\fold_0\\temp_allClasses', filename2))\n",
    "        data_ed_gt = nib.load(os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task030_Quorum_RV_removed\\labelsTr', filename1))\n",
    "        data_es_gt = nib.load(os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task030_Quorum_RV_removed\\labelsTr', filename2))\n",
    "        zoom = list(data_ed.header.get_zooms())\n",
    "        zoom_gt = list(data_ed_gt.header.get_zooms())\n",
    "        arr_ed = data_ed.get_fdata()\n",
    "        arr_es = data_es.get_fdata()\n",
    "        arr_ed_gt = data_ed_gt.get_fdata()\n",
    "        arr_es_gt = data_es_gt.get_fdata()\n",
    "\n",
    "        assert spacing_ed == spacing_es\n",
    "        assert zoom == zoom_gt\n",
    "\n",
    "        right_slices = np.arange(min(arr_es.shape[-1], arr_ed.shape[-1]))\n",
    "        arr_ed_keep = arr_ed[:, :, right_slices]\n",
    "        arr_es_keep = arr_es[:, :, right_slices]\n",
    "\n",
    "        right_slices_gt = np.arange(min(arr_es_gt.shape[-1], arr_ed_gt.shape[-1]))\n",
    "        arr_ed_keep_gt = arr_ed_gt[:, :, right_slices_gt]\n",
    "        arr_es_keep_gt = arr_es_gt[:, :, right_slices_gt]\n",
    "\n",
    "        assert arr_ed_keep.shape == arr_es_keep.shape == arr_ed_keep_gt.shape == arr_es_keep_gt.shape\n",
    "\n",
    "        class_volume = {'Patient ID': name1}\n",
    "        class_volume_gt = {'Patient ID': name1}\n",
    "        for i, class_name in enumerate(['RV', 'MYO', 'LV'], 1):\n",
    "            volume_ed = get_volume(arr_ed_keep == i, zoom, spacing_ed)\n",
    "            volume_es = get_volume(arr_es_keep == i, zoom, spacing_es)\n",
    "            volume_ed_gt = get_volume(arr_ed_keep_gt == i, zoom, spacing_ed)\n",
    "            volume_es_gt = get_volume(arr_es_keep_gt == i, zoom, spacing_es)\n",
    "            if class_name == 'MYO':\n",
    "                volume_ed = volume_ed * 1.055\n",
    "                volume_es = volume_es * 1.055\n",
    "                volume_ed_gt = volume_ed_gt * 1.055\n",
    "                volume_es_gt = volume_es_gt * 1.055\n",
    "            else:\n",
    "                class_volume[class_name + 'EF_pred'] = ((volume_ed - volume_es) / volume_ed) * 100\n",
    "                class_volume[class_name + 'EV_pred'] = (volume_ed - volume_es)\n",
    "                class_volume_gt[class_name + 'EF_gt'] = ((volume_ed_gt - volume_es_gt) / volume_ed_gt) * 100\n",
    "                class_volume_gt[class_name + 'EV_gt'] = (volume_ed_gt - volume_es_gt)\n",
    "\n",
    "            class_volume[class_name + 'EDV_pred'] = volume_ed\n",
    "            class_volume[class_name + 'ESV_pred'] = volume_es\n",
    "            class_volume_gt[class_name + 'EDV_gt'] = volume_ed_gt\n",
    "            class_volume_gt[class_name + 'ESV_gt'] = volume_es_gt\n",
    "\n",
    "        results_list.append(class_volume)\n",
    "        results_list_gt.append(class_volume_gt)\n",
    "\n",
    "df_pred = pd.DataFrame.from_records(results_list)\n",
    "df_pred_gt = pd.DataFrame.from_records(results_list_gt)\n",
    "\n",
    "new_df = df.loc[(df['Patient ID'].isin(name_list)) & (df['Study description'] == 'Baseline_MRI')]\n",
    "\n",
    "out = pd.merge(new_df, df_pred, on='Patient ID')\n",
    "out = pd.merge(out, df_pred_gt, on='Patient ID')\n",
    "\n",
    "out.to_csv(r'cross_dataset\\fold_0\\volume.csv')\n",
    "#out.to_csv(r'Quorum_output_2\\volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['num_stages', 'num_modalities', 'modalities', 'normalization_schemes', 'dataset_properties', 'list_of_npz_files', 'original_spacings', 'original_sizes', 'preprocessed_data_folder', 'num_classes', 'all_classes', 'base_num_features', 'use_mask_for_norm', 'keep_only_largest_region', 'min_region_size_per_class', 'min_size_per_class', 'transpose_forward', 'transpose_backward', 'data_identifier', 'plans_per_stage', 'preprocessor_name'])\n",
      "{0: {'batch_size': 1, 'num_pool_per_axis': [3, 3], 'patch_size': array([224, 224]), 'median_patient_size_in_voxels': array([  9, 249, 219]), 'current_spacing': array([10.        ,  1.48438001,  1.48438001]), 'original_spacing': array([10.        ,  1.48438001,  1.48438001]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'ACDC_output\\Baseline\\plans.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print(data.keys())\n",
    "    print(data['plans_per_stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:32<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2 as cv\n",
    "from skimage.morphology import binary_erosion\n",
    "\n",
    "def get_volume(arr, zoom, spacing):\n",
    "    pixel_size = np.prod(zoom)\n",
    "    gap_size = np.prod((zoom[0], zoom[1], spacing))\n",
    "    \n",
    "    volume_ml_interp = 0\n",
    "    for s in range(len(arr)):\n",
    "        top = arr[s]\n",
    "        assert np.all(np.isin(top, [0, 1]))\n",
    "        volume_ml_interp += (pixel_size * top.sum()) / 1000\n",
    "        if s < len(arr) - 1:\n",
    "            bottom = arr[s + 1]\n",
    "            assert np.all(np.isin(bottom, [0, 1]))\n",
    "            interpolated = interp_shape(top, bottom, precision=0.5)\n",
    "            volume_ml_interp += (gap_size * interpolated.sum()) / 1000\n",
    "    \n",
    "    return volume_ml_interp\n",
    "#df_keep = pd.read_csv(r'data_saud_2\\keep.csv', converters={\"Slices\": lambda x: list(map(int, x.strip(\"[]\").split(\", \")))})\n",
    "\n",
    "results_list = []\n",
    "results_list_gt = []\n",
    "#with open(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "with open(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\ACDC_output\\Baseline\\temp_allClasses\\summary.json\", 'r') as fd_in:\n",
    "    metric_file = json.load(fd_in)['results']['all']\n",
    "    for idx in tqdm(range(0, len(metric_file), 2)):\n",
    "        spacing_ed = 0.0\n",
    "        spacing_es = 0.0\n",
    "        filename1 = os.path.basename(metric_file[idx]['reference'])\n",
    "        filename2 = os.path.basename(metric_file[idx+1]['reference'])\n",
    "\n",
    "        name1 = filename1.split('_')[0]\n",
    "        name2 = filename2.split('_')[0]\n",
    "\n",
    "        assert name1 == name2\n",
    "\n",
    "        path1 = metric_file[idx]['test']\n",
    "        path2 = metric_file[idx+1]['test']\n",
    "        #if name1 in flipped_list:\n",
    "        #    data_ed = nib.load(os.path.join(r'Quorum_output_2\\only_sfb\\temp_allClasses', filename2))\n",
    "        #    data_es = nib.load(os.path.join(r'Quorum_output_2\\only_sfb\\temp_allClasses', filename1))\n",
    "        #else:\n",
    "        #data_ed = nib.load(os.path.join(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses', filename1))\n",
    "        #data_es = nib.load(os.path.join(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses', filename2))\n",
    "        data_ed = nib.load(os.path.join(r'C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\ACDC_output\\Baseline\\temp_allClasses', filename1))\n",
    "        data_es = nib.load(os.path.join(r'C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\ACDC_output\\Baseline\\temp_allClasses', filename2))\n",
    "        data_ed_gt = nib.load(os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr', filename1))\n",
    "        data_es_gt = nib.load(os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr', filename2))\n",
    "        zoom = list(data_ed.header.get_zooms())\n",
    "        zoom_gt = list(data_ed_gt.header.get_zooms())\n",
    "        arr_ed = data_ed.get_fdata()\n",
    "        arr_es = data_es.get_fdata()\n",
    "        arr_ed_gt = data_ed_gt.get_fdata()\n",
    "        arr_es_gt = data_es_gt.get_fdata()\n",
    "\n",
    "        assert spacing_ed == spacing_es\n",
    "        assert zoom == zoom_gt\n",
    "\n",
    "        right_slices = np.arange(min(arr_es.shape[-1], arr_ed.shape[-1]))\n",
    "        arr_ed_keep = arr_ed[:, :, right_slices]\n",
    "        arr_es_keep = arr_es[:, :, right_slices]\n",
    "\n",
    "        right_slices_gt = np.arange(min(arr_es_gt.shape[-1], arr_ed_gt.shape[-1]))\n",
    "        arr_ed_keep_gt = arr_ed_gt[:, :, right_slices_gt]\n",
    "        arr_es_keep_gt = arr_es_gt[:, :, right_slices_gt]\n",
    "\n",
    "        assert arr_ed_keep.shape == arr_es_keep.shape == arr_ed_keep_gt.shape == arr_es_keep_gt.shape\n",
    "\n",
    "        class_volume = {'Patient ID': name1}\n",
    "        class_volume_gt = {'Patient ID': name1}\n",
    "        for i, class_name in enumerate(['RV', 'MYO', 'LV'], 1):\n",
    "            volume_ed = get_volume(arr_ed_keep == i, zoom, spacing_ed)\n",
    "            volume_es = get_volume(arr_es_keep == i, zoom, spacing_es)\n",
    "            volume_ed_gt = get_volume(arr_ed_keep_gt == i, zoom, spacing_ed)\n",
    "            volume_es_gt = get_volume(arr_es_keep_gt == i, zoom, spacing_es)\n",
    "            if class_name == 'MYO':\n",
    "                volume_ed = volume_ed * 1.055\n",
    "                volume_es = volume_es * 1.055\n",
    "                volume_ed_gt = volume_ed_gt * 1.055\n",
    "                volume_es_gt = volume_es_gt * 1.055\n",
    "            else:\n",
    "                class_volume[class_name + 'EF_pred'] = ((volume_ed - volume_es) / volume_ed) * 100\n",
    "                class_volume[class_name + 'EV_pred'] = (volume_ed - volume_es)\n",
    "                class_volume_gt[class_name + 'EF_gt'] = ((volume_ed_gt - volume_es_gt) / volume_ed_gt) * 100\n",
    "                class_volume_gt[class_name + 'EV_gt'] = (volume_ed_gt - volume_es_gt)\n",
    "\n",
    "            class_volume[class_name + 'EDV_pred'] = volume_ed\n",
    "            class_volume[class_name + 'ESV_pred'] = volume_es\n",
    "            class_volume_gt[class_name + 'EDV_gt'] = volume_ed_gt\n",
    "            class_volume_gt[class_name + 'ESV_gt'] = volume_es_gt\n",
    "\n",
    "        results_list.append(class_volume)\n",
    "        results_list_gt.append(class_volume_gt)\n",
    "\n",
    "df_pred = pd.DataFrame.from_records(results_list)\n",
    "df_pred_gt = pd.DataFrame.from_records(results_list_gt)\n",
    "\n",
    "out = pd.merge(df_pred, df_pred_gt, on='Patient ID')\n",
    "\n",
    "out.to_csv(r'ACDC_output\\Baseline\\volume.csv')\n",
    "#out.to_csv(r'Quorum_output_2\\volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.182186062279962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:02<03:23,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7318886979699173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:04<03:28,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.844169102090524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:06<03:40,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4422857653307313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:08<03:34,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.844169102090524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:11<03:32,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8862773804080817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:13<03:32,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.195016827258979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:15<03:21,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.195022063001857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:17<03:10,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5813674340056938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:19<03:06,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.787549510848197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:21<03:03,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8862773804080817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:23<02:58,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6105173805020423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:24<02:49,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.693619021103658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:26<02:50,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9031365319240376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:29<02:56,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5726769636954658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:31<02:54,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6414100808613927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:33<02:46,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.032000772712185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:35<02:44,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8314986276451997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:36<02:37,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2620046948346793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:39<02:41,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2681567906276614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:40<02:32,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.395254174088047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:42<02:39,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4253552891538395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:44<02:27,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8212441440728049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:46<02:27,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0071135940086577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:48<02:20,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9684999854999254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:50<02:29,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.809707801753247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:52<02:29,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6073371890346104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:54<02:28,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9297129817826044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:56<02:28,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.584369523663308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:58<02:23,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4801299086330861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [01:00<02:21,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0551984354652992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [01:02<02:19,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4545556367259445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [01:05<02:22,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.961249907167639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [01:07<02:19,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9213919263181758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [01:09<02:18,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.915380387381685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [01:11<02:19,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8330172650003775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [01:13<02:10,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1408667473357195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [01:15<02:01,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6105173805020423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [01:17<02:18,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.150645892536524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [01:19<02:08,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.078054722690883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [01:21<02:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.809707801753247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [01:22<01:45,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.079028824529516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [01:24<01:47,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8339519798752781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [01:26<01:42,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.689297882896972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [01:29<01:53,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8179453206415095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [01:31<01:52,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9778196578827574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [01:33<01:59,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0174825039900472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [01:36<02:01,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8186452004067353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [01:38<01:52,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8393095029692157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [01:40<01:46,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6014431378683767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [01:42<01:47,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.023179348848269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [01:44<01:44,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8893748376805812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [01:46<01:37,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9325091817714626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [01:47<01:30,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1034991729812336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [01:49<01:26,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0174825039900472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [01:51<01:26,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7187323830585406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [01:53<01:25,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9560599156154186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [01:59<02:10,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7722779770159982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [02:01<01:54,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8103645479923425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [02:04<01:56,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5952978228804815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [02:07<01:51,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.207923493579491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [02:09<01:41,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8687358987692388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [02:11<01:37,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.521862855491863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [02:13<01:27,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.122984855113656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [02:15<01:22,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.118598773285929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [02:17<01:15,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9560599156154186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [02:19<01:14,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7524075142482711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [02:22<01:14,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6216059724284189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [02:24<01:09,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5387588213796124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [02:26<01:03,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.806692432586144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [02:27<00:58,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6996103392967246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [02:29<00:56,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8133996068105735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [02:31<00:54,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2005492902212933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [02:33<00:51,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0270506068569203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [02:35<00:52,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.119776260983299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [02:37<00:50,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0270506068569203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [02:40<00:51,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9424056238059446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [02:42<00:47,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0791437799464463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [02:44<00:45,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9803140549028222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [02:46<00:43,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7329270693777732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [02:47<00:39,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8634322556783456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [02:50<00:38,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0418049435352117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [02:52<00:37,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5988591561979186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [02:54<00:34,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1709492381647046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [02:56<00:33,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7214310315235932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [02:59<00:34,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7709604002911539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [03:00<00:29,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.742080745098588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [03:03<00:27,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8862773804080817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [03:05<00:27,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1325420927519563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [03:08<00:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6119020925345235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [03:09<00:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5526799816584504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [03:10<00:16,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7986748212204233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [03:12<00:14,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7160516682260583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [03:14<00:12,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4613136191518248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [03:15<00:10,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8339519798752781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [03:17<00:08,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.000363792125321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [03:20<00:07,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0740650613993648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [03:21<00:05,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5484529617685159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [03:23<00:03,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.996421022275045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [03:26<00:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6934092071865368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:28<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2 as cv\n",
    "from skimage.morphology import binary_erosion\n",
    "\n",
    "def get_volume(arr, zoom, spacing):\n",
    "    pixel_size = np.prod(zoom)\n",
    "    gap_size = np.prod((zoom[0], zoom[1], spacing))\n",
    "    \n",
    "    volume_ml_interp = 0\n",
    "    for s in range(len(arr)):\n",
    "        top = arr[s]\n",
    "        assert np.all(np.isin(top, [0, 1]))\n",
    "        volume_ml_interp += (pixel_size * top.sum()) / 1000\n",
    "        if s < len(arr) - 1:\n",
    "            bottom = arr[s + 1]\n",
    "            assert np.all(np.isin(bottom, [0, 1]))\n",
    "            interpolated = interp_shape(top, bottom, precision=0.5)\n",
    "            volume_ml_interp += (gap_size * interpolated.sum()) / 1000\n",
    "    \n",
    "    return volume_ml_interp\n",
    "#df_keep = pd.read_csv(r'data_saud_2\\keep.csv', converters={\"Slices\": lambda x: list(map(int, x.strip(\"[]\").split(\", \")))})\n",
    "\n",
    "results_list = []\n",
    "results_list_gt = []\n",
    "#with open(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses\\summary.json', 'r') as fd_in:\n",
    "with open(r\"C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\ACDC_output\\Baseline\\temp_allClasses\\summary.json\", 'r') as fd_in:\n",
    "    metric_file = json.load(fd_in)['results']['all']\n",
    "    for idx in tqdm(range(0, len(metric_file), 2)):\n",
    "        spacing_ed = 0.0\n",
    "        spacing_es = 0.0\n",
    "        filename1 = os.path.basename(metric_file[idx]['reference'])\n",
    "        filename2 = os.path.basename(metric_file[idx+1]['reference'])\n",
    "\n",
    "        name1 = filename1.split('_')[0]\n",
    "        name2 = filename2.split('_')[0]\n",
    "\n",
    "        assert name1 == name2\n",
    "\n",
    "        path1 = metric_file[idx]['test']\n",
    "        path2 = metric_file[idx+1]['test']\n",
    "        #if name1 in flipped_list:\n",
    "        #    data_ed = nib.load(os.path.join(r'Quorum_output_2\\only_sfb\\temp_allClasses', filename2))\n",
    "        #    data_es = nib.load(os.path.join(r'Quorum_output_2\\only_sfb\\temp_allClasses', filename1))\n",
    "        #else:\n",
    "        #data_ed = nib.load(os.path.join(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses', filename1))\n",
    "        #data_es = nib.load(os.path.join(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses', filename2))\n",
    "        data_ed = nib.load(os.path.join(r'C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\ACDC_output\\Baseline\\temp_allClasses', filename1))\n",
    "        data_es = nib.load(os.path.join(r'C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\ACDC_output\\Baseline\\temp_allClasses', filename2))\n",
    "        data_ed_gt = nib.load(os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr', filename1))\n",
    "        data_es_gt = nib.load(os.path.join(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr', filename2))\n",
    "        zoom = list(data_ed.header.get_zooms())\n",
    "        zoom_gt = list(data_ed_gt.header.get_zooms())\n",
    "        arr_ed = data_ed.get_fdata()\n",
    "        arr_es = data_es.get_fdata()\n",
    "        arr_ed_gt = data_ed_gt.get_fdata()\n",
    "        arr_es_gt = data_es_gt.get_fdata()\n",
    "\n",
    "        assert spacing_ed == spacing_es\n",
    "        assert zoom == zoom_gt\n",
    "\n",
    "        right_slices = np.arange(min(arr_es.shape[-1], arr_ed.shape[-1]))\n",
    "        arr_ed_keep = arr_ed[:, :, right_slices]\n",
    "        arr_es_keep = arr_es[:, :, right_slices]\n",
    "\n",
    "        right_slices_gt = np.arange(min(arr_es_gt.shape[-1], arr_ed_gt.shape[-1]))\n",
    "        arr_ed_keep_gt = arr_ed_gt[:, :, right_slices_gt]\n",
    "        arr_es_keep_gt = arr_es_gt[:, :, right_slices_gt]\n",
    "\n",
    "        assert arr_ed_keep.shape == arr_es_keep.shape == arr_ed_keep_gt.shape == arr_es_keep_gt.shape\n",
    "\n",
    "        with open(os.path.join('ACDC_training', name1, 'info.cfg')) as f:\n",
    "            lines = f.readlines()\n",
    "            height = float(lines[3].strip().split(' ')[-1])\n",
    "            weight = float(lines[5].strip().split(' ')[-1])\n",
    "        \n",
    "        BSA = 0.007184 * (weight**0.425*height**0.725)\n",
    "        print(BSA)\n",
    "\n",
    "        class_volume = {'Patient ID': name1}\n",
    "        class_volume_gt = {'Patient ID': name1}\n",
    "        for i, class_name in enumerate(['RV', 'MYO', 'LV'], 1):\n",
    "            volume_ed = get_volume(arr_ed_keep == i, zoom, spacing_ed)\n",
    "            volume_es = get_volume(arr_es_keep == i, zoom, spacing_es)\n",
    "            volume_ed_gt = get_volume(arr_ed_keep_gt == i, zoom, spacing_ed)\n",
    "            volume_es_gt = get_volume(arr_es_keep_gt == i, zoom, spacing_es)\n",
    "            if class_name == 'MYO':\n",
    "                volume_ed = (volume_ed * 1.055) / BSA\n",
    "                volume_es = (volume_es * 1.055) / BSA\n",
    "                volume_ed_gt = (volume_ed_gt * 1.055) / BSA\n",
    "                volume_es_gt = (volume_es_gt * 1.055) / BSA\n",
    "            else:\n",
    "                class_volume[class_name + 'EF_pred'] = (((volume_ed - volume_es) / volume_ed) * 100) / BSA\n",
    "                class_volume[class_name + 'EV_pred'] = (volume_ed - volume_es) / BSA\n",
    "                class_volume_gt[class_name + 'EF_gt'] = (((volume_ed_gt - volume_es_gt) / volume_ed_gt) * 100) / BSA\n",
    "                class_volume_gt[class_name + 'EV_gt'] = ((volume_ed_gt - volume_es_gt)) / BSA\n",
    "\n",
    "            class_volume[class_name + 'EDV_pred'] = volume_ed / BSA\n",
    "            class_volume[class_name + 'ESV_pred'] = volume_es / BSA\n",
    "            class_volume_gt[class_name + 'EDV_gt'] = volume_ed_gt / BSA\n",
    "            class_volume_gt[class_name + 'ESV_gt'] = volume_es_gt / BSA\n",
    "\n",
    "        results_list.append(class_volume)\n",
    "        results_list_gt.append(class_volume_gt)\n",
    "\n",
    "df_pred = pd.DataFrame.from_records(results_list)\n",
    "df_pred_gt = pd.DataFrame.from_records(results_list_gt)\n",
    "\n",
    "out = pd.merge(df_pred, df_pred_gt, on='Patient ID')\n",
    "\n",
    "out.to_csv(r'ACDC_output\\Baseline\\volume.csv')\n",
    "#out.to_csv(r'Quorum_output_2\\volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:01<00:00, 155.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def get_volume(arr_ed, arr_es, zoom):\n",
    "    pixel_size = np.prod(zoom)\n",
    "\n",
    "    class_nb_pixels_ed = arr_ed == i\n",
    "    class_nb_pixels_es = arr_es == i\n",
    "\n",
    "    volume_no_interp_ed = (pixel_size * class_nb_pixels_ed.sum()) / 1000\n",
    "    volume_no_interp_es = (pixel_size * class_nb_pixels_es.sum()) / 1000\n",
    "\n",
    "    volume_interp_ed = volume_no_interp_ed\n",
    "\n",
    "    volume_interp_es = volume_no_interp_es\n",
    "    \n",
    "    return volume_no_interp_ed, volume_no_interp_es, volume_interp_ed, volume_interp_es\n",
    "\n",
    "df = pd.read_excel('Quorum_Qmass_12_07_2021.xlsx')\n",
    "\n",
    "name_list = []\n",
    "results_list = []\n",
    "path_list = glob(r'data_saud_2\\inference_2\\*.gz')\n",
    "#path_list = glob(r'data_saud_2\\inference\\*.gz')\n",
    "path_list = sorted(path_list)\n",
    "test_names = os.listdir(r'Quorum_output_2\\all_data\\only_sfb\\temp_allClasses')\n",
    "for idx in tqdm(range(0, len(path_list), 2)):\n",
    "    path1 = path_list[idx]\n",
    "    path2 = path_list[idx+1]\n",
    "    if path1.split('\\\\')[-1] not in test_names:\n",
    "        continue\n",
    "    csv_path1 = path1.replace('.nii.gz', '.csv')\n",
    "    csv_path2 = path2.replace('.nii.gz', '.csv')\n",
    "    patient_df1 = pd.read_csv(csv_path1)\n",
    "    patient_df2 = pd.read_csv(csv_path2)\n",
    "    \n",
    "    name1 = patient_df1['Name'].iloc[0]\n",
    "    name2 = patient_df2['Name'].iloc[0]\n",
    "\n",
    "    assert name1 == name2\n",
    "    name_list.append(name1)\n",
    "\n",
    "    data_ed = nib.load(path1)\n",
    "    data_es = nib.load(path2)\n",
    "    zoom = list(data_ed.header.get_zooms())\n",
    "    arr_ed = data_ed.get_fdata()\n",
    "    arr_es = data_es.get_fdata()\n",
    "\n",
    "    class_volume = {'Patient ID': name1}\n",
    "    for i, class_name in enumerate(['RV', 'MYO', 'LV'], 1):\n",
    "        volume_no_interp_ed, volume_no_interp_es, volume_interp_ed, volume_interp_es = get_volume(arr_ed, arr_es, zoom)\n",
    "\n",
    "        class_volume[class_name + 'EF_pred'] = ((volume_no_interp_ed - volume_no_interp_es) / volume_no_interp_ed) * 100\n",
    "        class_volume[class_name + 'EV_pred'] = (volume_no_interp_ed - volume_no_interp_es)\n",
    "\n",
    "        volume_no_interp_ed, volume_no_interp_es, volume_interp_ed, volume_interp_es = get_volume(arr_ed, arr_es, zoom)\n",
    "\n",
    "        if class_name == 'MYO':\n",
    "            class_volume[class_name + 'EDV_pred'] = volume_no_interp_ed * 1.055\n",
    "            class_volume[class_name + 'ESV_pred'] = volume_no_interp_es * 1.055\n",
    "        else:\n",
    "            class_volume[class_name + 'EDV_pred'] = volume_no_interp_ed\n",
    "            class_volume[class_name + 'ESV_pred'] = volume_no_interp_es\n",
    "    results_list.append(class_volume)\n",
    "\n",
    "df_pred = pd.DataFrame.from_records(results_list)\n",
    "\n",
    "new_df = df.loc[(df['Patient ID'].isin(name_list)) & (df['Study description'] == 'Baseline_MRI')]\n",
    "\n",
    "out = pd.merge(new_df, df_pred, on='Patient ID')\n",
    "\n",
    "out.to_csv(r'Quorum_output_2\\volume.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get volume and diseases for jmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.37s/it]\n",
      "100%|██████████| 3/3 [00:11<00:00,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "df_disease = pd.read_excel(r'disease_saud\\radiomics_all.xlsx')\n",
    "payload_disease = df_disease.loc[df_disease['Descriptor'] == 'Infarction_Ratio']\n",
    "\n",
    "#path_list = glob(r'quorum_output_2\\temp_allClasses\\*.gz')\n",
    "\n",
    "for data_dict, output_file_name in zip([data_dict_cut, data_dict_all], ['disease_volume_cut.csv', 'disease_volume_all.csv']):\n",
    "    if os.path.exists(output_file_name):\n",
    "        os.remove(output_file_name)\n",
    "    results = []\n",
    "    for i, class_name in enumerate(tqdm(['RV', 'MYO', 'LV']), 1):\n",
    "        for phase in data_dict.keys():\n",
    "            list_of_tuple = data_dict[phase]\n",
    "            for path, spacing, name in list_of_tuple:\n",
    "                data = nib.load(path)\n",
    "                zoom = list(data.header.get_zooms())\n",
    "                arr = data.get_fdata()\n",
    "                pixel_size = np.prod(zoom)\n",
    "\n",
    "                new_depth = round((spacing * (arr.shape[-1] - 1)) / zoom[-1]) + arr.shape[-1]\n",
    "                #print(arr.shape[-1])\n",
    "                #print(new_depth)\n",
    "                #print('******************************')\n",
    "\n",
    "                class_nb_pixels = arr == i\n",
    "\n",
    "                volume_no_interp = (pixel_size * class_nb_pixels.sum()) / 1000\n",
    "                if class_name == 'MYO':\n",
    "                    volume_no_interp = volume_no_interp * 1.055\n",
    "\n",
    "                if new_depth > arr.shape[-1]:\n",
    "                    arr_interpolated = interp_shape(class_nb_pixels, new_depth)\n",
    "                    volume_interp = (pixel_size * arr_interpolated.sum()) / 1000\n",
    "                    if class_name == 'MYO':\n",
    "                        volume_interp = volume_interp * 1.055\n",
    "                else:\n",
    "                    volume_interp = volume_no_interp\n",
    "\n",
    "                if name in payload_disease:\n",
    "                    infraction_percent = payload_disease[name][0]\n",
    "                else:\n",
    "                    infraction_percent = ''\n",
    "\n",
    "                results.append({'Phase': phase, 'Class': class_name, 'Volume': volume_no_interp, 'Interpolated_volume': volume_interp, 'Infraction_percent': infraction_percent})\n",
    "            \n",
    "        #if class_name != 'MYO':\n",
    "        #    results[class_name].update({\n",
    "        #        'Fraction d\\'ejection': {'no_interpolation': ((results[class_name]['ed']['volume'] - results[class_name]['es']['volume']) / results[class_name]['ed']['volume']) * 100,\n",
    "        #                                'interpolated': ((results[class_name]['ed']['interpolated_volume'] - results[class_name]['es']['interpolated_volume']) / results[class_name]['ed']['interpolated_volume']) * 100},\n",
    "        #        'Volume d\\'ejection': {'no_interpolation': (results[class_name]['ed']['volume'] - results[class_name]['es']['volume']),\n",
    "        #                                'interpolated': (results[class_name]['ed']['interpolated_volume'] - results[class_name]['es']['interpolated_volume'])}\n",
    "        #                                })          \n",
    "\n",
    "    with open(os.path.join(r'Quorum_output_2', output_file_name), 'w') as fd:\n",
    "        writer = csv.DictWriter(fd, fieldnames=list(results[0].keys()))\n",
    "        writer.writeheader() \n",
    "        writer.writerows(results) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get disease and dice for jmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(r'disease_saud\\radiomics_all.xlsx')\n",
    "payload = df.loc[df['Descriptor'] == 'Infarction_Ratio']\n",
    "\n",
    "folders = [r'Quorum_output_2\\only_sfb\\fold_0']\n",
    "for folder in folders:\n",
    "    results_list = []\n",
    "    with open(os.path.join(folder, r'temp_allClasses\\summary.json')) as fd_json:\n",
    "        data = json.load(fd_json)\n",
    "        results = data['results']['all']\n",
    "        for res in results:\n",
    "            rv_dice = res['1']['Dice']\n",
    "            myo_dice = res['2']['Dice']\n",
    "            lv_dice = res['3']['Dice']\n",
    "            patient_name = res['center']\n",
    "            if patient_name in payload:\n",
    "                infraction_percent = payload[patient_name][0]\n",
    "            else:\n",
    "                infraction_percent = ''\n",
    "            results_list.append({'Patient': patient_name, 'Infraction_percent': infraction_percent, 'RV': rv_dice, 'MYO': myo_dice, 'LV': lv_dice, 'Mean': (rv_dice + myo_dice + lv_dice) / 3})\n",
    "\n",
    "    with open(os.path.join(folder, 'disease_jmp.csv'), 'w') as fd_csv:\n",
    "        writer = csv.DictWriter(fd_csv, fieldnames=list(results_list[0].keys()))\n",
    "        writer.writeheader() \n",
    "        writer.writerows(results_list) \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get jmp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "folders = [r'Quorum_output_2\\all_data\\only_sfb', r'Quorum_output_2\\all_data\\no_transformer', r'Quorum_output_2\\all_data\\no_sfb', r'Quorum_output_2\\all_data\\no_ds']\n",
    "results_list = []\n",
    "for folder in folders:\n",
    "    with open(os.path.join(folder, r'validation_raw\\summary.json')) as fd_json:\n",
    "        data = json.load(fd_json)\n",
    "        results = data['results']['all']\n",
    "        for res in results:\n",
    "            name = res['test'].split('\\\\')[-1].split('.')[0]\n",
    "            rv_hd = res['1']['Hausdorff Distance']\n",
    "            myo_hd = res['2']['Hausdorff Distance']\n",
    "            lv_hd = res['3']['Hausdorff Distance']\n",
    "            rv_dice = res['1']['Dice']\n",
    "            myo_dice = res['2']['Dice']\n",
    "            lv_dice = res['3']['Dice']\n",
    "            rv_assd = res['1']['Avg. Symmetric Surface Distance']\n",
    "            myo_assd = res['2']['Avg. Symmetric Surface Distance']\n",
    "            lv_assd = res['3']['Avg. Symmetric Surface Distance']\n",
    "            results_list.append({'Name': name, \n",
    "                                'Method': folder.split('\\\\')[-1], \n",
    "                                'RV_HD': rv_hd, \n",
    "                                'MYO_HD': myo_hd, \n",
    "                                'LV_HD': lv_hd, \n",
    "                                'Mean_HD': (rv_hd + myo_hd + lv_hd) / 3,\n",
    "                                'RV_Dice': rv_dice,\n",
    "                                'MYO_Dice': myo_dice,\n",
    "                                'LV_Dice': lv_dice,\n",
    "                                'Mean Dice': (rv_dice + myo_dice + lv_dice) / 3,\n",
    "                                'RV_assd': rv_assd,\n",
    "                                'MYO_assd': myo_assd,\n",
    "                                'LV_assd': lv_assd,\n",
    "                                'Mean assd': (rv_assd + myo_assd + lv_assd) / 3})\n",
    "\n",
    "with open(os.path.join('Quorum_output_2', 'Quorum_methods_jmp.csv'), 'w') as fd_csv:\n",
    "    writer = csv.DictWriter(fd_csv, fieldnames=list(results_list[0].keys()))\n",
    "    writer.writeheader() \n",
    "    writer.writerows(results_list) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dice for 'temp_allClasses' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "results_list = []\n",
    "with open(r'Quorum_output_2\\only_sfb\\fold_0\\temp_allClasses\\summary.json') as fd_json:\n",
    "    data = json.load(fd_json)\n",
    "    results = data['results']['all']\n",
    "    for res in results:\n",
    "        rv_dice = res['1']['Dice']\n",
    "        myo_dice = res['2']['Dice']\n",
    "        lv_dice = res['3']['Dice']\n",
    "        results_list.append({'RV': rv_dice, 'MYO': myo_dice, 'LV': lv_dice, 'Mean': (rv_dice + myo_dice + lv_dice) / 3})\n",
    "\n",
    "with open(os.path.join('Quorum_output_2\\only_sfb', 'Quorum_postprocess_allClasses_jmp.csv'), 'w') as fd_csv:\n",
    "    writer = csv.DictWriter(fd_csv, fieldnames=list(results_list[0].keys()))\n",
    "    writer.writeheader() \n",
    "    writer.writerows(results_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.01147097773805 47.330550062705605 48.839061275274325\n",
      " 49.033918182821814 51.262682340547975 55.518688973869814\n",
      " 56.44497458808888 59.39555565233817 74.49475816552884]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation.metrics import dice, hausdorff_distance\n",
    "import cv2 as cv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_memory = np.zeros(shape=(9,), dtype=object)\n",
    "score_memory = np.full_like(path_memory, fill_value=0)\n",
    "arr_memory = np.zeros(shape=(9,), dtype=object)\n",
    "idx_memory = np.zeros(shape=(9,), dtype=np.uint8)\n",
    "path_list = glob(r'ACDC_output\\Baseline\\temp_allClasses\\*.gz')\n",
    "for path in path_list:\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "\n",
    "    zoom = list(data.header.get_zooms())\n",
    "    zoom = zoom[:-1]\n",
    "\n",
    "    filename = path.split('\\\\')[-1].split('.')[0] \n",
    "    path_array_gt = os.path.join('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\labelsTr\\\\', filename + '.nii.gz')\n",
    "\n",
    "    gt_data = nib.load(path_array_gt)\n",
    "    gt_arr = gt_data.get_fdata()\n",
    "\n",
    "    for j in range(arr.shape[-1]):\n",
    "        score = hausdorff_distance(arr[:, :, j], gt_arr[:, :, j], voxel_spacing=zoom)\n",
    "        if score > score_memory[0]:\n",
    "            arr_memory[0] = arr[:, :, j]\n",
    "            score_memory[0] = score\n",
    "            path_memory[0] = path\n",
    "            idx_memory[0] = j\n",
    "            sorted_indices = score_memory.argsort()\n",
    "            arr_memory = arr_memory[sorted_indices]\n",
    "            score_memory = score_memory[sorted_indices]\n",
    "            path_memory = path_memory[sorted_indices]\n",
    "            idx_memory = idx_memory[sorted_indices]\n",
    "print(score_memory)\n",
    "h = 3\n",
    "#w = int(round(h * (16/9)))\n",
    "w = h\n",
    "\n",
    "fig, ax = plt.subplots(h, w, figsize=(8, 8))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99, wspace=0.05, hspace=0.01)\n",
    "\n",
    "for i in range(9):\n",
    "    current_path = path_memory[i]\n",
    "    current_idx = idx_memory[i]\n",
    "    filename = current_path.split('\\\\')[-1].split('.')[0] \n",
    "    path_array_gt = os.path.join('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\labelsTr\\\\', filename + '.nii.gz')\n",
    "    path_array_img = os.path.join('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\imagesTr\\\\', filename + '_0000.nii.gz')\n",
    "\n",
    "    gt_data = nib.load(path_array_gt)\n",
    "    gt_arr = gt_data.get_fdata()[:, :, current_idx]\n",
    "\n",
    "    img_data = nib.load(path_array_img)\n",
    "    img_arr = img_data.get_fdata()[:, :, current_idx]\n",
    "\n",
    "    pred_arr = arr_memory[i]\n",
    "\n",
    "    img_arr = cv.normalize(img_arr, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX).astype(np.uint8)\n",
    "    img_arr = cv.cvtColor(img_arr, cv.COLOR_GRAY2RGB)\n",
    "\n",
    "    for j in range(1, 4):\n",
    "        pred = (pred_arr == j).astype(np.uint8)\n",
    "        gt = (gt_arr == j).astype(np.uint8)\n",
    "        pred_contours, hierarchy = cv.findContours(pred, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        gt_contours, hierarchy = cv.findContours(gt, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        pred_color = [255, 0, 0]\n",
    "        gt_color = [0, 255, 0]\n",
    "        img_arr = cv.drawContours(img_arr, pred_contours, -1, pred_color, 1)\n",
    "        img_arr = cv.drawContours(img_arr, gt_contours, -1, gt_color, 1)\n",
    "\n",
    "    ax[int(i//w), int(i%w)].imshow(img_arr)\n",
    "    ax[int(i//w), int(i%w)].set_axis_off()\n",
    "\n",
    "plt.savefig(\"squares.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gt = nib.load(r'custom_quorum_RV_removed\\patient004\\patient004_ed_gt.nii.gz')\n",
    "arr_gt = data_gt.get_fdata()\n",
    "\n",
    "data_img = nib.load(r'custom_quorum_RV_removed\\patient004\\patient004_ed.nii.gz')\n",
    "arr_img = data_img.get_fdata()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(arr_img[:, :, 4], cmap='gray')\n",
    "ax[1].imshow(arr_gt[:, :, 4], cmap='gray')\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation.metrics import dice, hausdorff_distance, avg_surface_distance_symmetric\n",
    "import cv2 as cv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.ticker as mtick\n",
    "import math\n",
    "from skimage.measure import regionprops\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def update_dict(d, key, value):\n",
    "    if key not in d:\n",
    "        d[key] = [value]\n",
    "    else:\n",
    "        d[key].append(value)\n",
    "    return d\n",
    "\n",
    "def get_metric(path_list, path_list_gt, metric=None):\n",
    "    path_list = sorted(path_list, key=lambda x:x.split('\\\\')[-1])\n",
    "    path_list_gt = sorted(path_list_gt, key=lambda x:x.split('\\\\')[-1])\n",
    "\n",
    "    out_dict = {}\n",
    "    scores = []\n",
    "    for path, path_gt in tqdm(zip(path_list, path_list_gt), total=len(path_list)):\n",
    "        data = nib.load(path)\n",
    "        arr = data.get_fdata()\n",
    "\n",
    "        zoom = list(data.header.get_zooms())\n",
    "        zoom = zoom[:-1]\n",
    "\n",
    "        data_gt = nib.load(path_gt)\n",
    "        arr_gt = data_gt.get_fdata()\n",
    "\n",
    "        assert arr.shape == arr_gt.shape\n",
    "        patient_scores = []\n",
    "        for i in range(arr.shape[-1]):\n",
    "            #fig, ax = plt.subplots(1, 2)\n",
    "            #ax[0].imshow(arr[:, :, i], cmap='gray')\n",
    "            #ax[1].imshow(arr_gt[:, :, i], cmap='gray')\n",
    "            #plt.show()\n",
    "            #plt.waitforbuttonpress()\n",
    "            #plt.close(fig)\n",
    "\n",
    "            current_pred = arr[:, :, i]\n",
    "            current_gt = arr_gt[:, :, i]\n",
    "\n",
    "            #current_pred = arr\n",
    "            #current_gt = arr_gt\n",
    "\n",
    "            class_score = []\n",
    "            for j in range(1, 4):\n",
    "                current_class_pred = current_pred == j\n",
    "                current_gt_pred = current_gt == j\n",
    "                if metric == 'dice':\n",
    "                    score = dice(current_class_pred, current_gt_pred)\n",
    "                elif metric == 'hd':\n",
    "                    score = hausdorff_distance(current_class_pred, current_gt_pred, voxel_spacing=zoom)\n",
    "                elif metric == 'assd':\n",
    "                    score = avg_surface_distance_symmetric(current_class_pred, current_gt_pred, voxel_spacing=zoom)\n",
    "                class_score.append(score)\n",
    "            out_dict = update_dict(out_dict, key=i/arr.shape[-1], value=np.array(class_score))\n",
    "            patient_scores.append(np.array(class_score))\n",
    "        patient_class_score = np.stack(patient_scores, axis=0)\n",
    "        patient_class_score = np.nanmean(patient_class_score, axis=0)\n",
    "        scores.append(patient_class_score)\n",
    "    scores = np.stack(scores, 0)\n",
    "    scores = np.nanmean(scores, axis=0)\n",
    "\n",
    "    x = [[], [], []]\n",
    "    y = [[], [], []]\n",
    "    for key in out_dict.keys():\n",
    "        class_dice = np.stack(out_dict[key], axis=0)\n",
    "        class_dice = np.nanmean(class_dice, axis=0)\n",
    "        for i in range(3):\n",
    "            if not math.isnan(class_dice[i]):\n",
    "                x[i].append(key)\n",
    "                y[i].append(class_dice[i])\n",
    "\n",
    "    print(np.array(scores))\n",
    "    return x, y\n",
    "\n",
    "def arrowed_spines(ax, metric=None):\n",
    "\n",
    "    xmin, xmax = ax.get_xlim() \n",
    "    ymin, ymax = ax.get_ylim()\n",
    "\n",
    "    #print(xmin)\n",
    "    #print(xmax)\n",
    "    #print(ymin)\n",
    "    #print(ymax)\n",
    "\n",
    "    # removing the default axis on all sides:\n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax.spines[side].set_visible(False)\n",
    "\n",
    "    # removing the axis ticks\n",
    "    #plt.xticks([]) # labels \n",
    "    #plt.yticks([])\n",
    "    ax.xaxis.set_ticks_position('none') # tick markers\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "\n",
    "    ax.spines[\"left\"].set_position((\"data\", -0.05))\n",
    "    ax.spines[\"bottom\"].set_position((\"data\", 0.0))\n",
    "    ax.set_xticks([0.0, 1.0])\n",
    "\n",
    "    # get width and height of axes object to compute \n",
    "    # matching arrowhead length and width\n",
    "    #dps = fig.dpi_scale_trans.inverted()\n",
    "    #bbox = ax.get_window_extent().transformed(dps)\n",
    "    #width, height = bbox.width, bbox.height\n",
    "\n",
    "    # manual arrowhead width and length\n",
    "    hw = 1./20.*(ymax-ymin) \n",
    "    hl = 1./20.*(xmax-xmin)\n",
    "    lw = 1. # axis line width\n",
    "    ohg = 0.0 # arrow overhang\n",
    "\n",
    "    # compute matching arrowhead length and width\n",
    "    #yhw = hw/(ymax-ymin)*(xmax-xmin)* height/width \n",
    "    #yhl = hl/(xmax-xmin)*(ymax-ymin)* width/height\n",
    "\n",
    "    if metric == 'assd':\n",
    "        head_width = 0.2\n",
    "        head_length = 0.02\n",
    "    elif metric == 'hd':\n",
    "        head_width = 1\n",
    "        head_length = 0.02\n",
    "\n",
    "    # draw x and y axis\n",
    "    ax.arrow(-0.05, 0, xmax-xmin, 0., fc='k', ec='k', lw = lw, \n",
    "             head_width=head_width, head_length=head_length, overhang = ohg, \n",
    "             length_includes_head= True, clip_on = False) \n",
    "    \n",
    "    if metric == 'assd':\n",
    "        head_width = 0.02\n",
    "        head_length = 0.2\n",
    "    elif metric == 'hd':\n",
    "        head_width = 0.02\n",
    "        head_length = 1\n",
    "\n",
    "    ax.arrow(-0.05, 0, 0., ymax-ymin, fc='k', ec='k', lw = lw, \n",
    "             head_width=head_width, head_length=head_length, overhang = ohg, \n",
    "             length_includes_head= True, clip_on = False)\n",
    "    \n",
    "    ax.set_xlabel('Depth')\n",
    "    if metric == 'assd':\n",
    "        ax.set_ylabel('ASSD (mm)')\n",
    "    elif metric == 'hd':\n",
    "        ax.set_ylabel('Hausdorff distance (mm)')\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "def get_image_ready(path, index):\n",
    "    filename = path.split('\\\\')[-1].split('.')[0] \n",
    "    patient_name = filename.split('_')[0]\n",
    "    path_array_gt = os.path.join(os.path.join('ACDC_training', patient_name, filename + '_gt.nii.gz'))\n",
    "    path_array_img = os.path.join(os.path.join('ACDC_training', patient_name, filename + '.nii.gz'))\n",
    "    path_array_pred = os.path.join(os.path.join(r'ACDC_output\\Baseline\\temp_allClasses', filename + '.nii.gz'))\n",
    "\n",
    "    gt_data = nib.load(path_array_gt)\n",
    "    gt_arr = gt_data.get_fdata()[:, :, index]\n",
    "\n",
    "    img_data = nib.load(path_array_img)\n",
    "    img_arr = img_data.get_fdata()[:, :, index]\n",
    "\n",
    "    pred_data = nib.load(path_array_pred)\n",
    "    pred_arr = pred_data.get_fdata()[:, :, index]\n",
    "\n",
    "    img_arr = cv.normalize(img_arr, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX).astype(np.uint8)\n",
    "    img_arr = cv.cvtColor(img_arr, cv.COLOR_GRAY2RGB)\n",
    "\n",
    "    label = np.logical_or(gt_arr > 0, pred_arr > 0).astype(np.uint8)\n",
    "    regions = regionprops(label)\n",
    "    assert len(regions) == 1\n",
    "    min_row, min_col, max_row, max_col = regions[0].bbox\n",
    "\n",
    "    for j in range(1, 4):\n",
    "        pred = (pred_arr == j).astype(np.uint8)\n",
    "        gt = (gt_arr == j).astype(np.uint8)\n",
    "        pred_contours, hierarchy = cv.findContours(pred, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        gt_contours, hierarchy = cv.findContours(gt, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        pred_color = [255, 0, 0]\n",
    "        gt_color = [0, 255, 0]\n",
    "        img_arr = cv.drawContours(img_arr, pred_contours, -1, pred_color, 1)\n",
    "        img_arr = cv.drawContours(img_arr, gt_contours, -1, gt_color, 1)\n",
    "    \n",
    "    #y1 = max(0, int(round(y - 64)))\n",
    "    #y2 = min(gt_arr.shape[0], int(round(y + 64)))\n",
    "    #x1 = max(0, int(round(x - 64)))\n",
    "    #x2 = min(gt_arr.shape[1], int(round(x + 64)))\n",
    "    img_arr = img_arr[min_row - 10:max_row + 10, min_col - 10:max_col + 10]\n",
    "    \n",
    "    return img_arr\n",
    "\n",
    "h = 2\n",
    "#w = int(round(h * (16/9)))\n",
    "w = 4\n",
    "\n",
    "nb = h * w\n",
    "\n",
    "path_memory_assd = np.zeros(shape=(nb,), dtype=object)\n",
    "score_memory_assd = np.full_like(path_memory_assd, fill_value=0)\n",
    "arr_memory_assd = np.zeros(shape=(nb,), dtype=object)\n",
    "idx_memory_assd = np.zeros(shape=(nb,), dtype=np.uint8)\n",
    "\n",
    "path_memory_hd = np.zeros(shape=(nb,), dtype=object)\n",
    "score_memory_hd = np.full_like(path_memory_hd, fill_value=0)\n",
    "arr_memory_hd = np.zeros(shape=(nb,), dtype=object)\n",
    "idx_memory_hd = np.zeros(shape=(nb,), dtype=np.uint8)\n",
    "\n",
    "path_list = glob(r'ACDC_output\\Baseline\\temp_allClasses\\*.gz')\n",
    "score_list = []\n",
    "for path in path_list:\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "\n",
    "    zoom = list(data.header.get_zooms())\n",
    "    zoom = zoom[:-1]\n",
    "\n",
    "    filename = path.split('\\\\')[-1].split('.')[0] \n",
    "    patient_name = filename.split('_')[0]\n",
    "    path_array_gt = os.path.join(os.path.join('ACDC_training', patient_name, filename + '_gt.nii.gz'))\n",
    "    #path_array_gt = os.path.join(r'ACDC_training\\patient_name\\*_gt.nii.gz')\n",
    "    #path_array_gt = os.path.join('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\labelsTr\\\\', filename + '.nii.gz')\n",
    "\n",
    "    gt_data = nib.load(path_array_gt)\n",
    "    gt_arr = gt_data.get_fdata()\n",
    "\n",
    "    assert gt_arr.shape == arr.shape\n",
    "\n",
    "    patient_score_assd = []\n",
    "    patient_score_hd = []\n",
    "    assd_merged_list = []\n",
    "    hd_merged_list = []\n",
    "    for j in range(arr.shape[-1]):\n",
    "        class_assd_list = []\n",
    "        class_hd_list = []\n",
    "        assd_merged = avg_surface_distance_symmetric(arr[:, :, j], gt_arr[:, :, j], voxel_spacing=zoom)\n",
    "        hd_merged = hausdorff_distance(arr[:, :, j], gt_arr[:, :, j], voxel_spacing=zoom)\n",
    "        assd_merged_list.append(assd_merged)\n",
    "        hd_merged_list.append(hd_merged)\n",
    "        for c in range(1, 4):\n",
    "            current_arr = arr[:, :, j] == c\n",
    "            current_gt = gt_arr[:, :, j] == c\n",
    "            class_assd = avg_surface_distance_symmetric(current_arr, current_gt, voxel_spacing=zoom)\n",
    "            class_hd = hausdorff_distance(current_arr, current_gt, voxel_spacing=zoom)\n",
    "            class_assd_list.append(class_assd)\n",
    "            class_hd_list.append(class_hd)\n",
    "        class_assd = np.array(class_assd_list)\n",
    "        class_hd = np.array(class_hd_list)\n",
    "        patient_score_assd.append(class_assd)\n",
    "        patient_score_hd.append(class_hd)\n",
    "    assd_merged_list = np.stack(assd_merged_list, axis=0)\n",
    "    hd_merged_list = np.stack(hd_merged_list, axis=0)\n",
    "    patient_score_assd = np.stack(patient_score_assd, axis=0)\n",
    "    patient_score_hd = np.stack(patient_score_hd, axis=0)\n",
    "    data_dict = {'scores_assd': patient_score_assd, 'scores_hd': patient_score_hd, 'assd_merged': assd_merged_list, 'hd_merged': hd_merged_list, 'depth': np.arange(arr.shape[-1]), 'path': np.full(shape=(arr.shape[-1],), fill_value=path)}\n",
    "    score_list.append(data_dict)\n",
    "        #assd = avg_surface_distance_symmetric(arr[:, :, j], gt_arr[:, :, j], voxel_spacing=zoom)\n",
    "        #hd = hausdorff_distance(arr[:, :, j], gt_arr[:, :, j], voxel_spacing=zoom)\n",
    "        #if assd > score_memory_assd[0]:\n",
    "        #    arr_memory_assd[0] = arr[:, :, j]\n",
    "        #    score_memory_assd[0] = assd\n",
    "        #    path_memory_assd[0] = path\n",
    "        #    idx_memory_assd[0] = j\n",
    "        #    sorted_indices = score_memory_assd.argsort()\n",
    "        #    arr_memory_assd = arr_memory_assd[sorted_indices]\n",
    "        #    score_memory_assd = score_memory_assd[sorted_indices]\n",
    "        #    path_memory_assd = path_memory_assd[sorted_indices]\n",
    "        #    idx_memory_assd = idx_memory_assd[sorted_indices]\n",
    "        #if hd > score_memory_hd[0]:\n",
    "        #    arr_memory_hd[0] = arr[:, :, j]\n",
    "        #    score_memory_hd[0] = hd\n",
    "        #    path_memory_hd[0] = path\n",
    "        #    idx_memory_hd[0] = j\n",
    "        #    sorted_indices = score_memory_hd.argsort()\n",
    "        #    arr_memory_hd = arr_memory_hd[sorted_indices]\n",
    "        #    score_memory_hd = score_memory_hd[sorted_indices]\n",
    "        #    path_memory_hd = path_memory_hd[sorted_indices]\n",
    "        #    idx_memory_hd = idx_memory_hd[sorted_indices]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 11))\n",
    "#fig = plt.figure(figsize=(11, 6))\n",
    "#gs = GridSpec(4, 6, figure=fig)\n",
    "gs = GridSpec(6, 4, figure=fig)\n",
    "\n",
    "scores = np.concatenate([x['scores_assd'] for x in score_list], axis=0)\n",
    "#scores = np.concatenate([x['scores_hd'] for x in score_list], axis=0)\n",
    "depths = np.concatenate([x['depth'] for x in score_list], axis=0)\n",
    "paths = np.concatenate([x['path'] for x in score_list], axis=0)\n",
    "depth_percent = np.concatenate([x['depth'] / (len(x['scores_assd']) - 1) for x in score_list], axis=0)\n",
    "#depth_percent = np.concatenate([x['depth'] / (len(x['scores_hd']) - 1) for x in score_list], axis=0)\n",
    "\n",
    "sort_indices_depth = np.argsort(depth_percent)\n",
    "sorted_scores_depth = scores[sort_indices_depth]\n",
    "sorted_depth_depth = depth_percent[sort_indices_depth]\n",
    "\n",
    "ax = fig.add_subplot(gs[2:, :])\n",
    "\n",
    "for i, (label, color) in enumerate(zip(['RV', 'MYO', 'LV'], ['r', 'g', 'b'])):\n",
    "    mask = ~np.isnan(sorted_scores_depth[:, i])\n",
    "    assert len(sorted_depth_depth[mask]) == len(sorted_scores_depth[mask, i])\n",
    "    ax.scatter(sorted_depth_depth[mask], sorted_scores_depth[mask, i], c=color, label=label)\n",
    "#ax.scatter(np.array(x_assd[0]), np.array(y_assd[0]), c='r', label='RV')\n",
    "#ax.scatter(np.array(x_assd[1]), np.array(y_assd[1]), c='g', label='MYO')\n",
    "#ax.scatter(np.array(x_assd[2]), np.array(y_assd[2]), c='b', label='LV')\n",
    "ax.legend(loc=(0.44, 0.7))\n",
    "arrowed_spines(ax, metric='assd')\n",
    "#arrowed_spines(ax, metric='hd')\n",
    "ax.axvline(x=0.30, c='black', linestyle='dashed', alpha=0.5)\n",
    "ax.axvline(x=0.68, c='black', linestyle='dashed', alpha=0.5)\n",
    "ax.text(x=0.10, y=28.00, s='Base', alpha=1.0)\n",
    "ax.text(x=0.45, y=28.00, s='Middle', alpha=1.0)\n",
    "ax.text(x=0.85, y=28.00, s='Apex', alpha=1.0)\n",
    "\n",
    "#ax.set_xlim(left=-50)\n",
    "\n",
    "#mask = ~np.isnan(np.nanmean(scores, axis=1))\n",
    "#scores = scores[mask, :]\n",
    "#depths = depths[mask]\n",
    "#paths = paths[mask]\n",
    "#sort_indices_score = np.argsort(np.nanmean(scores, axis=1))\n",
    "#sorted_scores_score = scores[sort_indices_score]\n",
    "#sorted_depths_score = depths[sort_indices_score]\n",
    "#sorted_paths_score = paths[sort_indices_score]\n",
    "#display_paths = sorted_paths_score[-nb:]\n",
    "#display_depths = sorted_depths_score[-nb:]\n",
    "\n",
    "merged_scores = np.concatenate([x['assd_merged'] for x in score_list], axis=0)\n",
    "#merged_scores = np.concatenate([x['hd_merged'] for x in score_list], axis=0)\n",
    "mask = ~np.isnan(merged_scores)\n",
    "merged_scores = merged_scores[mask]\n",
    "depths = depths[mask]\n",
    "paths = paths[mask]\n",
    "sort_indices_score = np.argsort(merged_scores)\n",
    "sorted_scores_score = merged_scores[sort_indices_score]\n",
    "sorted_depths_score = depths[sort_indices_score]\n",
    "sorted_paths_score = paths[sort_indices_score]\n",
    "display_paths = sorted_paths_score[-nb:]\n",
    "display_depths = sorted_depths_score[-nb:]\n",
    "\n",
    "for i in range(nb):\n",
    "    ax = fig.add_subplot(gs[int(i%h), int(i//h)])\n",
    "    assd_img = get_image_ready(display_paths[i], display_depths[i])\n",
    "\n",
    "    ax.imshow(assd_img)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "#path_list = glob(r'ACDC_output\\Baseline\\temp_allClasses\\*.gz')\n",
    "#path_list_names = [x.split('\\\\')[-1][:13] for x in path_list]\n",
    "#path_list_gt = glob(r'ACDC_training\\**\\*_gt.nii.gz', recursive=True)\n",
    "#path_list_gt = [x for x in path_list_gt if x.split('\\\\')[-1][:13] in path_list_names]\n",
    "#assert len(path_list_gt) == len(path_list)\n",
    "#\n",
    "##x_dice, y_dice = get_metric(path_list, path_list_gt, metric='dice')\n",
    "##x_hd, y_hd = get_metric(path_list, path_list_gt, metric='hd')\n",
    "#x_assd, y_assd = get_metric(path_list, path_list_gt, metric='assd')\n",
    "\n",
    "\n",
    "\n",
    "#fig.add_artist(lines.Line2D([0.5, 0.5], [0, 1], linewidth=4, color='black'))\n",
    "fig.tight_layout()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"squares_assd_2.png\", dpi=1200)\n",
    "#plt.show()\n",
    "\n",
    "png1 = io.BytesIO()\n",
    "plt.savefig(png1, format=\"png\", dpi=600)\n",
    "\n",
    "png2 = Image.open(png1)\n",
    "\n",
    "png2.save(\"Image_4.tiff\", dpi=(600, 600))\n",
    "png1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portal\\AppData\\Local\\Temp/ipykernel_19664/2630023403.py:1: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean([np.nan, np.nan, np.nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean([np.nan, np.nan, np.nan])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get images for worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23812/590206307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_gt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mwaitforbuttonpress\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m   2306\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2308\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mwaitforbuttonpress\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   3123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3125\u001b[1;33m         _blocking_input.blocking_input_loop(\n\u001b[0m\u001b[0;32m   3126\u001b[0m             self, [\"button_press_event\", \"key_press_event\"], timeout, handler)\n\u001b[0;32m   3127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\_blocking_input.py\u001b[0m in \u001b[0;36mblocking_input_loop\u001b[1;34m(figure, event_names, timeout, handler)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmpl_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevent_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Start event loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Run even on exception like ctrl-c.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Disconnect the callbacks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backends\\backend_qt.py\u001b[0m in \u001b[0;36mstart_event_loop\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_maybe_allow_interrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mqt_compat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backends\\qt_compat.py\u001b[0m in \u001b[0;36m_maybe_allow_interrupt\u001b[1;34m(qapp)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_sigint_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhandler_args\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[0mold_sigint_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mhandler_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task029_Quorum\\imagesTr\\patient082_ed_0000.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "\n",
    "data_gt = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task029_Quorum\\labelsTr\\patient082_ed.nii.gz')\n",
    "arr_gt = data_gt.get_fdata()\n",
    "\n",
    "for i in range(arr.shape[-1]):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(arr[:, :, i], cmap='gray')\n",
    "    ax[1].imshow(arr_gt[:, :, i], cmap='gray')\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['250001' '250005' '276004' '276005' '276006' '276007' '276008' '348001'\n",
      " '348002' '348003' '348004' '348007' '616003' '616005' '616006' '616009'\n",
      " '616010' '616012' '703001' '703003' '703004' '724002' '724005' '724006']\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "center_list = []\n",
    "path_list = glob(r'data_saud_2\\3D\\**\\*.csv', recursive=True)\n",
    "for path in path_list:\n",
    "    df = pd.read_csv(path)\n",
    "    center = df['Name'][0].split('-')[0]\n",
    "    center_list.append(center)\n",
    "center_list = np.array(center_list)\n",
    "out = np.unique(center_list)\n",
    "print(out)\n",
    "print(out.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dice per depth level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.04it/s]\n",
      "C:\\Users\\Portal\\AppData\\Local\\Temp/ipykernel_18508/2623967950.py:74: RuntimeWarning: Mean of empty slice\n",
      "  class_dice = np.nanmean(class_dice, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32893187 0.91881481 0.90285253]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from evaluation.metrics import dice, hausdorff_distance, avg_surface_distance_symmetric\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from glob import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def update_dict(d, key, value):\n",
    "    if key not in d:\n",
    "        d[key] = [value]\n",
    "    else:\n",
    "        d[key].append(value)\n",
    "    return d\n",
    "\n",
    "def get_metric(path_list, path_list_gt, metric=None):\n",
    "    path_list = sorted(path_list, key=lambda x:x.split('\\\\')[-1])\n",
    "    path_list_gt = sorted(path_list_gt, key=lambda x:x.split('\\\\')[-1])\n",
    "\n",
    "    out_dict = {}\n",
    "    scores = []\n",
    "    for path, path_gt in tqdm(zip(path_list, path_list_gt), total=len(path_list)):\n",
    "        data = nib.load(path)\n",
    "        arr = data.get_fdata()\n",
    "\n",
    "        zoom = list(data.header.get_zooms())\n",
    "        zoom = zoom[:-1]\n",
    "\n",
    "        data_gt = nib.load(path_gt)\n",
    "        arr_gt = data_gt.get_fdata()\n",
    "\n",
    "        assert arr.shape == arr_gt.shape\n",
    "        patient_scores = []\n",
    "        for i in range(arr.shape[-1]):\n",
    "            #fig, ax = plt.subplots(1, 2)\n",
    "            #ax[0].imshow(arr[:, :, i], cmap='gray')\n",
    "            #ax[1].imshow(arr_gt[:, :, i], cmap='gray')\n",
    "            #plt.show()\n",
    "            #plt.waitforbuttonpress()\n",
    "            #plt.close(fig)\n",
    "\n",
    "            current_pred = arr[:, :, i]\n",
    "            current_gt = arr_gt[:, :, i]\n",
    "\n",
    "            #current_pred = arr\n",
    "            #current_gt = arr_gt\n",
    "\n",
    "            class_score = []\n",
    "            for j in range(1, 4):\n",
    "                current_class_pred = current_pred == j\n",
    "                current_gt_pred = current_gt == j\n",
    "                if metric == 'dice':\n",
    "                    score = dice(current_class_pred, current_gt_pred)\n",
    "                elif metric == 'hd':\n",
    "                    score = hausdorff_distance(current_class_pred, current_gt_pred, voxel_spacing=zoom)\n",
    "                elif metric == 'assd':\n",
    "                    score = avg_surface_distance_symmetric(current_class_pred, current_gt_pred, voxel_spacing=zoom)\n",
    "                class_score.append(score)\n",
    "            out_dict = update_dict(out_dict, key=i/arr.shape[-1], value=np.array(class_score))\n",
    "            patient_scores.append(np.array(class_score))\n",
    "        patient_class_score = np.stack(patient_scores, axis=0)\n",
    "        patient_class_score = np.nanmean(patient_class_score, axis=0)\n",
    "        scores.append(patient_class_score)\n",
    "    scores = np.stack(scores, 0)\n",
    "    scores = np.nanmean(scores, axis=0)\n",
    "\n",
    "    x = [[], [], []]\n",
    "    y = [[], [], []]\n",
    "    for key in out_dict.keys():\n",
    "        class_dice = np.stack(out_dict[key], axis=0)\n",
    "        class_dice = np.nanmean(class_dice, axis=0)\n",
    "        for i in range(3):\n",
    "            if not math.isnan(class_dice[i]):\n",
    "                x[i].append(key)\n",
    "                y[i].append(class_dice[i])\n",
    "\n",
    "    print(np.array(scores))\n",
    "    return x, y\n",
    "\n",
    "def arrowed_spines(fig, ax, metric=None):\n",
    "\n",
    "    xmin, xmax = ax.get_xlim() \n",
    "    ymin, ymax = ax.get_ylim()\n",
    "\n",
    "    #print(xmin)\n",
    "    #print(xmax)\n",
    "    #print(ymin)\n",
    "    #print(ymax)\n",
    "\n",
    "    # removing the default axis on all sides:\n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax.spines[side].set_visible(False)\n",
    "\n",
    "    # removing the axis ticks\n",
    "    #plt.xticks([]) # labels \n",
    "    #plt.yticks([])\n",
    "    ax.xaxis.set_ticks_position('none') # tick markers\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "\n",
    "    ax.spines[\"left\"].set_position((\"data\", 0.0))\n",
    "    ax.spines[\"bottom\"].set_position((\"data\", 0.0))\n",
    "    ax.set_xticks([0.0, 1.0])\n",
    "\n",
    "    # get width and height of axes object to compute \n",
    "    # matching arrowhead length and width\n",
    "    dps = fig.dpi_scale_trans.inverted()\n",
    "    bbox = ax.get_window_extent().transformed(dps)\n",
    "    width, height = bbox.width, bbox.height\n",
    "\n",
    "    # manual arrowhead width and length\n",
    "    hw = 1./20.*(ymax-ymin) \n",
    "    hl = 1./20.*(xmax-xmin)\n",
    "    lw = 1. # axis line width\n",
    "    ohg = 0.0 # arrow overhang\n",
    "\n",
    "    # compute matching arrowhead length and width\n",
    "    yhw = hw/(ymax-ymin)*(xmax-xmin)* height/width \n",
    "    yhl = hl/(xmax-xmin)*(ymax-ymin)* width/height\n",
    "\n",
    "    if metric == 'assd':\n",
    "        head_width = 0.2\n",
    "        head_length = 0.02\n",
    "    elif metric == 'hd':\n",
    "        head_width = 1\n",
    "        head_length = 0.02\n",
    "\n",
    "    # draw x and y axis\n",
    "    ax.arrow(0, 0, xmax-xmin, 0., fc='k', ec='k', lw = lw, \n",
    "             head_width=head_width, head_length=head_length, overhang = ohg, \n",
    "             length_includes_head= True, clip_on = False) \n",
    "    \n",
    "    if metric == 'assd':\n",
    "        head_width = 0.02\n",
    "        head_length = 0.2\n",
    "    elif metric == 'hd':\n",
    "        head_width = 0.02\n",
    "        head_length = 1\n",
    "\n",
    "    ax.arrow(0, 0, 0., ymax-ymin, fc='k', ec='k', lw = lw, \n",
    "             head_width=head_width, head_length=head_length, overhang = ohg, \n",
    "             length_includes_head= True, clip_on = False)\n",
    "    \n",
    "    ax.set_xlabel('Depth')\n",
    "    if metric == 'assd':\n",
    "        ax.set_ylabel('ASSD')\n",
    "    elif metric == 'hd':\n",
    "        ax.set_ylabel('Hausdorff distance')\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "path_list = glob(r'ACDC_output\\Baseline\\temp_allClasses\\*.gz')\n",
    "path_list_names = [x.split('\\\\')[-1][:13] for x in path_list]\n",
    "path_list_gt = glob(r'ACDC_training\\**\\*_gt.nii.gz', recursive=True)\n",
    "path_list_gt = [x for x in path_list_gt if x.split('\\\\')[-1][:13] in path_list_names]\n",
    "assert len(path_list_gt) == len(path_list)\n",
    "\n",
    "#x_dice, y_dice = get_metric(path_list, path_list_gt, metric='dice')\n",
    "#x_hd, y_hd = get_metric(path_list, path_list_gt, metric='hd')\n",
    "x_assd, y_assd = get_metric(path_list, path_list_gt, metric='assd')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "#fig.subplots_adjust(left=0.05, right=0.99, bottom=0.05, top=0.99)\n",
    "\n",
    "ax.scatter(np.array(x_assd[0]), np.array(y_assd[0]), c='r', label='RV')\n",
    "ax.scatter(np.array(x_assd[1]), np.array(y_assd[1]), c='g', label='MYO')\n",
    "ax.scatter(np.array(x_assd[2]), np.array(y_assd[2]), c='b', label='LV')\n",
    "ax.legend(loc=(0.44, 0.7))\n",
    "arrowed_spines(fig, ax, metric='assd')\n",
    "ax.axvline(x=0.333333, c='black', linestyle='dashed', alpha=0.5)\n",
    "ax.axvline(x=0.666666, c='black', linestyle='dashed', alpha=0.5)\n",
    "ax.text(x=0.13, y=5.04, s='Base', alpha=0.5)\n",
    "ax.text(x=0.46, y=5.04, s='Middle', alpha=0.5)\n",
    "ax.text(x=0.80, y=5.04, s='Apex', alpha=0.5)\n",
    "\n",
    "#ax[1].scatter(np.array(x_hd[0]), np.array(y_hd[0]), c='r', label='RV')\n",
    "#ax[1].scatter(np.array(x_hd[1]), np.array(y_hd[1]), c='g', label='MYO')\n",
    "#ax[1].scatter(np.array(x_hd[2]), np.array(y_hd[2]), c='b', label='LV')\n",
    "#ax[1].legend(loc=(0.44, 0.7))\n",
    "#arrowed_spines(fig, ax[1], metric='hd')\n",
    "#ax[1].axvline(x=0.333333, c='black', linestyle='dashed', alpha=0.5)\n",
    "#ax[1].axvline(x=0.666666, c='black', linestyle='dashed', alpha=0.5)\n",
    "#ax[1].text(x=0.13, y=30.04, s='Base', alpha=0.5)\n",
    "#ax[1].text(x=0.46, y=30.04, s='Middle', alpha=0.5)\n",
    "#ax[1].text(x=0.80, y=30.04, s='Apex', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"depth_assd.png\", dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de6e7f1d27ee98e0dd03d720850162ba8f013030d5557c31bd8d79f8fd588abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
