{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "for path in glob('2022-10-18_14H59\\gt_niftis\\*.gz'):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    slice_nb = arr.shape[2] // 2\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(arr[:, :, slice_nb], cmap='gray')\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.random.rand(2, 8, 4)\n",
    "arr2 = np.random.rand(2, 8, 100, 2)\n",
    "out = arr2[:, :, :, 0] + arr[:, :, None, 0]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "count = []\n",
    "for path in glob(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task026_MMs\\labelsTr\\*.gz'):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    #print(arr.shape)\n",
    "    count.append(arr.shape[2])\n",
    "out = np.array(count)\n",
    "print(out.min())\n",
    "print(out.max())\n",
    "print(out.sum())\n",
    "\n",
    "    #fig, ax = plt.subplots(1, 1)\n",
    "    #ax.imshow(arr[0, 5, :, :], cmap='gray')\n",
    "    #plt.waitforbuttonpress()\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(r'data_saud\\2D\\cineED\\250001-001\\250001-001_slice_00.npz')\n",
    "print(data.files)\n",
    "print(data['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from skimage.registration import optical_flow_tvl1\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(r'out\\nnUNet_raw_data_base\\nnUNet_cropped_data\\Task027_ACDC\\patient001_frame01.npz')\n",
    "#arr = data.get_fdata()\n",
    "arr = data['data']\n",
    "print(arr.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(arr[0, 2, :, :], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.waitforbuttonpress()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mean_list = []\n",
    "with open(r'no_sfb\\validation_raw\\summary.json') as f:\n",
    "    data = json.load(f)\n",
    "    for d in data['results']['all']:\n",
    "        m = (d['1']['Dice'] + d['2']['Dice'] + d['3']['Dice']) / 3\n",
    "        mean_list.append(m)\n",
    "mean_list = np.array(mean_list)\n",
    "print(mean_list.mean())\n",
    "print(mean_list.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.training_utils import read_config\n",
    "from run.default_configuration import get_default_configuration\n",
    "\n",
    "config = read_config('adversarial_acdc_middle.yaml', True)\n",
    "\n",
    "plans_file, output_folder_name, dataset_directory, batch_dice, stage, \\\n",
    "        trainer_class = get_default_configuration('2d', 'Task027_ACDC', 'nnMTLTrainerV2', config, 'custom_experiment_planner')\n",
    "\n",
    "output_folder_name = '2022-10-07_16H20'\n",
    "trainer = trainer_class(plans_file, 0, output_folder=output_folder_name, dataset_directory=dataset_directory,\n",
    "                            batch_dice=batch_dice, stage=stage, unpack_data=True, middle=True,\n",
    "                            deterministic=True,\n",
    "                            fp16=True)\n",
    "\n",
    "trainer.load_final_checkpoint(train=False)\n",
    "\n",
    "trainer.network.eval()\n",
    "percents = [0.08333, 0.056666, 0.1176, 0.1667, 0.1333, 0.1765, 0.1875, 0.2, 0.1538, 0.1429, 0.125, 0.06667, 0.07143]\n",
    "\n",
    "for idx, percent in enumerate(percents):\n",
    "    trainer.network.percent = percent\n",
    "\n",
    "    folder = \"inference_testings/validation_raw_\" + str(percent)\n",
    "\n",
    "    trainer.validate(save_softmax=False, validation_folder_name=folder,\n",
    "                            run_postprocessing_on_folds=True,\n",
    "                            overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.training_utils import read_config\n",
    "from run.default_configuration import get_default_configuration\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "folder_name = 'no_sfb'\n",
    "weight_folder = os.path.join(r'ACDC_output', folder_name)\n",
    "\n",
    "config = read_config('adversarial_acdc.yaml', False, False)\n",
    "\n",
    "plans_file, output_folder_name, dataset_directory, batch_dice, stage, \\\n",
    "        trainer_class = get_default_configuration('2d', 'Task027_ACDC', 'nnMTLTrainerV2', config, 'custom_experiment_planner')\n",
    "\n",
    "trainer = trainer_class(plans_file, 0, output_folder=weight_folder, dataset_directory=dataset_directory,\n",
    "                            batch_dice=batch_dice, stage=stage, unpack_data=True, middle=False, video=False,\n",
    "                            deterministic=True,\n",
    "                            fp16=True)\n",
    "\n",
    "trainer.load_final_checkpoint(train=False)\n",
    "\n",
    "trainer.network.eval()\n",
    "\n",
    "trainer.validate(save_softmax=False,\n",
    "                run_postprocessing_on_folds=True,\n",
    "                overwrite=False, \n",
    "                output_folder=os.path.join(r'ACDC_output\\all_data', folder_name),\n",
    "                debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "I am running the following nnUNet: 2d\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnMTLTrainerV2.nnMTLTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  3\n",
      "modalities:  {0: 'MRI'}\n",
      "use_mask_for_norm OrderedDict([(0, False)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [3, 3], 'patch_size': array([224, 224]), 'median_patient_size_in_voxels': array([  9, 249, 219]), 'current_spacing': array([10.        ,  1.48438001,  1.48438001]), 'original_spacing': array([10.        ,  1.48438001,  1.48438001]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using batch dice + CE loss\n",
      "\n",
      "I am using data from this folder:  C:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner\n",
      "###############################################\n",
      "2023-01-23 15:40:18.389885: loading checkpoint only_sfb\\model_final_checkpoint.model train= False\n",
      "2023-01-23 15:40:18.587116: model has 21,556,620 parameters\n",
      "2023-01-23 15:40:22.611918: The Whole model has 21,556,620 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 368.00 MiB (GPU 0; 8.00 GiB total capacity; 6.21 GiB already allocated; 0 bytes free; 7.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26252/2150568188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_throughput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\training\\network_training\\nnMTLTrainerV2.py\u001b[0m in \u001b[0;36mget_throughput\u001b[1;34m(self, optimal_batch_size)\u001b[0m\n\u001b[0;32m    990\u001b[0m                 \u001b[0mender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menable_timing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m                 \u001b[0mstarter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m                 \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m                 \u001b[0mender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\network_architecture\\MTL_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_bottleneck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[0mseg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_skip_connections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffinity\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\lib\\decoder_alt.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, encoder_skip_connections)\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mencoder_skip_connection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_skip_co_segmentation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m                     \u001b[0mencoder_skip_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_skip_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_skip_connection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_skip_connection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresizer_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\lib\\swin_cross_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, skip_co)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[0mg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mpsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\lib\\swin_cross_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, rescaled, rescaler)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# W-MSA/SW-MSA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mattn_windows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_windows_rescaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_windows_rescaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# nW*B, window_size*window_size, C\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# merge windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\portal\\documents\\isensee\\nnunet\\nnunet\\lib\\swin_cross_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, rescaled, rescaler, mask)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpe_table\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mrpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpe_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpe_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrpe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_rpe_table\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_rpe_table\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 368.00 MiB (GPU 0; 8.00 GiB total capacity; 6.21 GiB already allocated; 0 bytes free; 7.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from lib.training_utils import read_config\n",
    "from run.default_configuration import get_default_configuration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = read_config('adversarial_acdc.yaml', False, False)\n",
    "\n",
    "plans_file, output_folder_name, dataset_directory, batch_dice, stage, \\\n",
    "        trainer_class = get_default_configuration('2d', 'Task027_ACDC', 'nnMTLTrainerV2', config, 'custom_experiment_planner')\n",
    "\n",
    "trainer = trainer_class(plans_file, 0, output_folder='only_sfb', dataset_directory=dataset_directory,\n",
    "                            batch_dice=batch_dice, stage=stage, unpack_data=True, middle=False, video=False,\n",
    "                            deterministic=True,\n",
    "                            fp16=True)\n",
    "\n",
    "trainer.load_final_checkpoint(train=False)\n",
    "\n",
    "trainer.network.eval()\n",
    "\n",
    "trainer.get_throughput(optimal_batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 772/772 [00:07<00:00, 103.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "slice_number = 0\n",
    "path_list = glob(r'custom_quorum_2/**/*.gz', recursive=True)\n",
    "for path in tqdm(path_list):\n",
    "    if '_gt' in path:\n",
    "        continue\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    slice_number += arr.shape[-1]\n",
    "print(slice_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr = arr[0]\n",
    "t = torch.from_numpy(arr).unsqueeze(0)\n",
    "B, C, H, W = t.shape\n",
    "\n",
    "theta_list = []\n",
    "for i in range(128):\n",
    "    x1 = i\n",
    "    x2 = i+1\n",
    "    y1 = i\n",
    "    y2 = i+1\n",
    "    theta = torch.zeros(size=(2, 3))\n",
    "    theta[0, 0] = (x2 - x1) / W\n",
    "    theta[0, 1] = 0.0\n",
    "    theta[0, 2] = -1 + (x2 + x1) / W\n",
    "    theta[1, 0] = 0.0\n",
    "    theta[1, 1] = (y2 - y1) / H\n",
    "    theta[1, 2] = -1 + (y2 + y1) / H\n",
    "    theta_list.append(theta)\n",
    "theta_list = torch.stack(theta_list, dim=0)\n",
    "grid = torch.nn.functional.affine_grid(theta_list, size=(128, C, 1, 1))\n",
    "t = t.repeat(128, 1, 1, 1)\n",
    "out1 = torch.nn.functional.grid_sample(t, grid, mode='nearest')\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 1])\n",
      "tensor([25.])\n",
      "tensor(True)\n",
      "tensor([ 0., 25.])\n",
      "tensor([[5, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2bc035be2b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr = arr[0]\n",
    "t = torch.from_numpy(arr).unsqueeze(0)\n",
    "t = torch.zeros(size=(1, 256, 10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        t[:, :, i, j] = i * j\n",
    "\n",
    "B, C, H, W = t.shape\n",
    "\n",
    "x1 = 5\n",
    "x2 = 6\n",
    "y1 = 5\n",
    "y2 = 6\n",
    "theta = torch.zeros(size=(B, 2, 3))\n",
    "theta[:, 0, 0] = (x2 - x1) / W\n",
    "theta[:, 0, 1] = 0.0\n",
    "theta[:, 0, 2] = -1 + (x2 + x1) / W\n",
    "theta[:, 1, 0] = 0.0\n",
    "theta[:, 1, 1] = (y2 - y1) / H\n",
    "theta[:, 1, 2] = -1 + (y2 + y1) / H\n",
    "\n",
    "grid = torch.nn.functional.affine_grid(theta, size=(B, C, 1, 1))\n",
    "out1 = torch.nn.functional.grid_sample(t, grid, mode='nearest')\n",
    "print(out1.shape)\n",
    "\n",
    "print(torch.unique(out1[0, 5]))\n",
    "print(torch.all(out1[0, 5] == t[0, 5, x1, y1]))\n",
    "\n",
    "inv_theta = torch.clone(theta)\n",
    "inv_theta[:, 0, 0] = 1 / theta[:, 0, 0]\n",
    "inv_theta[:, 1, 1] = 1 / theta[:, 1, 1]\n",
    "inv_theta[:, 0, 2] = -theta[:, 0, 2] / theta[:, 0, 0]\n",
    "inv_theta[:, 1, 2] = -theta[:, 1, 2] / theta[:, 1, 1]\n",
    "\n",
    "inv_grid = torch.nn.functional.affine_grid(inv_theta, size=(B, C, H, W))\n",
    "inv_out1 = torch.nn.functional.grid_sample(out1, inv_grid, mode='nearest')\n",
    "\n",
    "print(torch.unique(inv_out1[0, 5]))\n",
    "print(torch.nonzero(inv_out1[0, 5]))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(inv_out1[0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "folders = [r'ACDC_output\\only_sfb', r'ACDC_output\\no_transformer', r'ACDC_output\\no_sfb']\n",
    "results_list = []\n",
    "for folder in folders:\n",
    "    with open(os.path.join(folder, r'validation_raw\\summary.json')) as fd_json:\n",
    "        data = json.load(fd_json)\n",
    "        results = data['results']['all']\n",
    "        for res in results:\n",
    "            rv_dice = res['1']['Dice']\n",
    "            myo_dice = res['2']['Dice']\n",
    "            lv_dice = res['3']['Dice']\n",
    "            results_list.append({'Method': folder.split('\\\\')[-1], 'RV': rv_dice, 'MYO': myo_dice, 'LV': lv_dice, 'Mean': (rv_dice + myo_dice + lv_dice) / 3})\n",
    "\n",
    "with open(os.path.join('ACDC_output', 'jmp.csv'), 'w') as fd_csv:\n",
    "    writer = csv.DictWriter(fd_csv, fieldnames=list(results_list[0].keys()))\n",
    "    writer.writeheader() \n",
    "    writer.writerows(results_list) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6872/4152755484.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mwaitforbuttonpress\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m   2306\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2308\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mwaitforbuttonpress\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   3123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3125\u001b[1;33m         _blocking_input.blocking_input_loop(\n\u001b[0m\u001b[0;32m   3126\u001b[0m             self, [\"button_press_event\", \"key_press_event\"], timeout, handler)\n\u001b[0;32m   3127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\_blocking_input.py\u001b[0m in \u001b[0;36mblocking_input_loop\u001b[1;34m(figure, event_names, timeout, handler)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmpl_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevent_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Start event loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Run even on exception like ctrl-c.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Disconnect the callbacks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backends\\backend_qt.py\u001b[0m in \u001b[0;36mstart_event_loop\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_maybe_allow_interrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mqt_compat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Portal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\backends\\qt_compat.py\u001b[0m in \u001b[0;36m_maybe_allow_interrupt\u001b[1;34m(qapp)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_sigint_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhandler_args\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[0mold_sigint_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mhandler_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nnunet.lib.position_embedding import PositionEmbeddingSine1d\n",
    "\n",
    "pos_1d = PositionEmbeddingSine1d(num_pos_feats=64, normalize=True)\n",
    "out = pos_1d(shape_util=(1, 20), device='cpu')\n",
    "print(out.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(out[:, 0])\n",
    "plt.waitforbuttonpress()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr = arr[0]\n",
    "\n",
    "t = torch.from_numpy(arr).unsqueeze(0)\n",
    "B, C, H, W = t.shape\n",
    "print(t.shape)\n",
    "nb_patches = 2\n",
    "\n",
    "patch_size = 64\n",
    "x1 = 100\n",
    "x2 = 164\n",
    "y1 = 100\n",
    "y2 = 164\n",
    "h = H\n",
    "w = W\n",
    "\n",
    "theta1 = torch.tensor([[(x2 - x1) / w, 0, -1 + (x2 + x1)/w], \n",
    "                        [0, (y2 - y1) / h, -1 + (y2 + y1)/h]])\n",
    "\n",
    "patch_size = 64\n",
    "x1 = 40\n",
    "x2 = 104\n",
    "y1 = 40\n",
    "y2 = 104\n",
    "h = H\n",
    "w = W\n",
    "\n",
    "theta2 = torch.tensor([[(x2 - x1) / w, 0, -1 + (x2 + x1)/w], \n",
    "                        [0, (y2 - y1) / h, -1 + (y2 + y1)/h]])\n",
    "\n",
    "\n",
    "theta = torch.stack([theta1, theta2], dim=0)\n",
    "print(theta)\n",
    "\n",
    "theta_inv = torch.clone(theta)\n",
    "#theta_inv = torch.cat([theta, torch.tensor([0, 0, 1]).view(1, 1, 3)], dim=1)\n",
    "#print(torch.inverse(theta_inv))\n",
    "theta_inv[:, 0, 0] = 1 / theta_inv[:, 0, 0]\n",
    "theta_inv[:, 1, 1] = 1 / theta_inv[:, 1, 1]\n",
    "theta_inv[:, 0, 2] = -theta_inv[:, 0, 2] * theta_inv[:, 0, 0]\n",
    "theta_inv[:, 1, 2] = -theta_inv[:, 1, 2] * theta_inv[:, 1, 1]\n",
    "\n",
    "grid = torch.nn.functional.affine_grid(theta, size=(B * nb_patches, C, patch_size, patch_size))\n",
    "grid2 = torch.nn.functional.affine_grid(theta_inv, size=(B * nb_patches, C, H, W))\n",
    "t = t.unsqueeze(1).repeat(1, nb_patches, 1, 1, 1).view(B * nb_patches, C, H, W)\n",
    "out = torch.nn.functional.grid_sample(t, grid)\n",
    "print(out.shape)\n",
    "out2 = torch.nn.functional.grid_sample(out, grid2)\n",
    "print(out2.shape)\n",
    "\n",
    "spatial_tokens = torch.where(out2 == 0, t, out2)\n",
    "\n",
    "cropped = t[:, :, y1:y2, x1:x2]\n",
    "print(torch.all(cropped[0, 5] == out[0, 5]))\n",
    "\n",
    "#assert torch.all(spatial_tokens[0, 5] == t[0, 5])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5)\n",
    "ax[0].imshow(out[0, 5], cmap='gray')\n",
    "ax[1].imshow(out[1, 5], cmap='gray')\n",
    "ax[2].imshow(out2[0, 5], cmap='gray')\n",
    "ax[3].imshow(out2[1, 5], cmap='gray')\n",
    "ax[4].imshow(spatial_tokens[0, 5], cmap='gray')\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr1 = arr[0, 2]\n",
    "arr2 = arr[0, 5]\n",
    "arr = np.stack([arr1, arr2], axis=0)[:, None, :, :]\n",
    "print(arr.shape)\n",
    "t = torch.from_numpy(arr).unsqueeze(0).repeat(2, 1, 1, 1, 1)\n",
    "t = t.unsqueeze(0).repeat(5, 1, 1, 1, 1, 1)\n",
    "T, B, M, C, H, W = t.shape\n",
    "\n",
    "t[:, :, :, :, :100, :100] = 0\n",
    "\n",
    "nonzero = torch.count_nonzero(t, dim=2)\n",
    "my_sum = t.sum(dim=2)\n",
    "out = torch.where(my_sum == 0, 0, my_sum / nonzero)\n",
    "print(out.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(arr1, cmap='gray')\n",
    "ax[1].imshow(arr2, cmap='gray')\n",
    "ax[2].imshow(out[0, 0, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_align\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr = arr[0, 5]\n",
    "\n",
    "arr = arr[None, None, :, :]\n",
    "arr = torch.from_numpy(arr)\n",
    "\n",
    "B, C, H, W = arr.shape\n",
    "b = torch.arange(1).unsqueeze(-1)\n",
    "coord = torch.tensor([100, 100, 164, 164], dtype=torch.float32).unsqueeze(0)\n",
    "boxes = torch.cat([b, coord], dim=1)\n",
    "out = roi_align(arr, boxes, output_size=64)\n",
    "out2 = torch.clone(arr)\n",
    "out2[:, :, 100:164, 100:164] = out\n",
    "print(torch.all(out2 == arr))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(arr[0, 0], cmap='gray')\n",
    "ax[1].imshow(out[0, 0], cmap='gray')\n",
    "ax[2].imshow(out2[0, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.ops import roi_align\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr = arr[0, 5]\n",
    "arr = arr[None, None, :, :]\n",
    "arr = torch.from_numpy(arr)\n",
    "B, C, H, W = arr.shape\n",
    "\n",
    "b = torch.arange(2).unsqueeze(-1)\n",
    "coord = torch.tensor([100, 100, 164, 164]).unsqueeze(0).repeat(2, 1)\n",
    "boxes = torch.cat([b, coord], dim=1)\n",
    "print(boxes.shape)\n",
    "print(boxes[:, 0])\n",
    "\n",
    "src = roi_align(arr, boxes.to(torch.float32), output_size=64)\n",
    "out2 = torch.clone(arr)\n",
    "\n",
    "print(boxes)\n",
    "\n",
    "for i in range(B):\n",
    "    out2[i, :, boxes[i, 1]:boxes[i, 3], boxes[i, 2]:boxes[i, 4]] = src[i]\n",
    "\n",
    "print(torch.all(out2 == arr))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(arr[0, 0], cmap='gray')\n",
    "ax[1].imshow(src[0, 0], cmap='gray')\n",
    "ax[2].imshow(out2[0, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _get_ref_points(H_key, W_key, B, dtype, device):\n",
    "        \n",
    "    ref_y, ref_x = torch.meshgrid(\n",
    "        torch.linspace(0.5, H_key - 0.5, H_key, dtype=dtype, device=device), \n",
    "        torch.linspace(0.5, W_key - 0.5, W_key, dtype=dtype, device=device)\n",
    "    )\n",
    "    ref = torch.stack((ref_y, ref_x), -1)\n",
    "    ref[..., 1].div_(W_key).mul_(2).sub_(1)\n",
    "    ref[..., 0].div_(H_key).mul_(2).sub_(1)\n",
    "    ref = ref[None, ...].expand(B * 1, -1, -1, -1) # B * g H W 2\n",
    "    \n",
    "    return ref\n",
    "\n",
    "arr = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "arr = arr[0]\n",
    "t = torch.from_numpy(arr).unsqueeze(0)\n",
    "B, C, H, W = t.shape\n",
    "\n",
    "ref_points = _get_ref_points(H, W, 1, dtype=torch.float, device='cpu')\n",
    "print(ref_points.shape)\n",
    "\n",
    "offsets = torch.rand_like(ref_points)\n",
    "grid = ref_points + offsets\n",
    "out = torch.nn.functional.grid_sample(t, grid, align_corners=True)\n",
    "grid2 = ref_points\n",
    "out2 = torch.nn.functional.grid_sample(out, -grid, align_corners=True)\n",
    "\n",
    "print(out.shape)\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(t[0, 5], cmap='gray')\n",
    "ax[1].imshow(out[0, 5], cmap='gray')\n",
    "ax[2].imshow(out2[0, 5], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x, weight):\n",
    "    return 1 / (1 + np.exp(-weight * x))\n",
    "\n",
    "x = np.linspace(-7, 7, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, sigmoid(x, weight=4))\n",
    "ax.plot(x, sigmoid(x, weight=1))\n",
    "ax.plot(x, sigmoid(x, weight=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[0.25, 0, 0.0709], [0, 0.25, -0.0039], [0, 0, 1]])\n",
    "print(t.shape)\n",
    "torch.inverse(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def printname(name):\n",
    "    print(name)\n",
    "\n",
    "f = h5py.File(r'C:\\Users\\Portal\\Documents\\contour extraction\\Quorum_Baseline_CardioTrack\\ID_250001-001_default_user.mat', 'r')\n",
    "f.visit(printname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from skimage.registration import optical_flow_tvl1\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr\\patient001_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "\n",
    "img1 = arr[:, :, 2]\n",
    "img2 = arr[:, :, 8]\n",
    "\n",
    "out = optical_flow_tvl1(img1, img2)\n",
    "print(out.shape)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "ax[0, 0].imshow(img1, cmap='gray')\n",
    "ax[0, 1].imshow(img2, cmap='gray')\n",
    "ax[1, 0].imshow(out[0], cmap='plasma')\n",
    "ax[1, 1].imshow(out[1], cmap='plasma')\n",
    "plt.waitforbuttonpress()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.array([[0., 1, 2], [0., 1, 2], [0., 1, 2]])\n",
    "\n",
    "cmap = cm.jet  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "print(len(cmaplist))\n",
    "\n",
    "cmaplist[0] = (0, 0, 0, 1.0)\n",
    "\n",
    "# create the new map\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0, 4, 5)\n",
    "print(bounds)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "print(norm(a))\n",
    "a_new = cmap(norm(a))\n",
    "print(norm)\n",
    "print(a_new)\n",
    "\n",
    "plt.imshow(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skimage.transform import resize\n",
    "\n",
    "data = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\imagesTr\\\\patient001_frame01_0000.nii.gz')\n",
    "gata_gt = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\labelsTr\\\\patient001_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "arr_gt = gata_gt.get_fdata()\n",
    "print(arr.shape)\n",
    "print(arr_gt.shape)\n",
    "\n",
    "out = resize(arr_gt, (216, 256, 15), order=0)\n",
    "\n",
    "arr = torch.from_numpy(arr)\n",
    "arr_gt = torch.from_numpy(arr_gt)\n",
    "\n",
    "f = torch.fft.fft2(arr[:, :, 1])\n",
    "f_gt = torch.fft.fft2(arr_gt[:, :, 1])\n",
    "\n",
    "fig, ax = plt.subplots(3, 2)\n",
    "ax[0, 0].imshow(arr[:, :, 1], cmap='gray')\n",
    "ax[0, 1].imshow(arr_gt[:, :, 1], cmap='gray')\n",
    "ax[1, 0].imshow(torch.log(torch.fft.fftshift(f.abs())), cmap='gray')\n",
    "ax[1, 1].imshow(torch.log(torch.fft.fftshift(f_gt.abs())), cmap='gray')\n",
    "ax[2, 0].imshow(torch.fft.fftshift(f.angle()), cmap='gray')\n",
    "ax[2, 1].imshow(torch.fft.fftshift(f_gt.angle()), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "data = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\imagesTr\\\\patient001_frame01_0000.nii.gz')\n",
    "gata_gt = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\labelsTr\\\\patient001_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "arr_gt = gata_gt.get_fdata()\n",
    "print(arr.shape)\n",
    "print(arr_gt.shape)\n",
    "\n",
    "nb = 15\n",
    "out = resize(arr_gt, (216, 256, nb), order=0, anti_aliasing=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, nb)\n",
    "for i in range(nb):\n",
    "    for j in range(arr_gt.shape[2]):\n",
    "        print(np.all(arr_gt[:, :, j] == out[:, :, i]))\n",
    "    print('***************************')\n",
    "    ax[i].imshow(out[:, :, i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "data = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\imagesTr\\\\patient001_frame01_0000.nii.gz')\n",
    "gata_gt = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\labelsTr\\\\patient001_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "arr_gt = gata_gt.get_fdata()\n",
    "print(arr.shape)\n",
    "print(arr_gt.shape)\n",
    "\n",
    "arr = torch.from_numpy(arr)\n",
    "arr_gt = torch.from_numpy(arr_gt)\n",
    "\n",
    "f = torch.fft.fftshift(torch.fft.fft2(arr[:, :, 1]))\n",
    "f_gt = torch.fft.fftshift(torch.fft.fft2(arr_gt[:, :, 1]))\n",
    "\n",
    "cy = f.shape[0] / 2\n",
    "cx = f.shape[1] / 2\n",
    "\n",
    "f[int(cy - (cy * 0.5)):int(cy + (cy * 0.5)), int(cx - (cx * 0.5)):int(cx + (cx * 0.5))] = 0\n",
    "f_gt[int(cy - (cy * 0.5)):int(cy + (cy * 0.5)), int(cx - (cx * 0.5)):int(cx + (cx * 0.5))] = 0\n",
    "\n",
    "f = torch.fft.ifftshift(f)\n",
    "f_gt = torch.fft.ifftshift(f_gt)\n",
    "\n",
    "f_after = torch.fft.ifft2(f)\n",
    "f_gt_after = torch.fft.ifft2(f_gt)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "ax[0, 0].imshow(arr[:, :, 1], cmap='gray')\n",
    "ax[0, 1].imshow(arr_gt[:, :, 1], cmap='gray')\n",
    "ax[1, 0].imshow(f_after.real, cmap='gray')\n",
    "ax[1, 1].imshow(f_gt_after.real, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "\n",
    "def get_slice_numbers(path_list):\n",
    "    slice_number_list = []\n",
    "    for path in path_list:\n",
    "        slice_nb = path.split('Slice')[-1].split('_')[1]\n",
    "        slice_number_list.append(slice_nb)\n",
    "    return list(set(slice_number_list))\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "new_folder_name = 'custom_lib'\n",
    "\n",
    "delete_if_exist(new_folder_name)\n",
    "\n",
    "Path(new_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, patient in enumerate(tqdm(glob('data_ok/*/*'))):\n",
    "    patient_string = ('patient' + str(i + 1).zfill(3))\n",
    "    patient_folder_name = os.path.join(new_folder_name, patient_string)\n",
    "    delete_if_exist(patient_folder_name)\n",
    "    Path(patient_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "    all_patient_files = glob(os.path.join(patient, '*'))\n",
    "    slice_nb_list = get_slice_numbers(all_patient_files)\n",
    "    for slice_nb in slice_nb_list:\n",
    "        gt_list = []\n",
    "        image_list = []\n",
    "        for patient_file in all_patient_files:\n",
    "            if slice_nb not in patient_file:\n",
    "                continue\n",
    "            else:\n",
    "                data = np.load(patient_file)\n",
    "                image_list.append(data['arr_0'])\n",
    "                gt_list.append(data['arr_1'])\n",
    "        image_list = np.stack(image_list, axis=2)\n",
    "        gt_list = np.stack(gt_list, axis=2)\n",
    "        nib.save(nib.Nifti1Image(image_list, affine=np.eye(4)), os.path.join(patient_folder_name, patient_string + '_frame' + slice_nb + '.nii.gz'))  \n",
    "        nib.save(nib.Nifti1Image(gt_list, affine=np.eye(4)), os.path.join(patient_folder_name, patient_string + '_frame' + slice_nb + '_gt.nii.gz'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "shape_list = []\n",
    "name_list = []\n",
    "for path in tqdm(glob(r'out\\nnUNet_preprocessed\\Task028_Lib\\custom_experiment_planner_stage0/*.npz')):\n",
    "    data = np.load(path)\n",
    "    shape_list.append(data['data'].shape[1])\n",
    "    name_list.append(path)\n",
    "shape_list = np.array(shape_list)\n",
    "name_list = np.array(name_list)\n",
    "idx = np.argmax(shape_list)\n",
    "print(shape_list[idx])\n",
    "print(name_list[idx])\n",
    "print(shape_list.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = nib.load('patient177_slice14.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "print(arr.shape)\n",
    "\n",
    "data2 = nib.load('gt_lib\\patient177_slice14.nii.gz')\n",
    "arr2 = data2.get_fdata()\n",
    "print(arr2.shape)\n",
    "\n",
    "data3 = nib.load('patient177_slice14_0000.nii.gz')\n",
    "arr3 = data3.get_fdata()\n",
    "print(arr3.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(arr[:, :, arr.shape[-1] // 2], cmap='gray')\n",
    "ax[1].imshow(arr2[:, :, arr2.shape[-1] // 2], cmap='gray')\n",
    "ax[2].imshow(arr3[:, :, arr3.shape[-1] // 2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "x_list = []\n",
    "y_list = []\n",
    "z_list = []\n",
    "for path in glob(r'ACDC_training/**/*4d*.gz', recursive=True):\n",
    "    data = nib.load(path)\n",
    "    zoom = data.header.get_zooms()\n",
    "    x_list.append(zoom[0])\n",
    "    y_list.append(zoom[1])\n",
    "    z_list.append(zoom[2])\n",
    "\n",
    "x_list = np.array(x_list)\n",
    "y_list = np.array(y_list)\n",
    "z_list = np.array(z_list)\n",
    "\n",
    "print(x_list.min())\n",
    "print(x_list.max())\n",
    "print(y_list.min())\n",
    "print(y_list.max())\n",
    "print(z_list.min())\n",
    "print(z_list.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import LabelToContour\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "data3 = nib.load('patient177_slice14_0000.nii.gz')\n",
    "arr3 = data3.get_fdata()\n",
    "arr3 = np.transpose(arr3, (2, 0, 1))\n",
    "out_arr = []\n",
    "for i in range(len(arr3)):\n",
    "    out = cv.normalize(arr3[i], None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX).astype(np.uint8)\n",
    "    out = cv.cvtColor(out, cv.COLOR_GRAY2RGB)\n",
    "    out_arr.append(out)\n",
    "out_arr = np.stack(out_arr, axis=0)\n",
    "\n",
    "label_to_contour_obj = LabelToContour()\n",
    "data = nib.load('patient177_slice14.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "arr = np.transpose(arr, (2, 0, 1))\n",
    "\n",
    "contour_list = []\n",
    "for i in range(len(arr)):\n",
    "    for j in range(1, 4):\n",
    "        seg = (arr[i] == j).astype(np.uint8)\n",
    "        contours, hierarchy = cv.findContours(seg, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        color = [0, 0, 0]\n",
    "        color[j - 1] = 255\n",
    "        cv.drawContours(out_arr[i], contours, -1, color, 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(out_arr[20, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(r'out\\nnUNet_preprocessed\\Task028_Lib\\custom_experiment_planner_stage0\\patient158_slice05.npz')\n",
    "print(data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "data = nib.load(r'custom_lib_t\\patient158\\patient158_slice05.nii.gz')\n",
    "print(data.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "res_t_list = []\n",
    "for path in glob(r'custom_lib_t\\**\\*.gz', recursive=True):\n",
    "    if '_gt.' in path:\n",
    "        continue\n",
    "    data = nib.load(path)\n",
    "    p = data.header['pixdim'][3]\n",
    "    res_t_list.append(p)\n",
    "    if p > 0.3:\n",
    "        print(path)\n",
    "\n",
    "x = np.arange(len(res_t_list))\n",
    "plt.scatter(x=x, y=res_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "l = glob(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task028_Lib\\imagesTr\\*.gz', recursive=True)\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(r'out\\nnUNet_preprocessed\\Task028_Lib\\splits_final.pkl', 'rb') as f:\n",
    "    print(pkl.load(f))\n",
    "print('**********************************************************************************************************************************************')\n",
    "with open(r'splits_final.pkl', 'rb') as f:\n",
    "    print(pkl.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "video_length = 5\n",
    "s = 1\n",
    "voulume_length = 15\n",
    "\n",
    "start = s - (video_length // 2)\n",
    "stop = start + video_length\n",
    "assert (start >= 0 or stop <= voulume_length)\n",
    "if start < 0:\n",
    "    stop += abs(start)\n",
    "    start = 0\n",
    "if stop > voulume_length:\n",
    "    start -= stop - voulume_length\n",
    "    stop = voulume_length\n",
    "indices = np.arange(start=start, stop=stop)\n",
    "assert len(indices) == video_length\n",
    "idx = np.where(indices == s)[0]\n",
    "print(indices)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_length = 7\n",
    "video_length = 2\n",
    "s = 0\n",
    "\n",
    "\n",
    "values = np.arange(volume_length)\n",
    "step = video_length // 2\n",
    "start = min(max(s - step, 0), volume_length - video_length)\n",
    "indices = values[start:start + video_length]\n",
    "eval_idx = int(np.where(indices == s)[0])\n",
    "print(indices)\n",
    "print(eval_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from nnunet.postprocessing.connected_components import remove_all_but_the_largest_connected_component\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\custom_experiment_planner_stage0\\patient001_frame01.npy')\n",
    "print(data.shape)\n",
    "arr = data[1, 5][None, None, :, :]\n",
    "arr[:, :, 50:100, 50:60] = 2\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(arr[0, 0], cmap='gray')\n",
    "out = remove_all_but_the_largest_connected_component(arr, for_which_classes=[2], volume_per_voxel=1)\n",
    "\n",
    "print(out[1])\n",
    "\n",
    "ax[1].imshow(out[0][0, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('summary.json', 'r') as f:\n",
    "  data = json.load(f)['results']['all']\n",
    "  per_folder_results = {'RACINE': [], 'cholcoeur': [], 'desktop': []}\n",
    "  for patient in tqdm(data):\n",
    "    patient_1 = patient['1']['Dice']\n",
    "    patient_2 = patient['2']['Dice']\n",
    "    patient_3 = patient['3']['Dice']\n",
    "    patient_name = patient['test'].split('/')[-1].split('.')[0].split('_')[0]\n",
    "\n",
    "    path = os.path.join('custom_lib_t', patient_name, 'info.cfg')\n",
    "    with open(path, 'r') as fd:\n",
    "      folder = fd.read().splitlines()[0].split('\\\\')[1]\n",
    "      print(folder)\n",
    "    results = np.array([patient_1, patient_2, patient_3])\n",
    "    print(results)\n",
    "    print(patient['test'])\n",
    "    per_folder_results[str(folder)].append(results)\n",
    "  \n",
    "out_dict = {}\n",
    "for key in per_folder_results.keys():\n",
    "  temp = np.stack(per_folder_results[key], axis=0)\n",
    "  print(temp.shape)\n",
    "  out_dict[key] = np.mean(temp, axis=0)\n",
    "print(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "for path in tqdm(glob(r'custom_lib_t/**/*gt.nii.gz')):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    out = ~np.isin([1, 2, 3], np.unique(arr))\n",
    "    if np.any(out):\n",
    "        print(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load(r'data_ok_original\\cholcoeur\\Original\\ID_4002099103_20150325_default_user_Slice_07.npz')\n",
    "img = data['image']\n",
    "label = data['label_img']\n",
    "print(img.shape)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img[10], cmap='gray')\n",
    "ax[1].imshow(label[10], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(r'gt_lib\\patient114_slice04.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "print(arr.shape)\n",
    "\n",
    "data2 = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task028_Lib\\imagesTr\\patient114_slice04_0000.nii.gz')\n",
    "arr2 = data2.get_fdata()\n",
    "print(arr2.shape)\n",
    "\n",
    "#data3 = nib.load(r'patient114_slice04.nii.gz')\n",
    "#arr3 = data3.get_fdata()\n",
    "#print(arr3.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(arr[:, :, 20], cmap='gray')\n",
    "ax[1].imshow(arr2[:, :, 20], cmap='gray')\n",
    "ax[2].imshow(arr3[:, :, 20], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import cv2 as cv\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = nib.load(r'ACDC_training\\patient025\\patient025_4d.nii.gz')\n",
    "data = data.get_fdata()\n",
    "print(data.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout()\n",
    "image_list = []\n",
    "for i in range(data.shape[3]):\n",
    "    image = cv.normalize(data[:, :, 0, i], None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F).astype(np.uint8)\n",
    "    im = ax.imshow(image, cmap='gray', animated=True)\n",
    "    image_list.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, image_list, interval=500, blit=True, repeat_delay=1000)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import cv2 as cv\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = nib.load(r'only_sfb\\validation_raw\\patient005_frame01.nii.gz')\n",
    "data = data.get_fdata()\n",
    "print(data.shape)\n",
    "\n",
    "label = nib.load(r'only_sfb\\gt_niftis\\patient005_frame01.nii.gz')\n",
    "label = label.get_fdata()\n",
    "print(label.shape)\n",
    "\n",
    "blended_list = []\n",
    "for i in range(len(label.shape[2])):\n",
    "    blended = cv.addWeighted(data[:, :, i], 0.5, label[:, :, i], 0.5, 0.0)\n",
    "    blended_list.append(blended)\n",
    "blended_list = np.stack(blended_list, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout()\n",
    "image_list = []\n",
    "for i in range(data.shape[3]):\n",
    "    image = cv.normalize(data[:, :, 1, i], None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F).astype(np.uint8)\n",
    "    im = ax.imshow(image, cmap='gray', animated=True)\n",
    "    image_list.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, image_list, interval=500, blit=True, repeat_delay=1000)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "#class RelativeAttention2D(nn.Module):\n",
    "#    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "#        super().__init__()\n",
    "#        inner_dim = dim_head * heads\n",
    "#        project_out = not (heads == 1 and dim_head == inp)\n",
    "#\n",
    "#        self.ih, self.iw = image_size\n",
    "#\n",
    "#        self.heads = heads\n",
    "#        self.scale = dim_head ** -0.5\n",
    "#\n",
    "#        # parameter table of relative position bias\n",
    "#        self.relative_bias_table = nn.Parameter(\n",
    "#            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "#\n",
    "#        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "#        coords = torch.flatten(torch.stack(coords), 1)\n",
    "#        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "#\n",
    "#        relative_coords[0] += self.ih - 1\n",
    "#        relative_coords[1] += self.iw - 1\n",
    "#        relative_coords[0] *= 2 * self.iw - 1 # 2d indices to 1d, need to multiply by width of image * 2\n",
    "#        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "#        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "#        self.register_buffer(\"relative_index\", relative_index)\n",
    "#\n",
    "#        self.attend = nn.Softmax(dim=-1)\n",
    "#        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "#\n",
    "#        self.to_out = nn.Sequential(\n",
    "#            nn.Linear(inner_dim, oup),\n",
    "#            nn.Dropout(dropout)\n",
    "#        ) if project_out else nn.Identity()\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        # B, L, E\n",
    "#        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "#        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "#\n",
    "#        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "#\n",
    "#        # Use \"gather\" for more efficiency on GPUs\n",
    "#        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, self.heads))\n",
    "#        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "#        dots = dots + relative_bias\n",
    "#\n",
    "#        attn = self.attend(dots)\n",
    "#        out = torch.matmul(attn, v)\n",
    "#        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "#        out = self.to_out(out)\n",
    "#        return out\n",
    "\n",
    "class RelativeAttention1D(nn.Module):\n",
    "    def __init__(self, inp, oup, seq_length, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(torch.zeros(2 * seq_length - 1, heads))\n",
    "\n",
    "        coords = torch.arange(seq_length)\n",
    "        coords = torch.stack(torch.meshgrid([coords]))\n",
    "        coords = torch.flatten(coords, 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += seq_length - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, L, E\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=self.seq_length, w=self.seq_length)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossRelativeAttention(nn.Module):\n",
    "    def __init__(self, inp, oup, size_2d, size_1d, rescaled, heads, dim_head, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.rescaled = rescaled\n",
    "\n",
    "        self.ih = self.iw = size_2d\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table_2d = nn.Parameter(torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords_2d = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords_2d = torch.flatten(torch.stack(coords_2d), 1)\n",
    "        relative_coords_2d = coords_2d[:, :, None] - coords_2d[:, None, :]\n",
    "\n",
    "        relative_coords_2d[0] += self.ih - 1\n",
    "        relative_coords_2d[1] += self.iw - 1\n",
    "        relative_coords_2d[0] *= 2 * self.iw - 1\n",
    "        relative_coords_2d = rearrange(relative_coords_2d, 'c h w -> h w c')\n",
    "        relative_index_2d = relative_coords_2d.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index_2d\", relative_index_2d)\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.seq_length = size_1d\n",
    "        self.relative_bias_table_1d = nn.Parameter(torch.zeros(2 * size_1d - 1, heads))\n",
    "\n",
    "        coords_1d = torch.arange(size_1d)\n",
    "        coords_1d = torch.stack(torch.meshgrid([coords_1d]))\n",
    "        coords_1d = torch.flatten(coords_1d, 1)\n",
    "        relative_coords_1d = coords_1d[:, :, None] - coords_1d[:, None, :]\n",
    "\n",
    "        relative_coords_1d[0] += size_1d - 1\n",
    "        relative_coords_1d = rearrange(relative_coords_1d, 'c h w -> h w c')\n",
    "        relative_index_1d = relative_coords_1d.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index_1d\", relative_index_1d)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_q = nn.Linear(inp, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(inp, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(inp, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        # B, L, E\n",
    "        q = self.to_q(query)\n",
    "        k = self.to_k(key)\n",
    "        v = self.to_v(value)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), [q, k, v])\n",
    "\n",
    "        relative_bias_2d = self.relative_bias_table_2d.gather(0, self.relative_index_2d.repeat(1, self.heads))\n",
    "        relative_bias_2d = rearrange(relative_bias_2d, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "\n",
    "        relative_bias_1d = self.relative_bias_table_1d.gather(0, self.relative_index_1d.repeat(1, self.heads))\n",
    "        relative_bias_1d = rearrange(relative_bias_1d, '(h w) c -> 1 c h w', h=self.seq_length, w=self.seq_length)\n",
    "\n",
    "        if self.rescaled == '1d':\n",
    "            relative_bias_2d = torch.matmul(q.transpose(-1, -2), relative_bias_2d).transpose(-1, -2)\n",
    "            relative_bias_1d = torch.matmul(k.transpose(-1, -2), relative_bias_1d).transpose(-1, -2)\n",
    "            q = q + relative_bias_2d\n",
    "            k = k + relative_bias_1d\n",
    "            v = v + relative_bias_1d\n",
    "        elif self.rescaled == '2d':\n",
    "            relative_bias_1d = torch.matmul(q.transpose(-1, -2), relative_bias_1d).transpose(-1, -2)\n",
    "            relative_bias_2d = torch.matmul(k.transpose(-1, -2), relative_bias_2d).transpose(-1, -2)\n",
    "            q = q + relative_bias_1d\n",
    "            k = k + relative_bias_2d\n",
    "            v = v + relative_bias_2d\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "attn = CrossRelativeAttention(256, 256, size_2d=28, size_1d=6, rescaled='1d', heads=8, dim_head=64)\n",
    "\n",
    "q = torch.rand(size=(2, 28*28, 256))\n",
    "v = k = torch.rand(size=(2, 6, 256))\n",
    "attn(query=q, key=k, value=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caca(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(10, 30)\n",
    "    \n",
    "    def forward(x):\n",
    "        return x\n",
    "\n",
    "class Caca2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 10)\n",
    "        self.layer2 = nn.Linear(10, 10)\n",
    "        self.layer3 = nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(x):\n",
    "        return x\n",
    "\n",
    "caca = Caca()\n",
    "caca2 = Caca2()\n",
    "\n",
    "nb_params =  sum(p.numel() for p in caca.parameters() if p.requires_grad)\n",
    "nb_params2 =  sum(p.numel() for p in caca2.parameters() if p.requires_grad)\n",
    "\n",
    "print(nb_params)\n",
    "print(nb_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_1d(input_resolution):\n",
    "    # get pair-wise relative position index for each token inside the window\n",
    "    coords = torch.arange(input_resolution[0])\n",
    "    coords_flatten = torch.stack(torch.meshgrid([coords]))\n",
    "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww     i-i', j-j' [-(wh-1), wh-1]\n",
    "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "    relative_coords[:, :, 0] += input_resolution[0] - 1  # shift to start from 0    i-i'+ h-1, j-j'+ w-1 [0, 2wh-2]\n",
    "    relative_position_index = relative_coords.sum(-1)  # Wh, Ww  parameters weights are 1d [0, (2wh-1) * (2ww-1)]\n",
    "    return relative_position_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "from monai.visualize.utils import blend_images\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import cv2 as cv\n",
    "\n",
    "def get_patient():\n",
    "    path_list = glob(r'only_sfb\\validation_raw\\*gz', recursive=True)\n",
    "    max_depth = 0\n",
    "    max_path = 'caca'\n",
    "    for path in path_list:\n",
    "        data = nib.load(path)\n",
    "        data = data.get_fdata()\n",
    "        depth = data.shape[2]\n",
    "        if depth >= max_depth:\n",
    "            max_depth = depth\n",
    "            max_path = path\n",
    "    return max_path.split(os.sep)[-1][:-7]\n",
    "\n",
    "name = get_patient()\n",
    "print(name)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "pred = nib.load(r\"only_sfb\\validation_raw\\\\\" + name + \".nii.gz\")\n",
    "pred = pred.get_fdata()\n",
    "print(pred.shape)\n",
    "pred = np.flipud(pred)\n",
    "\n",
    "img = nib.load(r\"out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\imagesTr\\\\\" + name + \"_0000.nii.gz\")\n",
    "img = img.get_fdata()\n",
    "print(img.shape)\n",
    "\n",
    "img = np.flipud(img)\n",
    "\n",
    "img = np.transpose(img, (2, 0, 1))\n",
    "out_arr = []\n",
    "for i in range(len(img)):\n",
    "    out = cv.normalize(img[i], None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX).astype(np.uint8)\n",
    "    out = cv.cvtColor(out, cv.COLOR_GRAY2RGB)\n",
    "    out_arr.append(out)\n",
    "out_arr = np.stack(out_arr, axis=0)\n",
    "\n",
    "patient_images = []\n",
    "for i in range(pred.shape[-1]):\n",
    "    current_seg = pred[:, :, i]\n",
    "    current_img = img[:, :, i]\n",
    "    for j in range(1, 4):\n",
    "        seg = (current_seg == j).astype(np.uint8)\n",
    "        contours, hierarchy = cv.findContours(seg, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        color = [0, 0, 0]\n",
    "        color[j - 1] = 255\n",
    "        cv.drawContours(out_arr[i], contours, -1, color, 1)\n",
    "\n",
    "    #out = blend_images(x[None, :, :], seg[None, :, :], alpha=0.75, cmap=cm.hsv).transpose(1, 2, 0)\n",
    "    #W, H, C = out.shape\n",
    "    #out = cv2.normalize(out, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    #patient_images.append(out)\n",
    "\n",
    "    #print(out.transpose(1, 2, 0).shape)\n",
    "    #fig, ax = plt.subplots(1, 1)\n",
    "    #ax.imshow(out.transpose(1, 2, 0))\n",
    "    #plt.show()\n",
    "    #plt.waitforbuttonpress()\n",
    "    #plt.close(fig)\n",
    "\n",
    "video_name = name + '.avi'\n",
    "W, H = out_arr.shape[1], out_arr.shape[2]\n",
    "video = cv2.VideoWriter(video_name, fourcc, 2, (H, W))\n",
    "\n",
    "for i in range(len(out_arr)):\n",
    "    video.write(cv2.cvtColor(out_arr[i], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "from monai.visualize.utils import blend_images\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "name = \"patient002_slice05\"\n",
    "print(name)\n",
    "\n",
    "pred = nib.load(name + \".nii.gz\")\n",
    "pred = pred.get_fdata()\n",
    "print(pred.shape)\n",
    "\n",
    "img = nib.load(r\"out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task028_Lib\\imagesTr\\\\\" + name + \"_0000.nii.gz\")\n",
    "img = img.get_fdata()\n",
    "print(img.shape)\n",
    "\n",
    "img = np.transpose(img, (2, 0, 1))\n",
    "out_arr = []\n",
    "for i in range(len(img)):\n",
    "    out = cv.normalize(img[i], None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX).astype(np.uint8)\n",
    "    out = cv.cvtColor(out, cv.COLOR_GRAY2RGB)\n",
    "    out_arr.append(out)\n",
    "out_arr = np.stack(out_arr, axis=0)\n",
    "\n",
    "patient_images = []\n",
    "for i in range(pred.shape[-1]):\n",
    "    current_seg = pred[:, :, i]\n",
    "    current_img = img[:, :, i]\n",
    "    for j in range(1, 4):\n",
    "        seg = (current_seg == j).astype(np.uint8)\n",
    "        contours, hierarchy = cv.findContours(seg, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        color = [0, 0, 0]\n",
    "        color[j - 1] = 255\n",
    "        cv.drawContours(out_arr[i], contours, -1, color, 1)\n",
    "\n",
    "    #out = blend_images(x[None, :, :], seg[None, :, :], alpha=0.75, cmap=cm.hsv).transpose(1, 2, 0)\n",
    "    #W, H, C = out.shape\n",
    "    #out = cv2.normalize(out, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    #patient_images.append(out)\n",
    "\n",
    "    #print(out.transpose(1, 2, 0).shape)\n",
    "    #fig, ax = plt.subplots(1, 1)\n",
    "    #ax.imshow(out.transpose(1, 2, 0))\n",
    "    #plt.show()\n",
    "    #plt.waitforbuttonpress()\n",
    "    #plt.close(fig)\n",
    "\n",
    "video_name = name + '.avi'\n",
    "W, H = out_arr.shape[1], out_arr.shape[2]\n",
    "video = cv2.VideoWriter(video_name, fourcc, 6, (H, W))\n",
    "\n",
    "for i in range(len(out_arr)):\n",
    "    video.write(cv2.cvtColor(out_arr[i], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res_t_list = []\n",
    "for path in glob(r'data_ok_original/**/*.npz', recursive=True):\n",
    "    data = np.load(path)\n",
    "    res_t_list.append(data['resT'])\n",
    "\n",
    "x = np.arange(len(res_t_list))\n",
    "plt.scatter(x=x, y=res_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(r'data_ok_original\\cholcoeur\\Original\\ID_._20150218_default_user_Slice_04.npz')\n",
    "print(data.files)\n",
    "print(data['image'].shape)\n",
    "print(data['resT'])\n",
    "print(data['resX'])\n",
    "print(data['Pixel_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "def fix_labels(label):\n",
    "    label[label == 1] = 3\n",
    "    label[label == 4] = 1\n",
    "    return label\n",
    "\n",
    "new_folder_name = 'custom_lib_t'\n",
    "\n",
    "delete_if_exist(new_folder_name)\n",
    "\n",
    "Path(new_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "folder_nb = 0\n",
    "\n",
    "all_files = glob('data_ok_original/**/*.npz', recursive=True)\n",
    "sorted_all_files = sorted(all_files)\n",
    "\n",
    "past_name = ''\n",
    "for patient in tqdm(sorted_all_files):\n",
    "\n",
    "    data = np.load(patient)\n",
    "    img = np.transpose(data['image'], (1, 2, 0))\n",
    "    label = np.transpose(data['label_img'], (1, 2, 0))\n",
    "    label = fix_labels(label)\n",
    "    if np.any(~np.isin([1, 2, 3], np.unique(label))):\n",
    "        with open(os.path.join(new_folder_name, 'failed_patients.cfg'), 'a') as fd:\n",
    "            fd.write(patient + '\\n')\n",
    "        continue\n",
    "\n",
    "    broken_string = patient.split('_Slice_')\n",
    "    name = broken_string[0]\n",
    "    if name != past_name:\n",
    "        folder_nb += 1\n",
    "        patient_string = ('patient' + str(folder_nb).zfill(3))\n",
    "        patient_folder_name = os.path.join(new_folder_name, patient_string)\n",
    "        delete_if_exist(patient_folder_name)\n",
    "        Path(patient_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "    with open(os.path.join(patient_folder_name, 'info.cfg'), 'a') as fd:\n",
    "        fd.write(patient + '\\n')\n",
    "    \n",
    "    slice_nb = broken_string[-1].split('.')[0]\n",
    "    img = nib.Nifti1Image(img, affine=np.eye(4))\n",
    "    label = nib.Nifti1Image(label, affine=np.eye(4))\n",
    "    res_t = max(1e-7, data['resT'])\n",
    "    img.header['pixdim'] = [1.0, data['resY'], data['resX'], res_t, 1.0, 1.0, 1.0, 1.0]\n",
    "    label.header['pixdim'] = [1.0, data['resY'], data['resX'], res_t, 1.0, 1.0, 1.0, 1.0]\n",
    "    nib.save(img, os.path.join(patient_folder_name, patient_string + '_slice' + slice_nb + '.nii.gz'))  \n",
    "    nib.save(label, os.path.join(patient_folder_name, patient_string + '_slice' + slice_nb + '_gt.nii.gz'))\n",
    "    past_name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(r'data_saud\\3D\\cineED\\250001-001_ED\\250001-001_ED_label.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "print(arr.shape)\n",
    "\n",
    "plt.imshow(arr[:, :, 5], cmap='gray')\n",
    "\n",
    "data = nib.load(r'ACDC_training\\patient001\\patient001_frame01_gt.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(r'custom_quorum\\patient001\\patient001_ed_gt.nii.gz')\n",
    "ed_img_arr = data.get_fdata()\n",
    "print(ed_img_arr.dtype)\n",
    "print(np.unique(ed_img_arr))\n",
    "\n",
    "plt.imshow(ed_img_arr[:, :, 5], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from torch.nn.functional import pad\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.rand(size=(2, 3, 128, 128))\n",
    "video_padding = torch.ones(size=(2, 3), dtype=bool)\n",
    "video_padding[1, 1] = False\n",
    "\n",
    "x[video_padding] = torch.zeros(size=(128, 128))\n",
    "\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "for i in range(len(video_padding)):\n",
    "    for j in range(video_padding.shape[1]):\n",
    "        ax[i, j].imshow(x[i, j], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "path = r'data_saud_2\\3D\\ES\\348003-008\\348003-008_image.nii.gz'\n",
    "label_path = r'data_saud_2\\3D\\ES\\348003-008\\348003-008_label.nii.gz'\n",
    "data = nib.load(path)\n",
    "label_data = nib.load(label_path)\n",
    "arr = data.get_fdata()\n",
    "label_arr = label_data.get_fdata()\n",
    "print(path)\n",
    "print(label_path)\n",
    "\n",
    "for i in range(arr.shape[-1]):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(arr[:, :, i], cmap='gray')\n",
    "    ax[1].imshow(label_arr[:, :, i], cmap='gray')\n",
    "    plt.show()\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nib.load(r'ACDC_training\\patient001\\patient001_frame01_gt.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(arr[:, :, 5], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_list = glob(r'data_saud_2\\3D\\**\\*label.nii.gz', recursive=True)\n",
    "for path in path_list:\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    arr = swap_labels(arr)\n",
    "    print(path)\n",
    "\n",
    "    for i in range(arr.shape[-1]):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.imshow(arr[:, :, i], cmap='gray')\n",
    "        plt.show()\n",
    "        plt.waitforbuttonpress()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:44<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def swap_labels(x):\n",
    "    x[x == 3] = 4\n",
    "    x[x == 1] = 3\n",
    "    x[x == 4] = 1\n",
    "    return x\n",
    "\n",
    "def get_metadata():\n",
    "    data_dict = {}\n",
    "    for path in glob(r'data_saud\\2D\\**\\*.npz', recursive=True):\n",
    "        patient_name = path.split('\\\\')[-1].split('_')[0]\n",
    "        data = np.load(path)\n",
    "        if patient_name not in data_dict:\n",
    "            data_dict[patient_name] = {'strength': [], 'manufacturer': []}\n",
    "        data_dict[patient_name]['strength'].append(data['strength'])\n",
    "        data_dict[patient_name]['manufacturer'].append(data['manufacturer'])\n",
    "\n",
    "    out_dict = data_dict.copy()\n",
    "    for key in data_dict.keys():\n",
    "        strength_list = data_dict[key]['strength']\n",
    "        manufacturer_list = data_dict[key]['manufacturer']\n",
    "        assert strength_list.count(strength_list[0]) == len(strength_list), print(key)\n",
    "        assert manufacturer_list.count(manufacturer_list[0]) == len(manufacturer_list), print(key)\n",
    "        out_dict[key]['strength'] = data_dict[key]['strength'][0]\n",
    "        out_dict[key]['manufacturer'] = data_dict[key]['manufacturer'][0]\n",
    "    return out_dict\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "new_folder_name = 'custom_quorum_2'\n",
    "\n",
    "delete_if_exist(new_folder_name)\n",
    "\n",
    "Path(new_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "folder_nb = 0\n",
    "\n",
    "ed_images = glob(r'data_saud_2\\3D\\ED\\**\\*image.nii.gz', recursive=True)\n",
    "ed_labels = glob(r'data_saud_2\\3D\\ED\\**\\*label.nii.gz', recursive=True)\n",
    "es_images = glob(r'data_saud_2\\3D\\ES\\**\\*image.nii.gz', recursive=True)\n",
    "es_labels = glob(r'data_saud_2\\3D\\ES\\**\\*label.nii.gz', recursive=True)\n",
    "ed_metadata = glob(r'data_saud_2\\3D\\ED\\**\\*.csv', recursive=True)\n",
    "es_metadata = glob(r'data_saud_2\\3D\\ES\\**\\*.csv', recursive=True)\n",
    "ed_images = sorted(ed_images)\n",
    "ed_labels = sorted(ed_labels)\n",
    "es_images = sorted(es_images)\n",
    "es_labels = sorted(es_labels)\n",
    "ed_metadata = sorted(ed_metadata)\n",
    "es_metadata = sorted(es_metadata)\n",
    "\n",
    "#info_dict = get_metadata()\n",
    "#assert len(info_dict) == len(ed_images)\n",
    "\n",
    "for idx, (ed_img_path, es_img_path, ed_gt_path, es_gt_path, ed_metadata_path, es_metadata_path) in enumerate(tqdm(zip(ed_images, es_images, ed_labels, es_labels, ed_metadata, es_metadata), total=len(ed_images))):\n",
    "    patient_name = ed_img_path.split('\\\\')[-1].split('_')[0]\n",
    "\n",
    "    #strength = str(info_dict[patient_name]['strength'])\n",
    "    #manufacturer = str(info_dict[patient_name]['manufacturer'])\n",
    "\n",
    "    ed_img = nib.load(ed_img_path)\n",
    "    es_img = nib.load(es_img_path)\n",
    "    ed_gt = nib.load(ed_gt_path)\n",
    "    es_gt = nib.load(es_gt_path)\n",
    "\n",
    "    ed_header = ed_img.header.copy()\n",
    "    es_header = es_img.header.copy()\n",
    "    ed_gt_header = ed_gt.header.copy()\n",
    "    es_gt_header = es_gt.header.copy()\n",
    "\n",
    "    ed_affine = ed_img.affine.copy()\n",
    "    es_affine = es_img.affine.copy()\n",
    "    ed_gt_affine = ed_gt.affine.copy()\n",
    "    es_gt_affine = es_gt.affine.copy()\n",
    "\n",
    "    ed_img_arr = ed_img.get_fdata()\n",
    "    es_img_arr = es_img.get_fdata()\n",
    "    ed_gt_arr = ed_gt.get_fdata()\n",
    "    es_gt_arr = es_gt.get_fdata()\n",
    "\n",
    "    assert np.any(np.isin([1, 2, 3], np.unique(ed_gt_arr))), print(ed_gt_path)\n",
    "    assert np.any(np.isin([1, 2, 3], np.unique(es_gt_arr))), print(es_gt_path)\n",
    "\n",
    "    ed_gt_arr = swap_labels(ed_gt_arr)\n",
    "    es_gt_arr = swap_labels(es_gt_arr)\n",
    "\n",
    "    patient_string = ('patient' + str(idx + 1).zfill(3))\n",
    "    patient_folder_name = os.path.join(new_folder_name, patient_string)\n",
    "    delete_if_exist(patient_folder_name)\n",
    "    Path(patient_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy(ed_metadata_path, os.path.join(patient_folder_name, 'ed_info.csv'))\n",
    "    shutil.copy(es_metadata_path, os.path.join(patient_folder_name, 'es_info.csv'))\n",
    "\n",
    "    ed_img = nib.Nifti1Image(ed_img_arr, affine=ed_affine, header=ed_header)\n",
    "    es_img = nib.Nifti1Image(es_img_arr, affine=es_affine, header=es_header)\n",
    "    ed_gt = nib.Nifti1Image(ed_gt_arr, affine=ed_gt_affine, header=ed_gt_header)\n",
    "    es_gt = nib.Nifti1Image(es_gt_arr, affine=es_gt_affine, header=es_gt_header)\n",
    "\n",
    "    ed_img.set_data_dtype(np.float64)\n",
    "    es_img.set_data_dtype(np.float64)\n",
    "    ed_gt.set_data_dtype(np.float64)\n",
    "    es_gt.set_data_dtype(np.float64)\n",
    "\n",
    "    nib.save(ed_img, os.path.join(patient_folder_name, patient_string + '_ed.nii.gz'))  \n",
    "    nib.save(ed_gt, os.path.join(patient_folder_name, patient_string + '_ed_gt.nii.gz'))\n",
    "\n",
    "    nib.save(es_img, os.path.join(patient_folder_name, patient_string + '_es.nii.gz'))  \n",
    "    nib.save(es_gt, os.path.join(patient_folder_name, patient_string + '_es_gt.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:30<00:00,  6.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def delete_if_exist(folder_name):\n",
    "    dirpath = Path(folder_name)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "\n",
    "new_folder_name = r'data_saud_2\\custom_quorum'\n",
    "\n",
    "delete_if_exist(new_folder_name)\n",
    "\n",
    "Path(new_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "folder_nb = 0\n",
    "\n",
    "ed_images = glob(r'data_saud_2\\3D_images\\ED\\**\\*image.nii.gz', recursive=True)\n",
    "es_images = glob(r'data_saud_2\\3D_images\\ES\\**\\*image.nii.gz', recursive=True)\n",
    "ed_metadata = glob(r'data_saud_2\\3D_images\\ED\\**\\*.csv', recursive=True)\n",
    "es_metadata = glob(r'data_saud_2\\3D_images\\ES\\**\\*.csv', recursive=True)\n",
    "ed_images = sorted(ed_images)\n",
    "es_images = sorted(es_images)\n",
    "ed_metadata = sorted(ed_metadata)\n",
    "es_metadata = sorted(es_metadata)\n",
    "\n",
    "#info_dict = get_metadata()\n",
    "#assert len(info_dict) == len(ed_images)\n",
    "\n",
    "for idx, (ed_img_path, es_img_path, ed_metadata_path, es_metadata_path) in enumerate(tqdm(zip(ed_images, es_images, ed_metadata, es_metadata), total=len(ed_images))):\n",
    "    patient_name = ed_img_path.split('\\\\')[-1].split('_')[0]\n",
    "\n",
    "    #strength = str(info_dict[patient_name]['strength'])\n",
    "    #manufacturer = str(info_dict[patient_name]['manufacturer'])\n",
    "\n",
    "    ed_img = nib.load(ed_img_path)\n",
    "    es_img = nib.load(es_img_path)\n",
    "\n",
    "    ed_header = ed_img.header.copy()\n",
    "    es_header = es_img.header.copy()\n",
    "\n",
    "    ed_affine = ed_img.affine.copy()\n",
    "    es_affine = es_img.affine.copy()\n",
    "\n",
    "    ed_img_arr = ed_img.get_fdata()\n",
    "    es_img_arr = es_img.get_fdata()\n",
    "\n",
    "    patient_string = ('patient' + str(idx + 1).zfill(3))\n",
    "    patient_folder_name = os.path.join(new_folder_name, patient_string)\n",
    "    delete_if_exist(patient_folder_name)\n",
    "    Path(patient_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy(ed_metadata_path, os.path.join(patient_folder_name, 'ed_info.csv'))\n",
    "    shutil.copy(es_metadata_path, os.path.join(patient_folder_name, 'es_info.csv'))\n",
    "\n",
    "    ed_img = nib.Nifti1Image(ed_img_arr, affine=ed_affine, header=ed_header)\n",
    "    es_img = nib.Nifti1Image(es_img_arr, affine=es_affine, header=es_header)\n",
    "\n",
    "    ed_img.set_data_dtype(np.float64)\n",
    "    es_img.set_data_dtype(np.float64)\n",
    "\n",
    "    nib.save(ed_img, os.path.join(patient_folder_name, patient_string + '_ed.nii.gz'))\n",
    "    nib.save(es_img, os.path.join(patient_folder_name, patient_string + '_es.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('original_size_of_raw_data', array([ 13, 248, 288])), ('original_spacing', array([7.        , 1.31944442, 1.31944442])), ('list_of_data_files', ['C:\\\\Users\\\\Portal\\\\Documents\\\\Isensee\\\\nnUNet\\\\nnunet\\\\out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task029_Quorum\\\\imagesTr\\\\patient001_ed_0000.nii.gz']), ('seg_file', 'C:\\\\Users\\\\Portal\\\\Documents\\\\Isensee\\\\nnUNet\\\\nnunet\\\\out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task029_Quorum\\\\labelsTr\\\\patient001_ed.nii.gz'), ('itk_origin', (0.0, 0.0, 0.0)), ('itk_spacing', (1.3194444179534912, 1.3194444179534912, 7.0)), ('itk_direction', (-0.0, 1.0, 0.0, 1.0, -0.0, 0.0, 0.0, 0.0, -1.0)), ('center', '250001-001'), ('manufacturer', 'siemens'), ('phase', '39'), ('strength', '1.5'), ('slice thickness', '7.0'), ('spacing between slices', '7.0'), ('crop_bbox', [[0, 13], [1, 248], [1, 288]]), ('classes', array([-1.,  0.,  1.,  2.,  3.], dtype=float32)), ('size_after_cropping', (13, 247, 287)), ('use_nonzero_mask_for_norm', OrderedDict([(0, False)])), ('size_after_resampling', (13, 232, 269)), ('spacing_after_resampling', array([7.        , 1.40629995, 1.40629995])), ('class_locations', {1: array([[  0,  90, 115],\n",
      "       [  0,  93, 116],\n",
      "       [  3,  83, 154],\n",
      "       ...,\n",
      "       [  5,  79, 137],\n",
      "       [  2,  90, 110],\n",
      "       [  2,  86, 152]], dtype=int64), 2: array([[  0, 135, 126],\n",
      "       [  4,  99, 148],\n",
      "       [  0, 106, 121],\n",
      "       ...,\n",
      "       [  8,  96, 142],\n",
      "       [  4, 144, 129],\n",
      "       [  8, 122, 156]], dtype=int64), 3: array([[  1, 114, 127],\n",
      "       [  5, 113, 135],\n",
      "       [  8, 118, 143],\n",
      "       ...,\n",
      "       [  5, 135, 133],\n",
      "       [  7, 116, 152],\n",
      "       [  5, 125, 149]], dtype=int64)})])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'out\\nnUNet_preprocessed\\Task029_Quorum\\custom_experiment_planner_stage0\\patient001_ed.pkl', 'rb') as fd:\n",
    "    print(pickle.load(fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 256])\n",
      "tensor([-0.8660])\n",
      "tensor([-0.5000])\n",
      "tensor([-0.6863])\n",
      "tensor([-1.7325e-06])\n",
      "tensor([1.])\n",
      "tensor([-0.4225])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Portal\\Documents\\Isensee\\nnUNet\\nnunet\\lib\\position_embedding.py:65: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19cfeae70d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from lib.position_embedding import PositionEmbeddingSine1d\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "B = 2\n",
    "T = 3\n",
    "H = 128\n",
    "W = 128\n",
    "C = 256\n",
    "\n",
    "pos_1d = PositionEmbeddingSine1d(num_pos_feats=C, normalize=True)\n",
    "out = pos_1d(shape_util=(B, T), device='cpu')\n",
    "print(out.shape)\n",
    "out1 = out[1].view(B, C, 1, 1).repeat(1, 1, H, W)\n",
    "out2 = out[2].view(B, C, 1, 1).repeat(1, 1, H, W)\n",
    "print(torch.unique(out1[0, 0]))\n",
    "print(torch.unique(out1[0, 1]))\n",
    "print(torch.unique(out1[0, 2]))\n",
    "print(torch.unique(out2[0, 0]))\n",
    "print(torch.unique(out2[0, 1]))\n",
    "print(torch.unique(out2[0, 2]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(out1[0, 0], cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class RelativeAttention2D(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads, dim_head, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_q = nn.Linear(inp, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(inp, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(inp, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        # B, L, E\n",
    "        q = self.to_q(query)\n",
    "        k = self.to_k(key)\n",
    "        v = self.to_v(value)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), [q, k, v])\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "        print(dots.shape)\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "t = torch.randn(size=(12, 256, 512))\n",
    "attn = RelativeAttention2D(inp=512, oup=512, image_size=[16, 16], heads=8, dim_head=64)\n",
    "out = attn(query=t, key=t, value=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class SlotAttention(nn.Module):\n",
    "    def __init__(self, num_slots, dim, iters = 3, eps = 1e-8, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.iters = iters\n",
    "        self.eps = eps\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.slots_logsigma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim)\n",
    "        self.to_k = nn.Linear(dim, dim)\n",
    "        self.to_v = nn.Linear(dim, dim)\n",
    "\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "\n",
    "        hidden_dim = max(dim, hidden_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "        self.norm_input  = nn.LayerNorm(dim)\n",
    "        self.norm_slots  = nn.LayerNorm(dim)\n",
    "        self.norm_pre_ff = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, inputs, num_slots = None):\n",
    "        b, n, d, device = *inputs.shape, inputs.device\n",
    "        n_s = num_slots if num_slots is not None else self.num_slots\n",
    "        \n",
    "        mu = self.slots_mu.expand(b, n_s, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(b, n_s, -1)\n",
    "\n",
    "        slots = mu + sigma * torch.randn(mu.shape, device = device)\n",
    "\n",
    "        inputs = self.norm_input(inputs)        \n",
    "        k, v = self.to_k(inputs), self.to_v(inputs)\n",
    "\n",
    "        for _ in range(self.iters):\n",
    "            slots_prev = slots\n",
    "\n",
    "            slots = self.norm_slots(slots)\n",
    "            q = self.to_q(slots)\n",
    "\n",
    "            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale\n",
    "            attn = dots.softmax(dim=1) + self.eps\n",
    "            print(attn.shape)\n",
    "            attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            updates = torch.einsum('bjd,bij->bid', v, attn)\n",
    "\n",
    "            print(updates.shape)\n",
    "            slots = self.gru(\n",
    "                updates.reshape(-1, d),\n",
    "                slots_prev.reshape(-1, d)\n",
    "            )\n",
    "            print(slots.shape)\n",
    "\n",
    "            slots = slots.reshape(b, -1, d)\n",
    "            slots = slots + self.mlp(self.norm_pre_ff(slots))\n",
    "\n",
    "        return slots\n",
    "\n",
    "t = torch.randn(size=(12, 256, 512))\n",
    "slot_attention = SlotAttention(num_slots=8, dim=512, iters=1)\n",
    "out = slot_attention(t)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_p_3 = nib.load(r'data_saud\\3D\\cineED\\276008-001_ED\\276008-001_ED_image.nii.gz')\n",
    "data_p_15 = nib.load(r'data_saud\\3D\\cineED\\616012-012_ED\\616012-012_ED_image.nii.gz')\n",
    "data_s_15 = nib.load(r'data_saud\\3D\\cineED\\250001-001_ED\\250001-001_ED_label.nii.gz')\n",
    "\n",
    "arr_p_3 = data_p_3.get_fdata()\n",
    "arr_p_15 = data_p_15.get_fdata()\n",
    "arr_s_15 = data_s_15.get_fdata()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(arr_p_3[:, :, arr_p_3.shape[-1] // 2], cmap='gray')\n",
    "ax[1].imshow(arr_p_15[:, :, arr_p_15.shape[-1] // 2], cmap='gray')\n",
    "ax[2].imshow(arr_s_15[:, :, arr_s_15.shape[-1] // 2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(r'out\\nnUNet_raw_data_base\\nnUNet_cropped_data\\Task029_Quorum\\patient001_ed.npz')\n",
    "arr = data['data']\n",
    "print(arr.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(arr_p_3[:, :, arr_p_3.shape[-1] // 2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'out\\nnUNet_preprocessed\\Task029_Quorum\\splits_final.pkl', 'rb') as fd:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nib.load(r'data_saud\\3D\\cineED\\250001-001_ED\\250001-001_ED_label.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "print(arr.shape)\n",
    "print(np.unique(arr))\n",
    "print(np.any(np.isin([1., 2.], np.unique(arr))))\n",
    "\n",
    "plt.imshow(arr[:, :, 5], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "patient_list = glob(r'ACDC_training/*')\n",
    "for patient_folder_path in tqdm(patient_list):\n",
    "    file_paths = glob(os.path.join(patient_folder_path, '*.gz'))\n",
    "    file_paths = [x for x in file_paths if '4d' not in x]\n",
    "    gt_file_path = [x for x in file_paths if '_gt' in x]\n",
    "    image_file_path = [x for x in file_paths if '_gt' not in x]\n",
    "    for img_path, gt_path in zip(image_file_path, gt_file_path):\n",
    "        data_img = nib.load(img_path)\n",
    "        data_gt = nib.load(gt_path)\n",
    "        img = data_img.get_fdata()\n",
    "        gt = data_gt.get_fdata()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(img[:, :, 0], cmap='gray')\n",
    "        ax[1].imshow(gt[:, :, 0], cmap='gray')\n",
    "        plt.show()\n",
    "        plt.waitforbuttonpress()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randn(size=(4, 64, 128*128))\n",
    "t2 = torch.randn(size=(2, 4, 64))\n",
    "t2 = t2.unsqueeze(0).repeat(2, 1, 1, 1).view(4, 4, 64)\n",
    "\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "\n",
    "out = t2 @ t1\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "nb_slices_list = []\n",
    "res_t_list = []\n",
    "path_list = glob(r'ACDC_training/**/*4d*', recursive=True)\n",
    "for path in tqdm(path_list):\n",
    "    data = nib.load(path)\n",
    "    res_t = data.header.get_zooms()[-1]\n",
    "    res_t_list.append(res_t)\n",
    "    arr = data.get_fdata()\n",
    "    nb_slices_list.append(arr.shape[3])\n",
    "\n",
    "nb_slices_list = np.array(nb_slices_list)\n",
    "res_t_list = np.array(res_t_list)\n",
    "print(nb_slices_list.min())\n",
    "print(nb_slices_list.max())\n",
    "\n",
    "print(res_t_list.min())\n",
    "print(res_t_list.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'caca': 3}\n",
      "{'caca': 3}\n",
      "0\n",
      "['dodo']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'out\\nnUNet_preprocessed\\Task029_Quorum\\custom_experiment_planner_stage0\\patient001_ed.pkl', 'rb') as fd:\n",
    "    properties = pickle.load(fd)\n",
    "    key_list = ['center', 'manufacturer', 'phase', 'strength', 'slice thickness', 'spacing between slices']\n",
    "    key_list = ['caca']\n",
    "    out = {key: properties[key] for key in key_list if key in properties}\n",
    "    dummy = {'caca': 3}\n",
    "    print(out)\n",
    "    print(dummy)\n",
    "    dummy.update(out)\n",
    "    print(dummy)\n",
    "    print(len(out))\n",
    "    a = list(out.keys()) + ['dodo']\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(r'data_saud_2\\3D\\ED\\703001-013\\703001-013_image.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "\n",
    "for i in range(arr.shape[-1]):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(arr[:, :, i], cmap='gray')\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(size=(3, 2, 1, 128, 128), requires_grad=True)\n",
    "\n",
    "indices = torch.zeros(size=(3, 2)).long()\n",
    "indices[1, 1] = 1\n",
    "indices[2, 0] = 1\n",
    "\n",
    "indx_tuple = torch.nonzero(indices, as_tuple=True)\n",
    "print(indx_tuple)\n",
    "\n",
    "out = t[indx_tuple[0], indx_tuple[1]]\n",
    "print(out.shape)\n",
    "print(out.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "nb_slices_list = []\n",
    "res_t_list = []\n",
    "path_list = glob(r'custom_lib_t/**/*.gz', recursive=True)\n",
    "for path in tqdm(path_list):\n",
    "    data = nib.load(path)\n",
    "    res_t = data.header.get_zooms()[2]\n",
    "    res_t_list.append(res_t)\n",
    "    arr = data.get_fdata()\n",
    "    nb_slices_list.append(arr.shape[2])\n",
    "\n",
    "nb_slices_list = np.array(nb_slices_list)\n",
    "res_t_list = np.array(res_t_list)\n",
    "print(nb_slices_list.min())\n",
    "print(nb_slices_list.max())\n",
    "\n",
    "print(res_t_list.min())\n",
    "print(res_t_list.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'out\\nnUNet_preprocessed\\Task028_Lib\\custom_experiment_planner_stage0\\patient001_slice05.pkl', 'rb') as fd:\n",
    "    print(pickle.load(fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "for path in glob(r'out\\nnUNet_preprocessed\\Task026_MMs\\gt_segmentations\\*'):\n",
    "    img = nib.load(path)\n",
    "    img = img.get_fdata()\n",
    "    assert np.count_nonzero(img > 0) > 0\n",
    "\n",
    "#plt.imshow(img[:, :, 8], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'splits_final.pkl', 'rb') as fd:\n",
    "    d = pickle.load(fd)\n",
    "    print(len(d[0]['val']))\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(size=(2, 384, 28, 28))\n",
    "x = torch.flatten(x, start_dim=2)\n",
    "norm = torch.linalg.norm(x, dim=2, keepdim=True)\n",
    "x = x / torch.max(norm, torch.tensor([1e-8], device=x.device))\n",
    "out = torch.transpose(x, dim0=1, dim1=2)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(2, 384, 28, 28))\n",
    "\n",
    "x = torch.flatten(x, start_dim=2)\n",
    "norm = torch.linalg.norm(x, dim=2, keepdim=True)\n",
    "x = x / torch.max(norm, torch.tensor([1e-8], device=x.device))\n",
    "x = torch.matmul(x, torch.transpose(x, dim0=1, dim1=2))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1, 2], [2, 1, 0]])\n",
    "x = x.float()\n",
    "means = x.mean(dim=1).unsqueeze(-1)\n",
    "print(x.shape)\n",
    "print(means.shape)\n",
    "out = (x - means).unsqueeze(0)\n",
    "norm = torch.linalg.norm(out, dim=2, keepdim=True)\n",
    "out = out / torch.max(norm, torch.tensor([1e-8], device=x.device))\n",
    "out = torch.matmul(out, torch.transpose(out, dim0=1, dim1=2))\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 2], [1, 1], [2, 0]]).T\n",
    "print(torch.cov(x))\n",
    "x = x.float()\n",
    "means = x.mean(dim=1).unsqueeze(-1)\n",
    "print(x.shape)\n",
    "print(means.shape)\n",
    "out = (x - means).unsqueeze(0)\n",
    "out = torch.matmul(out, torch.transpose(out, dim0=1, dim1=2)) / 2\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "for data in tqdm(glob(r'out\\nnUNet_preprocessed\\Task027_ACDC\\gt_segmentations\\*.gz', recursive=True)):\n",
    "    data = nib.load(data)\n",
    "    arr = data.get_fdata()\n",
    "    for i in range(arr.shape[-1]):\n",
    "        payload = torch.from_numpy(arr[:, :, i])\n",
    "        payload = center_crop(payload, 224)\n",
    "        top = payload[0]\n",
    "        bottom = payload[-1]\n",
    "        left = payload[:, 0]\n",
    "        right = payload[:, -1]\n",
    "        if torch.count_nonzero(top) > 0 or torch.count_nonzero(bottom) > 0 or torch.count_nonzero(left) > 0 or torch.count_nonzero(right) > 0:\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.imshow(payload, cmap='gray')\n",
    "            plt.show()\n",
    "            plt.waitforbuttonpress()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "for path in tqdm(glob(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task026_MMs\\labelsTr\\*.gz', recursive=True)):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    idx = arr.shape[-1] // 2\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    print(path)\n",
    "    ax.imshow(arr[:, :, idx], cmap='gray')\n",
    "    plt.show()\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = nib.load(r'out\\nnUNet_preprocessed\\Task027_ACDC\\gt_segmentations\\patient001_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "idx = arr.shape[-1] // 2\n",
    "idx2 = arr.shape[-1] - 1\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(arr[:, :, idx], cmap='gray')\n",
    "ax[1].imshow(arr[:, :, idx2], cmap='gray')\n",
    "plt.show()\n",
    "plt.waitforbuttonpress()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "a = np.zeros(shape=(224, 224))\n",
    "p1 = (112, 112)\n",
    "p2 = (160, 160)\n",
    "a[p1[0], p1[1]] = 1\n",
    "a[p2[0], p2[1]] = 1\n",
    "cv2.line(a, p1, p2, 1, thickness=3, lineType=8)\n",
    "#blurred = cv2.GaussianBlur(a, (51, 51), sigmaX=0)\n",
    "blurred = gaussian_filter(a, sigma=11)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(blurred, cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_m = 0\n",
    "for path in glob(r'out\\nnUNet_preprocessed\\Task027_ACDC\\gt_segmentations\\*.gz'):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    if arr.shape[-1] > max_m:\n",
    "        max_m = arr.shape[-1]\n",
    "\n",
    "print(max_m)\n",
    "\n",
    "s = np.zeros(shape=(max_m,))\n",
    "l_global = []\n",
    "for path in tqdm(glob(r'out\\nnUNet_preprocessed\\Task027_ACDC\\gt_segmentations\\*.gz')):\n",
    "    data = nib.load(path)\n",
    "    arr = data.get_fdata()\n",
    "    arr = np.reshape(arr, newshape=(-1, arr.shape[-1]))\n",
    "    #unique_list = []\n",
    "    l = []\n",
    "    for i in range(arr.shape[-1]):\n",
    "        c = (np.count_nonzero(arr[:, i] == 1) + np.count_nonzero(arr[:, i] == 2) + np.count_nonzero(arr[:, i] == 3)) / arr[:, i].size\n",
    "        l.append(c)\n",
    "        #un = len(np.unique(arr[:, i])) - 1\n",
    "        #unique_list.append(un)\n",
    "    xvals = np.linspace(0, 100, num=max_m)\n",
    "    x = np.linspace(0, 100, num=len(l))\n",
    "    l = np.interp(xvals, xp=x, fp=l)\n",
    "    l_global.append(l)\n",
    "\n",
    "out = np.stack(l_global, axis=0)\n",
    "out = out.mean(axis=0)\n",
    "\n",
    "print(np.argmax(out))\n",
    "print(np.argsort(out))\n",
    "print(out)\n",
    "x = np.arange(len(out))\n",
    "plt.bar(x, height=out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "\n",
    "d = {0.3333333333333333: tensor(1.1544), 0.6363636363636364: tensor(0.2079), 0.6: tensor(1.3401), 0.375: tensor(1.1136), 0.5: tensor(1.3585), 0.46153846153846156: tensor(0.1114), 0.25: tensor(1.1556), 0.3125: tensor(0.1625), 0.42857142857142855: tensor(1.0352), 0.4: tensor(0.9940), 0.625: tensor(1.0483), 0.3: tensor(1.0919), 0.2857142857142857: tensor(1.0862), 0.2222222222222222: tensor(1.0759), 0.2: tensor(1.1787), 0.6666666666666666: tensor(0.2189), 0.5625: tensor(0.2846), 0.5555555555555556: tensor(1.0100), 0.2727272727272727: tensor(0.1884), 0.5714285714285714: tensor(1.0634), 0.18181818181818182: tensor(0.2598), 0.36363636363636365: tensor(0.1537), 0.4444444444444444: tensor(1.0899), 0.4375: tensor(0.1911), 0.5294117647058824: tensor(0.1057), 0.29411764705882354: tensor(0.0806), 0.21428571428571427: tensor(0.0797), 0.5454545454545454: tensor(0.2504), 0.23529411764705882: \n",
    "tensor(0.0697), 0.4117647058823529: tensor(0.0584), 0.16666666666666666: tensor(0.3279), 0.35294117647058826: tensor(0.0639), 0.6153846153846154: tensor(0.0822), 0.47058823529411764: tensor(0.0736), 0.6875: tensor(0.1425), 0.4166666666666667: tensor(0.1050), 0.35714285714285715: tensor(0.1576), 0.45454545454545453: tensor(0.1356), 0.6428571428571429: tensor(0.0692), 0.5384615384615384: tensor(0.0627), 0.23076923076923078: tensor(0.0421), 0.6470588235294118: tensor(0.0502), 0.5833333333333334: tensor(0.0563), 0.5333333333333333: tensor(0.1363), 0.2777777777777778: tensor(0.0440), 0.3076923076923077: tensor(0.0696), 0.5882352941176471: tensor(0.0589), 0.4666666666666667: tensor(0.0482), 0.26666666666666666: tensor(0.0289), 0.38461538461538464: tensor(0.0116), 0.3888888888888889: tensor(0.0103), 0.6111111111111112: tensor(0.0189)}\n",
    "\n",
    "d = dict(sorted(d.items()))\n",
    "d2 = copy(d)\n",
    "out = savgol_filter(list(d.values()), 50, 3)\n",
    "\n",
    "for k, v in zip(d.keys(), out): # window size 20, polynomial order 3\n",
    "    d[k] = v\n",
    "\n",
    "print(min(d, key= lambda x: d[x]))\n",
    "\n",
    "x = np.linspace(min(list(d.keys())), max(list(d.keys())), len(list(d.values())))\n",
    "\n",
    "plt.plot(x, list(d2.values()))\n",
    "plt.plot(x,out, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def get_distance_image2(label1, label2, norm):\n",
    "\n",
    "        h, w = label1.shape\n",
    "\n",
    "        accumulation = np.zeros((2, h, w), dtype=np.float32)\n",
    "        for t in range(1, 4):\n",
    "            current_class1 = (label1 == t).astype(int)\n",
    "            current_class2 = (label2 == t).astype(int)\n",
    "            outer_mask = np.logical_or(current_class1, current_class2)\n",
    "            current_class = current_class1 - current_class2\n",
    "            #plt.imshow(current_class1 - current_class2, cmap='gray')\n",
    "            current_class[current_class < 0] = 0\n",
    "            current_class[outer_mask == 0] = 1\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(current_class, cmap='gray')\n",
    "            ax[1].imshow(np.logical_not(current_class), cmap='gray')\n",
    "            plt.waitforbuttonpress()\n",
    "            plt.close(fig)\n",
    "\n",
    "            dst, labels = cv.distanceTransformWithLabels(current_class.astype(np.uint8), cv.DIST_L2, cv.DIST_MASK_PRECISE, labelType=cv.DIST_LABEL_PIXEL)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.imshow(labels, cmap='gray')\n",
    "            plt.waitforbuttonpress()\n",
    "            plt.close(fig)\n",
    "\n",
    "            \n",
    "            # labels is a LABEL map indicating LABEL (not index) of nearest zero pixel. Zero pixels have different labels.\n",
    "            #  As a result som labels in backgound and in heart structure can have the same label.\n",
    "\n",
    "            #fig, ax = plt.subplots(1, 1)\n",
    "            #ax.imshow(index, cmap='gray')\n",
    "\n",
    "            place = np.argwhere(current_class == 0) # get coords of background pixels\n",
    "            nearCord = place[labels-1,:] # get coords of nearest zero pixel of EVERY pixels of the image. For background this is current coords.\n",
    "\n",
    "            nearPixel = np.transpose(nearCord, axes=(2, 0, 1))\n",
    "\n",
    "            grid = np.indices(current_class.shape).astype(float)\n",
    "\n",
    "            diff = grid - nearPixel\n",
    "            diff[0][outer_mask == 0] = 0\n",
    "            diff[1][outer_mask == 0] = 0\n",
    "            diff[0][current_class == 0] = 0\n",
    "            diff[1][current_class == 0] = 0\n",
    "\n",
    "            if norm:\n",
    "                dr = np.sqrt(np.sum(diff**2, axis = 0))\n",
    "            else:\n",
    "                dr = np.ones_like(current_class1)\n",
    "\n",
    "            #nearpixeltoshow2 = np.copy(nearPixel2)\n",
    "            #nearpixeltoshow2[0][current_class2 == 0] = 0\n",
    "            #nearpixeltoshow2[0][current_class2 == 0] = 0\n",
    "\n",
    "            fig, ax = plt.subplots(1, 6)\n",
    "            ax[0].imshow(current_class1, cmap='gray')\n",
    "            ax[1].imshow(current_class2, cmap='gray')\n",
    "            ax[2].imshow(outer_mask, cmap='gray')\n",
    "            ax[3].imshow(current_class, cmap='gray')\n",
    "            ax[4].imshow(nearPixel[0], cmap='gray')\n",
    "            ax[5].imshow(diff[0], cmap='gray')\n",
    "            plt.show()\n",
    "            plt.waitforbuttonpress()\n",
    "            plt.close(fig)\n",
    "\n",
    "            #direction = np.zeros((2, h, w), dtype=np.float32)\n",
    "            #direction[0, current_class>0] = np.divide(diff[0, current_class>0], dr[current_class>0])\n",
    "            #direction[1, current_class>0] = np.divide(diff[1, current_class>0], dr[current_class>0])\n",
    "#\n",
    "            #accumulation[:, current_class>0] = 0\n",
    "            #accumulation = accumulation + direction\n",
    "\n",
    "        #assert accumulation.max() <= 1.0 and accumulation.min() >= -1.0\n",
    "        #return accumulation\n",
    "\n",
    "def get_distance_image(label, norm):\n",
    "\n",
    "        h, w = label.shape\n",
    "\n",
    "        accumulation = np.zeros((2, h, w), dtype=np.float32)\n",
    "        for t in range(1, 4):\n",
    "            current_class = (label == t).astype(np.uint8)\n",
    "            dst, labels = cv.distanceTransformWithLabels(current_class, cv.DIST_L2, cv.DIST_MASK_PRECISE, labelType=cv.DIST_LABEL_PIXEL)\n",
    "            \n",
    "            # labels is a LABEL map indicating LABEL (not index) of nearest zero pixel. Zero pixels have different labels.\n",
    "            #  As a result som labels in backgound and in heart structure can have the same label.\n",
    "            index = np.copy(labels)\n",
    "            index[current_class > 0] = 0\n",
    "\n",
    "            #fig, ax = plt.subplots(1, 1)\n",
    "            #ax.imshow(index, cmap='gray')\n",
    "\n",
    "            place = np.argwhere(index > 0) # get coords of background pixels\n",
    "            nearCord = place[labels-1,:] # get coords of nearest zero pixel of EVERY pixels of the image. For background this is current coords.\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.imshow(nearCord[:, :, 0], cmap='gray')\n",
    "\n",
    "            nearPixel = np.transpose(nearCord, axes=(2, 0, 1))\n",
    "            grid = np.indices(current_class.shape).astype(float)\n",
    "\n",
    "            diff = grid - nearPixel\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.imshow(diff[0, :, :], cmap='gray')\n",
    "\n",
    "            if norm:\n",
    "                dr = np.sqrt(np.sum(diff**2, axis = 0))\n",
    "            else:\n",
    "                dr = np.ones_like(current_class)\n",
    "\n",
    "            direction = np.zeros((2, h, w), dtype=np.float32)\n",
    "            direction[0, current_class>0] = np.divide(diff[0, current_class>0], dr[current_class>0])\n",
    "            direction[1, current_class>0] = np.divide(diff[1, current_class>0], dr[current_class>0])\n",
    "\n",
    "            accumulation[:, current_class>0] = 0\n",
    "            accumulation = accumulation + direction\n",
    "\n",
    "            #fig, ax = plt.subplots(1, 5)\n",
    "            #ax[0].imshow(labels, cmap='gray')\n",
    "            #ax[1].imshow(index, cmap='gray')\n",
    "            #ax[2].imshow(nearPixel[0], cmap='gray')\n",
    "            #ax[3].imshow(diff[0], cmap='gray')\n",
    "            #ax[4].imshow(accumulation[0], cmap='gray')\n",
    "            #plt.show()\n",
    "            #plt.waitforbuttonpress()\n",
    "            #plt.close(fig)\n",
    "\n",
    "        assert accumulation.max() <= 1.0 and accumulation.min() >= -1.0\n",
    "        return accumulation\n",
    "\n",
    "data = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr\\patient001_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "get_distance_image2(arr[:, :, 2], arr[:, :, -2], norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from regex import D\n",
    "from scipy.ndimage import distance_transform_edt as eucl_distance\n",
    "import nibabel as nib\n",
    "\n",
    "data = nib.load(r'out\\nnUNet_raw_data_base\\nnUNet_raw_data\\Task027_ACDC\\labelsTr\\patient009_frame01.nii.gz')\n",
    "arr = data.get_fdata()\n",
    "\n",
    "label1 = arr[:, :, 5]\n",
    "label2 = arr[:, :, 5]\n",
    "for t in range(0, 4):\n",
    "    current_class1 = (label1 == t)\n",
    "    current_class2 = (label2 == t)\n",
    "    both_x = np.logical_xor(current_class1, current_class2)\n",
    "\n",
    "    negmask = ~both_x\n",
    "    d2 = eucl_distance(negmask) * negmask - (eucl_distance(both_x) - 1) * both_x\n",
    "\n",
    "    fig, ax = plt.subplots(3, 2)\n",
    "    ax[0, 0].imshow(eucl_distance(negmask), cmap='plasma')\n",
    "    ax[0, 1].imshow(d2, cmap='plasma')\n",
    "    ax[1, 0].imshow(negmask, cmap='plasma')\n",
    "    ax[1, 1].imshow(both_x, cmap='plasma')\n",
    "    ax[2, 0].imshow(current_class1, cmap='gray')\n",
    "    ax[2, 1].imshow(current_class2, cmap='gray')\n",
    "    plt.waitforbuttonpress()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema(x, alpha):\n",
    "    if len(x) < 2:\n",
    "        return x\n",
    "    return alpha * x[0] + (1 - alpha) * ema(x[1:], alpha)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.sin(x) + np.random.random(100) * 0.2\n",
    "\n",
    "y[0] = 1\n",
    "\n",
    "plt.plot(x,y, color='blue')\n",
    "plt.show()\n",
    "\n",
    "print(ema(y, 0.5))\n",
    "\n",
    "for i in range(len(y)):\n",
    "    ema(y[:i+1], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1: 2, 2: 3, 3: 4}\n",
    "print(list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = np.arange(0, 500)\n",
    "\n",
    "out1 = 0.1 * (1 - 0.1)**k\n",
    "out2 = 0.05 * (1 - 0.05)**k\n",
    "out4 = 0.01 * (1 - 0.01)**k\n",
    "\n",
    "x = np.linspace(0, 500, 500)\n",
    "plt.plot(x, out1, color='blue')\n",
    "#plt.plot(x, out2, color='red')\n",
    "plt.plot(x, out4, color='green')\n",
    "plt.plot(x, out2, color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "from scipy.signal import savgol_filter\n",
    "from copy import copy\n",
    "\n",
    "sorted_past_percent = {0.16666666666666666: tensor(1.3600), 0.18181818181818182: tensor(1.8635), 0.2: tensor(1.2755), 0.21428571428571427: tensor(1.2107), 0.2222222222222222: tensor(1.1766), 0.23076923076923078: tensor(1.3838), 0.23529411764705882: tensor(0.9441), 0.25: tensor(1.2574), 0.26666666666666666: tensor(0.7934), 0.2727272727272727: tensor(1.4807), 0.2777777777777778: tensor(0.8594), 0.2857142857142857: tensor(1.2444), 0.29411764705882354: tensor(0.8864), 0.3: tensor(1.4371), 0.3076923076923077: tensor(0.8846), 0.3125: tensor(1.3526), 0.3333333333333333: tensor(1.2939), 0.35294117647058826: tensor(1.1473), 0.35714285714285715: tensor(1.4043), 0.36363636363636365: tensor(1.4904), 0.375: tensor(1.4065), 0.38461538461538464: tensor(0.4982), 0.3888888888888889: tensor(0.5812), 0.4: tensor(1.3152), 0.4117647058823529: tensor(0.9062), 0.4166666666666667: tensor(0.9554), 0.42857142857142855: \n",
    "tensor(1.3833), 0.4375: tensor(1.7965), 0.4444444444444444: tensor(1.2466), 0.45454545454545453: tensor(1.3580), 0.46153846153846156: tensor(1.3141), 0.4666666666666667: tensor(0.8624), 0.47058823529411764: tensor(1.1150), 0.5: tensor(1.2459), 0.5294117647058824: tensor(1.0637), 0.5333333333333333: tensor(1.4058), 0.5384615384615384: tensor(1.0728), 0.5454545454545454: tensor(1.6017), 0.5555555555555556: tensor(1.3091), 0.5625: tensor(1.5785), 0.5714285714285714: tensor(1.3401), 0.5833333333333334: tensor(0.7572), 0.5882352941176471: tensor(0.5408), 0.6: tensor(1.6448), 0.6111111111111112: tensor(0.5751), 0.6153846153846154: tensor(1.0880), 0.625: tensor(1.4969), 0.6363636363636364: tensor(1.4138), \n",
    "0.6428571428571429: tensor(1.0688), 0.6470588235294118: tensor(0.7755), 0.6666666666666666: tensor(1.5928), 0.6875: tensor(1.5619)}\n",
    "\n",
    "print(len(sorted_past_percent))\n",
    "\n",
    "#smooth1 = savgol_filter(list(sorted_past_percent.values()), 40, 3)\n",
    "#smooth2 = savgol_filter(list(sorted_past_percent.values()), 50, 3)\n",
    "smooth3 = savgol_filter(list(sorted_past_percent.values()), len(sorted_past_percent), 3)\n",
    "\n",
    "new_d = copy(sorted_past_percent)\n",
    "for k, v in zip(new_d.keys(), smooth3): # window size 50, polynomial order 3\n",
    "    sorted_past_percent[k] = v\n",
    "\n",
    "percent = min(new_d, key= lambda x: new_d[x])\n",
    "print(percent)\n",
    "\n",
    "plt.scatter(list(sorted_past_percent.keys()), list(new_d.values()))\n",
    "#plt.plot(list(sorted_past_percent.keys()), smooth1, color='red')\n",
    "#plt.plot(list(sorted_past_percent.keys()), smooth2, color='green')\n",
    "plt.plot(list(sorted_past_percent.keys()), smooth3, color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1, 2], [3, 4]])\n",
    "t2 = torch.tensor([[1, 2], [3, 4]])\n",
    "print(t1.shape)\n",
    "\n",
    "torch.matmul(t1, torch.transpose(t2, dim0=0, dim1=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.position_embedding import PositionEmbeddingSine2d\n",
    "\n",
    "def get_emb(sin_inp):\n",
    "    \"\"\"\n",
    "    Gets a base embedding for one dimension with sin and cos intertwined\n",
    "    \"\"\"\n",
    "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
    "    return torch.flatten(emb, -2, -1)\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / 4) * 2)\n",
    "        self.channels = channels\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.cached_penc = None\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "\n",
    "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
    "            return self.cached_penc\n",
    "\n",
    "        self.cached_penc = None\n",
    "        batch_size, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x).unsqueeze(1)\n",
    "        emb_y = get_emb(sin_inp_y)\n",
    "        emb = torch.zeros((x, y, self.channels * 2), device=tensor.device).type(\n",
    "            tensor.type()\n",
    "        )\n",
    "        emb[:, :, : self.channels] = emb_x\n",
    "        emb[:, :, self.channels : 2 * self.channels] = emb_y\n",
    "\n",
    "        self.cached_penc = emb[None, :, :, :orig_ch].repeat(tensor.shape[0], 1, 1, 1)\n",
    "        return self.cached_penc\n",
    "\n",
    "t = torch.rand(size=(2, 28, 28, 256))\n",
    "obj = PositionalEncoding2D(256)\n",
    "out = torch.flatten(obj(t).permute(0, 3, 1, 2), start_dim=2)\n",
    "print(out.shape)\n",
    "\n",
    "p_obj = PositionEmbeddingSine2d(num_pos_feats=128, normalize=False)\n",
    "out2 = torch.flatten(p_obj(shape_util=(2, 28, 28), device='cpu'), start_dim=2)\n",
    "print(out2.shape)\n",
    "\n",
    "equal = out == out2\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(out.cpu()[0], cmap='plasma')\n",
    "ax[1].imshow(out2.cpu()[0], cmap='plasma')\n",
    "ax[2].imshow(equal.cpu()[0], cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(size=(8, 256))\n",
    "t2 = torch.rand(size=(8, 256))\n",
    "\n",
    "out = torch.matmul(t1, torch.transpose(t2, dim0=0, dim1=1))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.loss import DistanceLoss\n",
    "\n",
    "loss = DistanceLoss(1.0)\n",
    "t1 = torch.rand(size=(8, 256, 1))\n",
    "t2 = torch.rand(size=(8, 256, 1))\n",
    "\n",
    "loss(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(size=(8, 8))\n",
    "out = torch.tril(t).fill_diagonal_(0)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.loss import RelationLoss\n",
    "\n",
    "loss_obj = RelationLoss()\n",
    "batch1 = torch.rand(size=(2, 4, 224, 224))\n",
    "batch2 = torch.rand(size=(2, 4, 224, 224))\n",
    "loss_obj(batch1, batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "\n",
    "path = 'out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\imagesTr\\\\patient094_frame01_0000.nii.gz'\n",
    "data = sitk.ReadImage(path)\n",
    "print(data[0].GetSpacing())\n",
    "print(data[0].GetOrigin())\n",
    "print(data[0].GetDirection())\n",
    "\n",
    "n2_img = nib.load(path)\n",
    "print(n2_img.shape)\n",
    "print(n2_img.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "\n",
    "nib_img = nib.load('ACDC_training\\\\patient001\\\\patient001_4d.nii.gz')\n",
    "img = nib_img.get_fdata()\n",
    "nib_img2 = nib.load('ACDC_training\\\\patient001\\\\patient001_frame01.nii.gz')\n",
    "img2 = nib_img2.get_fdata()\n",
    "for key, value in nib_img.header.items():\n",
    "    nib_img2.header[key] = value\n",
    "\n",
    "print(img.shape)\n",
    "print(img2.shape)\n",
    "print(np.all(img2[:, :, 5] == img[:, :, 5, 0]))\n",
    "print(np.all(img2[:, :, 5] == img[:, :, 5, 1]))\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(img[:, :, 5, 1], cmap='gray')\n",
    "ax[1].imshow(img[:, :, 5, 0], cmap='gray')\n",
    "ax[2].imshow(img2[:, :, 5], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = nib.load('out\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task027_ACDC\\\\imagesUn\\\\patient030_frame04_0000.nii.gz')\n",
    "print(img.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(r'out\\nnUNet_raw_data_base\\nnUNet_cropped_data\\Task027_ACDC\\unlabeled\\patient001_frame02.npz')\n",
    "print(data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle(file: str, mode: str = 'rb'):\n",
    "    with open(file, mode) as f:\n",
    "        a = pickle.load(f)\n",
    "    return a\n",
    "\n",
    "data = load_pickle(r'out\\nnUNet_preprocessed\\Task027_ACDC\\dataset_properties.pkl')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rand_bbox(size):\n",
    "    B = size[0]\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "\n",
    "    lam = np.random.beta(1, 1, size=(B,))\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = (W * cut_rat).astype(np.int32)\n",
    "    cut_h = (H * cut_rat).astype(np.int32)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W, size=(B,))\n",
    "    cy = np.random.randint(H, size=(B,))\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "bbx1, bby1, bbx2, bby2 = rand_bbox(size=(5, 1, 224, 224))\n",
    "t = torch.rand(size=(5, 1, 224, 224))\n",
    "for i in range(len(bbx1)):\n",
    "    t[i, :, bbx1[i]:bbx2[i], bby1[i]:bby2[i]] = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 5)\n",
    "for j in range(5):\n",
    "    ax[j].imshow(t[j, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(10)\n",
    "t > 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de6e7f1d27ee98e0dd03d720850162ba8f013030d5557c31bd8d79f8fd588abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
