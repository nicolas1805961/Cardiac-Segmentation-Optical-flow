use_sfb: false

strain_loss_weight: 0.0 # 100.0
forward_flow_loss_weight: 0.0 #1.0

global_motion_forward_loss_weight: 1.0 # 0.1
semi_supervised_seg_loss_weight: 0.0 # 0.1

seg_curvature_loss_weight: 0.0
flow_curvature_loss_weight: 0.0 # 100

interpolation_loss_weight: 0.0

also_start_es: False
distance_map_power: 0
downsample_conv: 1
conv_bottleneck: false
nb_conv: 2
no_error: false
registration_loss: ncc
segmentation: false
backward: false
motion_from_ed: true
training_modality: forward   # ['backward', 'forward', 'forward_no_sum']
dataloader_modality: other   # ['all_first', 'all_adjacent', 'regular', 'other']
legacy: true
final_stride: 1
do_data_aug: true
all_data_lib: false
dataloader_not_random: false
nb_iters: 1
nb_interp_frame: 0
only_first: false
split: true
padding: false
all_to_all: true
one_to_all: false
inference_mode: one_step   # sliding_window, one_step, overlap
consistency_loss_weight: 0.0 #0.01
regularization_weight_xy_local: 1.0 #0.01
regularization_weight_xy_global: 1.0 #0.01
regularization_weight_z: 0.0 #0.001
nb_tokens: 1
segmentation_loss_weight: 0.0 # 0.1
image_flow_loss_weight_global: 0.5 # 1.0
image_flow_loss_weight_local: 0.5 # 1.0
force_one_label: true
feature_extractor: false
video_length: 4
crop: true
nb_layers: 1

labeled: false
log_images: false
device: cuda:0
deep_supervision: false
log_stats: true
overfit_log: 555 #10
epoch_log: 555 #50
scheduler: cosine
optimizer: adam
initial_lr: 0.0001 #0.0001
weight_decay: 0.0001 #0.0001
warmup_percent: 0.1
max_num_epochs: 180 #400
#window_size: 7
#image_size: 224
norm: group   # 'group', 'batch'
bottleneck_heads: 8 # 16
activation: gelu
conv_layer: other
dropout: 0
conv_depth: [1, 1, 1]
transformer_depth: [] #[2, 2, 2], [2]
num_heads: [] #[3, 6, 12], [8] [12]
spatial_cross_attention_num_heads: [8, 8, 8] #[2, 4, 6, 8, 12] [12, 8, 6, 4, 2] [bottom, ..., top] [3, 6, 12] [4, 4, 8, 8, 16]
batch_size: 1 #16
drop_path_rate: 0.0
in_encoder_dims: [6, 128, 256] #[1, 128, 256] [1, 96, 384] [1, 24, 96] [1, 32, 128] [1, 32, 128, 256, 512] [1, 48, 192] [1, 96, 192]
out_encoder_dims: [64, 128, 256] #[64, 128, 256] [96, 192, 384] [24, 48, 96] [32, 64, 128] [32, 64, 128, 256, 512] [48, 96, 192] [96, 192, 384]
loss: ce_and_dice   # ['ce_and_dice', 'focal_and_dice', 'topk_and_dice', 'ce']

do_adv: false
adversarial_weight: 0.0
discriminator_depth: [2, 2, 2]
discriminator_in_dims: [4, 128, 256]
discriminator_out_dims: [64, 128, 256]
discriminator_lr: 0.00001 # 0.00005
discriminator_decay: 0.0001

reinforcement: false
policy_net_learning_rate: 0.0001
number_of_intervals: 20
number_of_steps: 200
